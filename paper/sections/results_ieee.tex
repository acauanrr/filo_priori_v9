%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: RESULTS (IEEE TSE Format)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: Section header is in main_ieee_tse.tex

This section presents experimental results organized by research questions.
We evaluate \filopriori{} on two complementary domains: (1) industrial software
testing with rich semantic information, and (2) GNN benchmark datasets representing
node classification tasks on citation networks.

%------------------------------------------------------------------------------
% RQ1: Effectiveness on Industrial Dataset
%------------------------------------------------------------------------------

\subsection{RQ1: Effectiveness on Industrial Dataset}
\label{sec:rq1}

Table~\ref{tab:tcp_comparison} presents the comparison of \filopriori{} against
state-of-the-art TCP methods on the industrial QTA dataset containing
4,552 builds with 100,502 test executions (277 builds with failures).
We use APFD as the primary metric with Wilcoxon signed-rank tests for statistical
significance and Cliff's delta for effect size.

\begin{table}[!t]
\centering
\caption{Comparison with State-of-the-Art TCP Methods (Industrial Dataset)}
\label{tab:tcp_comparison}
\footnotesize
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{APFD} & \textbf{Std} & \textbf{p-value} & \textbf{Effect} & \textbf{$\Delta$} \\
\midrule
\textbf{\filopriori{}} & \textbf{0.7595} & \textbf{0.189} & -- & -- & -- \\
DeepOrder~\cite{chen2023deeporder} & 0.6919 & 0.267 & $<$0.001*** & small & +9.8\% \\
NodeRank~\cite{li2024noderank} & 0.6623 & 0.270 & $<$0.001*** & small & +14.7\% \\
Random & 0.5000 & -- & $<$0.001*** & large & +51.9\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item *** p $<$ 0.001 (Wilcoxon signed-rank test vs \filopriori{})
\item Effect sizes measured using Cliff's delta.
\end{tablenotes}
\end{table}

\textbf{Key Findings for RQ1:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.7595} (std: 0.189),
    representing a \textbf{51.9\%} improvement over random ordering.
    \item \filopriori{} \textbf{significantly outperforms} NodeRank~\cite{li2024noderank}
    by \textbf{+14.7\%} ($p < 0.001$, Cliff's delta = 0.179, small effect).
    \item \filopriori{} also outperforms DeepOrder~\cite{chen2023deeporder}
    by \textbf{+9.8\%} ($p < 0.001$), demonstrating the effectiveness of our
    dual-stream graph attention architecture.
    \item Per-build analysis shows \filopriori{} outperforms NodeRank in
    \textbf{52.7\%} of builds (vs 31.0\% for NodeRank, 16.2\% ties).
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ1]
\filopriori{} achieves APFD=0.7595, significantly outperforming state-of-the-art
methods: NodeRank ($+14.7\%$, $p < 0.001$) and DeepOrder ($+9.8\%$, $p < 0.001$).
The dual-stream GAT architecture with semantic and structural features provides
superior fault detection compared to existing GNN-based approaches.
\end{tcolorbox}

%------------------------------------------------------------------------------
% GNN Benchmark Evaluation
%------------------------------------------------------------------------------

\subsection{Evaluation on GNN Benchmark Datasets}
\label{sec:gnn_benchmark}

To validate the generalizability of our approach beyond industrial software testing,
we conducted a comprehensive evaluation on standard GNN benchmark datasets used in
Li et al.'s NodeRank~\cite{li2024noderank}. This experiment demonstrates that the
core principles of \filopriori{}---multi-edge graph construction, comprehensive
feature extraction, and learning-to-rank---transfer effectively to the domain
of test input prioritization for Graph Neural Networks.

\subsubsection{Experimental Setup}

We evaluated on three citation network datasets commonly used in GNN research:
\textbf{Cora} (2,708 nodes, 7 classes), \textbf{CiteSeer} (3,327 nodes, 6 classes),
and \textbf{PubMed} (19,717 nodes, 3 classes). Following the NodeRank experimental
protocol~\cite{li2024noderank}, we trained a 2-layer Graph Convolutional Network
(GCN) for node classification and treated misclassified nodes as ``faults'' to
be detected through prioritization.

We adapted the \filopriori{} methodology for GNN testing by implementing:
\begin{itemize}
    \item \textbf{Multi-Edge Graph Construction}: Combining original citation
    edges, semantic similarity edges (cosine similarity $> 0.65$), and prediction
    agreement edges (nodes with same predicted class).
    \item \textbf{Feature Extraction}: 25 features across three categories---structural
    (degree centrality, clustering coefficient, PageRank), uncertainty (entropy,
    margin, Gini impurity, confidence), and neighborhood (neighbor entropy,
    agreement, label homophily).
    \item \textbf{Learning-to-Rank}: LightGBM classifier trained to predict
    misclassification likelihood, with predicted probability as the ranking score.
\end{itemize}

We compared against five baselines: Random ordering, DeepGini~\cite{feng2020deepgini}
(Gini impurity), Entropy (prediction entropy), VanillaSM (softmax margin), and
PCS (least confidence). All experiments were repeated 5 times with different
random seeds, and we report mean and standard deviation.

\subsubsection{Results}

Table~\ref{tab:gnn_benchmark} presents detailed results for each dataset, and
Table~\ref{tab:gnn_summary} summarizes the comparison with NodeRank.

\input{tables/tab_gnn_benchmark}

\input{tables/tab_gnn_summary}

\textbf{Key Findings:}

\textbf{(1) Superior Performance on Cora.} \filopriori{} achieves APFD=0.861,
outperforming NodeRank (0.833) by \textbf{+3.4\%}. This represents a substantial
improvement on the most commonly used GNN benchmark. The strong PFD@10=0.412
indicates that \filopriori{} detects over 41\% of misclassifications in the first
10\% of the prioritized test sequence.

\textbf{(2) Consistent Improvement on CiteSeer.} On CiteSeer, \filopriori{}
achieves APFD=0.742 compared to NodeRank's 0.733, a \textbf{+1.3\%} improvement.
CiteSeer is known to be more challenging due to higher feature sparsity and less
distinct class boundaries, yet \filopriori{} maintains competitive performance.

\textbf{(3) Competitive Performance on PubMed.} On PubMed, \filopriori{} achieves
APFD=0.785, slightly below NodeRank (0.790, -0.6\%). PubMed's larger scale
(19,717 nodes) and lower class count (3 classes) present a different challenge
where uncertainty-based features may provide less discriminative power. Nevertheless,
the difference is within one standard deviation, indicating comparable performance.

\textbf{(4) Consistent Gains over Uncertainty Baselines.} Across all datasets,
\filopriori{} substantially outperforms uncertainty-based baselines (DeepGini,
Entropy, VanillaSM), with improvements ranging from +11\% to +21\%. This validates
the value of incorporating structural information and learning-to-rank beyond
simple uncertainty measures.

\textbf{(5) Strong Early Fault Detection.} The PFD@10 and PFD@20 metrics
demonstrate \filopriori{}'s effectiveness in early fault detection. On Cora,
PFD@20=0.755 means 75.5\% of all misclassifications are found in the first
20\% of the test sequence---a 3$\times$ improvement over random ordering.

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=GNN Benchmark Finding]
\filopriori{} achieves an average APFD of 0.796 across GNN benchmarks,
outperforming NodeRank~\cite{li2024noderank} on 2 of 3 datasets (Cora: +3.4\%,
CiteSeer: +1.3\%) and achieving competitive results on PubMed (-0.6\%).
The multi-edge graph construction and comprehensive feature extraction transfer
effectively from software testing to GNN test input prioritization.
\end{tcolorbox}

%------------------------------------------------------------------------------
% Cross-Dataset Validation: RTPTorrent
%------------------------------------------------------------------------------

\subsection{Cross-Dataset Validation: RTPTorrent}
\label{sec:rq1_rtptorrent}

To further evaluate generalization, we conducted a comprehensive experiment on
the RTPTorrent open-source benchmark~\cite{mattis2020rtptorrent}, which contains
test execution histories from 20 Java projects with over 100,000 Travis CI builds.
We adapted \filopriori{} using a LightGBM LambdaRank model with 16 ranking-optimized
features, trained independently on each project.

\begin{table}[!t]
\centering
\caption{RTPTorrent Cross-Dataset Results (20 Projects)}
\label{tab:rtptorrent_results}
\footnotesize
\begin{tabular}{lccr}
\toprule
\textbf{Baseline} & \textbf{APFD} & \textbf{Model Improv.} & \textbf{p-value} \\
\midrule
\textbf{Filo-Priori V10} & \textbf{0.8376} & -- & -- \\
\midrule
recently\_failed & 0.8209 & +2.02\% & sig.* \\
random & 0.4940 & +69.56\% & $<$0.001*** \\
untreated & 0.3574 & +134.32\% & $<$0.001*** \\
matrix\_naive & 0.5693 & +47.11\% & $<$0.001*** \\
matrix\_conditional & 0.5132 & +63.21\% & $<$0.001*** \\
optimal\_duration & 0.5934 & +41.15\% & $<$0.001*** \\
optimal\_failure (oracle) & 0.9249 & -9.45\% & -- \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Aggregate results over 1,250 test builds from 20 Java projects.
\item * Statistically significant improvement over strongest baseline.
\end{tablenotes}
\end{table}

Table~\ref{tab:rtptorrent_top5} shows the top-5 performing projects:

\begin{table}[!t]
\centering
\caption{Top-5 Projects by APFD (RTPTorrent)}
\label{tab:rtptorrent_top5}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Project} & \textbf{APFD} & \textbf{Builds} & \textbf{vs recent.} \\
\midrule
apache/sling & 0.9922 & 163 & +2.18\% \\
neuland/jade4j & 0.9799 & 20 & +0.88\% \\
eclipse/jetty.project & 0.9789 & 66 & +1.27\% \\
facebook/buck & 0.9722 & 69 & +1.97\% \\
deeplearning4j/dl4j & 0.9277 & 114 & +0.46\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for Cross-Dataset Validation:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.8376} across 20 diverse
    Java projects (1,250 test builds), demonstrating strong cross-domain generalization.
    \item The model \textbf{outperforms 6 out of 7 baselines}, including the strongest
    heuristic baseline (recently\_failed) by +2.02\%.
    \item Only the oracle baseline (optimal\_failure) achieves higher APFD (0.9249),
    representing the theoretical upper bound with perfect failure knowledge.
    \item Performance varies across projects (APFD: 0.29--0.99), with best results
    on projects with consistent failure patterns (e.g., apache/sling).
    \item The most important features are \texttt{novelty\_score}, \texttt{base\_risk},
    \texttt{execution\_frequency}, and \texttt{time\_decay\_score}.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Cross-Dataset Finding]
\filopriori{} generalizes effectively to open-source projects, achieving APFD=0.8376
across 20 RTPTorrent projects and outperforming 6 of 7 baselines. Combined with
GNN benchmark results (0.796 avg APFD), this demonstrates the approach's versatility
across software testing and machine learning domains.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ2: Component Contributions
%------------------------------------------------------------------------------

\subsection{RQ2: Ablation Study}
\label{sec:rq2}

To understand the contribution of each component, we conducted an ablation study
isolating five key innovations. Table~\ref{tab:ablation} shows the impact of
removing each component from the full system (APFD=0.7595).

\begin{table}[!t]
\centering
\caption{Ablation Study: Component Contributions to APFD}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{APFD} & \textbf{$\Delta$} & \textbf{p-value} \\
\midrule
Full Model & 0.7595 & -- & -- \\
\midrule
w/o Dense Multi-Edge Graph & 0.6835 & -10.0\% & $<$0.001*** \\
w/o Orphan KNN Scoring & 0.7145 & -5.9\% & $<$0.001*** \\
w/o Single Balancing & 0.7291 & -4.0\% & $<$0.001*** \\
w/o DeepOrder Features & 0.7443 & -2.0\% & 0.003** \\
w/o Threshold Optimization & 0.7519 & -1.0\% & 0.042* \\
\midrule
Baseline (V1) & 0.6503 & -14.4\% & $<$0.001*** \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001 (Wilcoxon signed-rank test)
\end{tablenotes}
\end{table}

\textbf{Component Analysis:}

\textbf{(1) Dense Multi-Edge Graph (+10.0\%):} The largest contribution comes from
increasing graph connectivity. Key changes: semantic threshold 0.75$\rightarrow$0.65,
top-k neighbors 5$\rightarrow$10, and adding temporal/component edges. This increased
in-graph coverage from 50--60\% to \textbf{77.4\%}.

\textbf{(2) Orphan KNN Scoring (+5.9\%):} Orphan tests (22.6\% of total) previously
received uniform scores, destroying ranking. Our KNN pipeline with Euclidean distance,
structural blending ($w=0.35$), and temperature scaling ($T=0.7$) restores meaningful
score variance (std=0.046 vs. 0.000).

\textbf{(3) Single Balancing (+4.0\%):} Prior work used triple compensation (class weights
+ balanced sampling + focal alpha), causing mode collapse. Using \emph{only} balanced
sampling (15:1 ratio) with neutral $\alpha=0.5$ stabilizes training and prevents
over-prediction of the minority class.

\textbf{(4) DeepOrder Features (+2.0\%):} Temporal execution features (execution\_status\_last\_[1,2,3,5,10],
cycles\_since\_last\_fail) capture recent failure patterns that static features miss.

\textbf{(5) Threshold Optimization (+1.0\%):} Two-phase search finding threshold 0.28
(vs. default 0.50) improves minority class recall from 5\% to 30\%.

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ2]
The five key innovations contribute cumulatively to the +16.8\% improvement over
baseline: Dense Multi-Edge Graph (+10.0\%), Orphan KNN Scoring (+5.9\%),
Single Balancing (+4.0\%), DeepOrder Features (+2.0\%), and Threshold Optimization (+1.0\%).
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ3: Temporal Robustness
%------------------------------------------------------------------------------

\subsection{RQ3: Temporal Validation}
\label{sec:rq3}

To evaluate generalization to future builds, we used temporal cross-validation
where training data always precedes test data chronologically.

\begin{table}[!t]
\centering
\caption{Temporal Cross-Validation Results}
\label{tab:temporal_cv}
\begin{tabular}{lcc}
\toprule
\textbf{Validation Method} & \textbf{APFD} & \textbf{Folds} \\
\midrule
Temporal 5-Fold CV & 0.7821 & 5 \\
Sliding Window CV & 0.7412 & 10 \\
Concept Drift Test & 0.7298 & 3 \\
\midrule
\textbf{Average} & \textbf{0.7510} & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for RQ3:}
\begin{itemize}
    \item Performance remains stable across temporal validation methods,
    with APFD ranging from 0.73 to 0.78.
    \item Concept drift causes only 3\% degradation, indicating robustness
    to evolving test patterns over time.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ3]
\filopriori{} demonstrates robust temporal generalization (APFD: 0.73--0.78),
with minimal concept drift degradation (3\%).
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ4: Sensitivity Analysis
%------------------------------------------------------------------------------

\subsection{RQ4: Hyperparameter Sensitivity}
\label{sec:rq4}

We analyzed sensitivity to key hyperparameters across multiple configurations.

\begin{table}[!t]
\centering
\caption{Hyperparameter Sensitivity Analysis}
\label{tab:sensitivity}
\begin{tabular}{llcc}
\toprule
\textbf{Parameter} & \textbf{Values} & \textbf{Best} & \textbf{Impact} \\
\midrule
Loss Function & CE, Focal, W.~Focal & W.~Focal & 4.7\% \\
Focal Gamma & 1.5, 2.0, 2.5, 3.0 & 2.5 & 4.5\% \\
Learning Rate & 1e-5, 3e-5, 5e-5, 1e-4 & 3e-5 & 3.6\% \\
GNN Layers & 1, 2, 3 & 1 & 3.6\% \\
GNN Heads & 1, 2, 4, 8 & 2 & 2.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for RQ4:}
\begin{itemize}
    \item \textbf{Loss Function}: Weighted Focal Loss has the largest impact (4.7\%).
    \item \textbf{Architecture}: Simpler configuration (1 layer, 2 heads) performs
    best, avoiding overfitting on the test graph.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ4]
Optimal configuration: Weighted Focal Loss ($\gamma=2.5$), learning rate 3e-5,
1-layer GAT with 2 heads. Loss function choice has the largest impact (4.7\%).
\end{tcolorbox}

%------------------------------------------------------------------------------
% Summary of Experimental Results
%------------------------------------------------------------------------------

\subsection{Summary of Results}
\label{sec:results_summary}

Table~\ref{tab:results_summary} summarizes \filopriori{}'s performance across
all experimental settings.

\begin{table}[!t]
\centering
\caption{Summary of Experimental Results}
\label{tab:results_summary}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Experiment} & \textbf{APFD} & \textbf{vs.\ Best Baseline} & \textbf{Significance} \\
\midrule
Industrial QTA & 0.760 & +9.8\% (DeepOrder) & $p < 0.001$ \\
GNN Benchmarks & 0.796 & +1.4\% (NodeRank) & 2/3 wins \\
RTPTorrent (20 proj.) & 0.838 & +2.0\% (recently\_failed) & sig.* \\
\midrule
\textbf{Overall Average} & \textbf{0.798} & -- & -- \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item APFD: Average Percentage of Faults Detected (higher is better).
\item Results demonstrate consistent performance across three distinct domains.
\end{tablenotes}
\end{table}

The experimental results demonstrate that \filopriori{} achieves consistent,
state-of-the-art performance across three distinct evaluation settings:
industrial software testing (0.760), GNN model testing (0.796), and open-source
CI/CD environments (0.838). The multi-edge graph construction and dual-stream
architecture provide robust prioritization regardless of the specific domain
or characteristics of the test data.
