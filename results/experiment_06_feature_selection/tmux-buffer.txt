INFO:__main__:Loading configuration...
INFO:__main__:Loading configuration from configs/experiment_06_feature_selection.yaml
INFO:__main__:Validating configuration...
INFO:utils.config_validator:Validating configuration...
INFO:utils.config_validator:Configuration validation passed!
INFO:__main__:Configuration validation passed!
INFO:__main__:Using device: cuda
INFO:__main__:======================================================================
INFO:__main__:STEP 1: DATA PREPARATION
INFO:__main__:======================================================================
INFO:__main__:
1.1: Loading datasets...
INFO:preprocessing.data_loader:Loading data from datasets/train.csv
INFO:preprocessing.data_loader:Loading full dataset (this may take a while...)
INFO:preprocessing.data_loader:Loaded 69169 records
INFO:preprocessing.data_loader:Columns: ['Build_ID', 'Build_ID_entry', 'TP_Key', 'TC_Key', 'TE_Summary', 'TE_Key', 'Build_Test_Start_Date', 'TE_Date', 'TE_Created_Date', 'TE_Test_Result', 'TC_Steps', 'TE_Updated_Date', 'commit', 'CR', 'CR_Resolution', 'CR_Resolved_Date', 'CR_Component_Name', 'CR_Type']
INFO:preprocessing.data_loader:Cleaning data...
INFO:preprocessing.data_loader:Dropped 0 rows with missing critical fields
INFO:preprocessing.data_loader:Target distribution:
TE_Test_Result
Pass                61224
Delete               3653
Blocked              1862
Fail                 1654
Conditional Pass      654
Pending               116
Blocked-NA              4
Indeterminate           2
Name: count, dtype: int64
INFO:preprocessing.data_loader:Binary classification strategy: pass_vs_fail
INFO:preprocessing.data_loader:Positive class: Pass
INFO:preprocessing.data_loader:Excluded samples with labels other than Pass/Fail
INFO:preprocessing.data_loader:Binary label mapping: {'Fail': 0, 'Pass': 1}
INFO:preprocessing.data_loader:Class distribution after encoding:
label
1    61224
0     1654
Name: count, dtype: int64
INFO:preprocessing.data_loader:Class weights: [19.00785973  0.51350777]
INFO:preprocessing.data_loader:ðŸ”’ Using GROUP-AWARE split (by Build_ID) to prevent leakage
INFO:preprocessing.data_loader:   Total samples: 62878, Total builds: 3067
INFO:preprocessing.data_loader:âœ… Train set: 50621 samples (2453 builds)
INFO:preprocessing.data_loader:âœ… Val set:   6062 samples (307 builds)
INFO:preprocessing.data_loader:âœ… Test set:  6195 samples (307 builds)
INFO:preprocessing.data_loader:âœ… No build leakage: All splits are disjoint by Build_ID
INFO:__main__:  Train: 50621 samples
INFO:__main__:  Val: 6062 samples
INFO:__main__:  Test: 6195 samples
INFO:__main__:
1.1.1: Computing class weights...
INFO:preprocessing.data_loader:Class weights: [19.13114135  0.51341839]
INFO:__main__:  Class weights: [19.13114135  0.51341839]
INFO:__main__:  Weight ratio (minority/majority): 37.26:1
INFO:__main__:
1.2: Extracting semantic embeddings with SBERT...
INFO:__main__:  Using EmbeddingManager with intelligent caching
INFO:__main__:  Generating/loading embeddings for 50621 train + 6062 val + 6195 test samples...
WARNING:embeddings.embedding_cache:Data has changed since cache was created
INFO:embeddings.embedding_manager:Cache found but invalid (data changed) - regenerating
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:GENERATING EMBEDDINGS
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:Initializing encoder: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:Initializing SBERT encoder on cuda
INFO:embeddings.sbert_encoder:Model: sentence-transformers/all-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:âœ“ Loaded SBERT model on cuda
INFO:embeddings.sbert_encoder:âœ“ Embedding dimension: 768
INFO:embeddings.sbert_encoder:âœ“ Batch size: 128
INFO:embeddings.sbert_encoder:âœ“ Max sequence length: 384
INFO:embeddings.embedding_manager:Preparing texts...
INFO:embeddings.embedding_manager:  Train TCs: 50621
INFO:embeddings.embedding_manager:  Test TCs: 6195
INFO:embeddings.embedding_manager:  Train Commits: 50621
INFO:embeddings.embedding_manager:  Test Commits: 6195
INFO:embeddings.embedding_manager:Encoding...
INFO:embeddings.sbert_encoder:Encoding 50621 texts in 40 chunks (chunk_size=1280)
Train TCs:   0%|          | 0/40 [00:00<?, ?it/s]Train TCs:   2%|â–Ž         | 1/40 [00:00<00:25,  1.52it/s]Train TCs:   5%|â–Œ         | 2/40 [00:01<00:18,  2.02it/s]Train TCs:   8%|â–Š         | 3/40 [00:01<00:15,  2.33it/s]Train TCs:  10%|â–ˆ         | 4/40 [00:01<00:14,  2.52it/s]Train TCs:  12%|â–ˆâ–Ž        | 5/40 [00:02<00:13,  2.65it/s]Train TCs:  15%|â–ˆâ–Œ        | 6/40 [00:02<00:12,  2.72it/s]Train TCs:  18%|â–ˆâ–Š        | 7/40 [00:02<00:12,  2.72it/s]Train TCs:  20%|â–ˆâ–ˆ        | 8/40 [00:03<00:11,  2.77it/s]Train TCs:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:03<00:11,  2.81it/s]Train TCs:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:03<00:10,  2.81it/s]Train TCs:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:04<00:11,  2.62it/s]Train TCs:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:04<00:10,  2.69it/s]Train TCs:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:04<00:09,  2.75it/s]Train TCs:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:05<00:09,  2.78it/s]Train TCs:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:05<00:08,  2.84it/s]Train TCs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:06<00:08,  2.87it/s]Train TCs:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:06<00:07,  2.88it/s]Train TCs:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:06<00:07,  2.87it/s]Train TCs:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:07<00:07,  2.87it/s]Train TCs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:07<00:06,  2.87it/s]Train TCs:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:07<00:07,  2.64it/s]Train TCs:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:08<00:06,  2.71it/s]Train TCs:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:08<00:06,  2.79it/s]Train TCs:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:08<00:05,  2.83it/s]Train TCs:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:09<00:05,  2.87it/s]Train TCs:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:09<00:04,  2.88it/s]Train TCs:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:09<00:04,  2.91it/s]Train TCs:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:10<00:04,  2.93it/s]Train TCs:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:10<00:03,  2.93it/s]Train TCs:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:10<00:03,  2.93it/s]Train TCs:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:11<00:03,  2.68it/s]Train TCs:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:11<00:02,  2.76it/s]Train TCs:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:12<00:02,  2.75it/s]Train TCs:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:12<00:02,  2.81it/s]Train TCs:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:12<00:01,  2.83it/s]Train TCs:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:13<00:01,  2.86it/s]Train TCs:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:13<00:01,  2.87it/s]Train TCs:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:13<00:00,  2.90it/s]Train TCs:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:14<00:00,  2.90it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:14<00:00,  3.33it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:14<00:00,  2.80it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 50621 texts â†’ shape: (50621, 768)
INFO:embeddings.sbert_encoder:Encoding 6195 texts in 5 chunks (chunk_size=1280)
Test TCs:   0%|          | 0/5 [00:00<?, ?it/s]Test TCs:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.21it/s]Test TCs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.51it/s]Test TCs:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.61it/s]Test TCs:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:02<00:00,  1.57it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.86it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.95it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6195 texts â†’ shape: (6195, 768)
INFO:embeddings.sbert_encoder:Encoding 50621 texts in 40 chunks (chunk_size=1280)
Train Commits:   0%|          | 0/40 [00:00<?, ?it/s]Train Commits:   2%|â–Ž         | 1/40 [00:00<00:19,  1.99it/s]Train Commits:   5%|â–Œ         | 2/40 [00:00<00:15,  2.53it/s]Train Commits:   8%|â–Š         | 3/40 [00:01<00:13,  2.75it/s]Train Commits:  10%|â–ˆ         | 4/40 [00:01<00:12,  2.86it/s]Train Commits:  12%|â–ˆâ–Ž        | 5/40 [00:01<00:11,  2.94it/s]Train Commits:  15%|â–ˆâ–Œ        | 6/40 [00:02<00:11,  2.98it/s]Train Commits:  18%|â–ˆâ–Š        | 7/40 [00:02<00:10,  3.01it/s]Train Commits:  20%|â–ˆâ–ˆ        | 8/40 [00:02<00:10,  3.03it/s]Train Commits:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:03<00:10,  3.05it/s]Train Commits:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:03<00:09,  3.06it/s]Train Commits:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:03<00:10,  2.79it/s]Train Commits:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:04<00:09,  2.84it/s]Train Commits:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:04<00:09,  2.87it/s]Train Commits:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [00:04<00:08,  2.96it/s]Train Commits:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:05<00:08,  3.02it/s]Train Commits:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:05<00:07,  3.07it/s]Train Commits:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:05<00:07,  3.10it/s]Train Commits:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:06<00:07,  3.13it/s]Train Commits:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [00:06<00:06,  3.15it/s]Train Commits:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:06<00:06,  3.16it/s]Train Commits:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [00:07<00:06,  2.90it/s]Train Commits:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:07<00:06,  2.97it/s]Train Commits:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [00:07<00:05,  3.05it/s]Train Commits:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:08<00:05,  3.08it/s]Train Commits:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:08<00:04,  3.13it/s]Train Commits:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [00:08<00:04,  3.15it/s]Train Commits:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:09<00:04,  3.15it/s]Train Commits:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [00:09<00:03,  3.17it/s]Train Commits:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:09<00:03,  3.18it/s]Train Commits:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:09<00:03,  3.18it/s]Train Commits:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:10<00:03,  2.89it/s]Train Commits:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:10<00:02,  2.98it/s]Train Commits:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:11<00:02,  3.03it/s]Train Commits:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [00:11<00:01,  3.03it/s]Train Commits:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:11<00:01,  3.10it/s]Train Commits:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:11<00:01,  3.14it/s]Train Commits:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [00:12<00:00,  3.18it/s]Train Commits:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:12<00:00,  3.19it/s]Train Commits:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [00:12<00:00,  3.20it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:13<00:00,  3.67it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:13<00:00,  3.06it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 50621 texts â†’ shape: (50621, 768)
INFO:embeddings.sbert_encoder:Encoding 6195 texts in 5 chunks (chunk_size=1280)
Test Commits:   0%|          | 0/5 [00:00<?, ?it/s]Test Commits:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.39it/s]Test Commits:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.83it/s]Test Commits:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.99it/s]Test Commits:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  3.07it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.27it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.09it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6195 texts â†’ shape: (6195, 768)
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:SAVING EMBEDDINGS TO CACHE
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:âœ“ Saved to: cache/embeddings.npz
INFO:embeddings.embedding_cache:âœ“ File size: 2.0 MB
INFO:embeddings.embedding_cache:âœ“ Metadata: cache/embeddings_metadata.txt
INFO:embeddings.embedding_cache:======================================================================
INFO:__main__:
  Encoding validation set...
WARNING:embeddings.embedding_cache:Data has changed since cache was created
INFO:embeddings.embedding_manager:Cache found but invalid (data changed) - regenerating
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:GENERATING EMBEDDINGS
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:Initializing encoder: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:Initializing SBERT encoder on cuda
INFO:embeddings.sbert_encoder:Model: sentence-transformers/all-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:âœ“ Loaded SBERT model on cuda
INFO:embeddings.sbert_encoder:âœ“ Embedding dimension: 768
INFO:embeddings.sbert_encoder:âœ“ Batch size: 128
INFO:embeddings.sbert_encoder:âœ“ Max sequence length: 384
INFO:embeddings.embedding_manager:Preparing texts...
INFO:embeddings.embedding_manager:  Train TCs: 6062
INFO:embeddings.embedding_manager:  Test TCs: 6062
INFO:embeddings.embedding_manager:  Train Commits: 6062
INFO:embeddings.embedding_manager:  Test Commits: 6062
INFO:embeddings.embedding_manager:Encoding...
INFO:embeddings.sbert_encoder:Encoding 6062 texts in 5 chunks (chunk_size=1280)
Train TCs:   0%|          | 0/5 [00:00<?, ?it/s]Train TCs:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:02,  1.40it/s]Train TCs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:01,  2.01it/s]Train TCs:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.34it/s]Train TCs:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.53it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.88it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.49it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6062 texts â†’ shape: (6062, 768)
INFO:embeddings.sbert_encoder:Encoding 6062 texts in 5 chunks (chunk_size=1280)
Test TCs:   0%|          | 0/5 [00:00<?, ?it/s]Test TCs:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.24it/s]Test TCs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.59it/s]Test TCs:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.74it/s]Test TCs:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.80it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.08it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.87it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6062 texts â†’ shape: (6062, 768)
INFO:embeddings.sbert_encoder:Encoding 6062 texts in 5 chunks (chunk_size=1280)
Train Commits:   0%|          | 0/5 [00:00<?, ?it/s]Train Commits:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.36it/s]Train Commits:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.74it/s]Train Commits:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.91it/s]Train Commits:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.98it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.29it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.06it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6062 texts â†’ shape: (6062, 768)
INFO:embeddings.sbert_encoder:Encoding 6062 texts in 5 chunks (chunk_size=1280)
Test Commits:   0%|          | 0/5 [00:00<?, ?it/s]Test Commits:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  2.36it/s]Test Commits:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:01,  2.74it/s]Test Commits:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.89it/s]Test Commits:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.96it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.27it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.04it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 6062 texts â†’ shape: (6062, 768)
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:SAVING EMBEDDINGS TO CACHE
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:âœ“ Saved to: cache/embeddings.npz
INFO:embeddings.embedding_cache:âœ“ File size: 0.4 MB
INFO:embeddings.embedding_cache:âœ“ Metadata: cache/embeddings_metadata.txt
INFO:embeddings.embedding_cache:======================================================================
INFO:__main__:  Embedding dimension: 768
INFO:__main__:  Combined dimension: 1536
INFO:__main__:  Model: sentence-transformers/all-mpnet-base-v2
INFO:__main__:
  Concatenating TC and Commit embeddings...
INFO:__main__:  Train embeddings: (50621, 1536)
INFO:__main__:  Val embeddings: (6062, 1536)
INFO:__main__:  Test embeddings: (6195, 1536)
INFO:__main__:
1.4: Extracting structural features...
INFO:__main__:  Initializing StructuralFeatureExtractorV2.5 (10 selected features)...
INFO:preprocessing.structural_feature_extractor_v2:Initialized StructuralFeatureExtractorV2 with:
INFO:preprocessing.structural_feature_extractor_v2:  recent_window=5
INFO:preprocessing.structural_feature_extractor_v2:  very_recent_window=2
INFO:preprocessing.structural_feature_extractor_v2:  medium_term_window=10
INFO:preprocessing.structural_feature_extractor_v2:  â†’ 29 features total
INFO:preprocessing.structural_feature_extractor_v2_5:Initialized StructuralFeatureExtractorV2.5 with:
INFO:preprocessing.structural_feature_extractor_v2_5:  recent_window=5
INFO:preprocessing.structural_feature_extractor_v2_5:  very_recent_window=2
INFO:preprocessing.structural_feature_extractor_v2_5:  medium_term_window=10
INFO:preprocessing.structural_feature_extractor_v2_5:  â†’ Extracting 10 selected features from 29
INFO:preprocessing.structural_feature_extractor_v2_5:  â†’ Selected indices: [0, 1, 2, 3, 7, 9, 13, 20, 21, 23]
INFO:__main__:  âœ“ Using V2.5 extractor with 10 selected features
INFO:__main__:  Fitting extractor on training data...
INFO:preprocessing.structural_feature_extractor_v2:Fitting StructuralFeatureExtractorV2 on training data...
INFO:preprocessing.structural_feature_extractor_v2:Training data shape: (50621, 22)
INFO:preprocessing.structural_feature_extractor_v2:Build chronology established using Build_Test_Start_Date
INFO:preprocessing.structural_feature_extractor_v2:Chronology spans 2453 builds
INFO:preprocessing.structural_feature_extractor_v2:Computing EXTENSIVE per-TC_Key historical statistics (V2)...
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Computed EXTENSIVE history for 2347 test cases
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Stored first appearances for 2347 test cases
INFO:preprocessing.structural_feature_extractor_v2:Computing global statistics for 29 features...
INFO:preprocessing.structural_feature_extractor_v2:  Feature means (first 10): [8.3890198e+02 2.6135275e-02 2.5333719e-02 2.3399048e-02 6.2583454e+01
 1.2965963e+00 6.1286858e+01 5.8631793e-02 4.2796230e+01 7.9253668e-01]
INFO:preprocessing.structural_feature_extractor_v2:  Feature medians (first 10): [684.   0.   0.   0.  36.   0.  35.   0.  25.   0.]
INFO:preprocessing.structural_feature_extractor_v2:  Feature stds (first 10): [7.0913013e+02 6.7458861e-02 1.0067556e-01 4.9048297e-02 7.6796181e+01
 2.4710934e+00 7.5966492e+01 3.7019184e-01 5.9424965e+01 1.2968451e+00]
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Fitted extractor on 2347 unique test cases
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Build chronology spans 2453 builds
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Extracting 29 features per test case
INFO:preprocessing.structural_feature_extractor_v2_5:âœ“ Saved V2.5 historical state to cache/structural_features_v2_5.pkl
INFO:__main__:  Transforming training data...
INFO:preprocessing.structural_feature_extractor_v2:Transforming 50621 samples into 29 features...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 10000/50621 samples...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 20000/50621 samples...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 30000/50621 samples...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 40000/50621 samples...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 50000/50621 samples...
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Extracted feature matrix: (50621, 29)
INFO:preprocessing.structural_feature_extractor_v2:  Feature means: [8.3890198e+02 2.6135275e-02 2.5333719e-02 2.3399048e-02 6.2583454e+01
 1.2965963e+00 6.1286858e+01 5.8631793e-02 4.2796230e+01 7.9253668e-01]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2:  Feature stds:  [7.0913013e+02 6.7458861e-02 1.0067556e-01 4.9048297e-02 7.6796181e+01
 2.4710934e+00 7.5966492e+01 3.7019184e-01 5.9424965e+01 1.2968451e+00]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2_5:âœ“ Selected 10 features from 29
INFO:preprocessing.structural_feature_extractor_v2_5:  Feature matrix: (50621, 10)
INFO:__main__:  Transforming validation data...
INFO:preprocessing.structural_feature_extractor_v2:Transforming 6062 samples into 29 features...
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Extracted feature matrix: (6062, 29)
INFO:preprocessing.structural_feature_extractor_v2:  Feature means: [2.0384877e+03 2.8686173e-02 2.5544673e-02 2.3697034e-02 5.6465687e+01
 1.2278126e+00 5.5237877e+01 5.9716266e-02 3.8985649e+01 7.6261961e-01]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2:  Feature stds:  [5.7918677e+02 7.7886522e-02 1.0489004e-01 5.1645517e-02 7.5002281e+01
 2.4427924e+00 7.4177452e+01 4.0378672e-01 5.6964211e+01 1.3299178e+00]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2_5:âœ“ Selected 10 features from 29
INFO:preprocessing.structural_feature_extractor_v2_5:  Feature matrix: (6062, 10)
INFO:__main__:  Transforming test data...
INFO:preprocessing.structural_feature_extractor_v2:Transforming 6195 samples into 29 features...
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Extracted feature matrix: (6195, 29)
INFO:preprocessing.structural_feature_extractor_v2:  Feature means: [2.0580222e+03 2.7250437e-02 2.4961963e-02 2.2497460e-02 6.1382404e+01
 1.2986280e+00 6.0083778e+01 5.1977400e-02 4.1303955e+01 8.0258274e-01]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2:  Feature stds:  [5.6877844e+02 7.6100342e-02 1.0366430e-01 4.8286945e-02 7.6021851e+01
 2.5101357e+00 7.5148628e+01 3.3064464e-01 5.8321339e+01 1.3322607e+00]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2_5:âœ“ Selected 10 features from 29
INFO:preprocessing.structural_feature_extractor_v2_5:  Feature matrix: (6195, 10)
INFO:__main__:
1.4b: Imputing missing structural features...
INFO:__main__:  (Uses semantic similarity to estimate features for tests without history)
INFO:__main__:  Validation samples needing imputation: 79/6062
INFO:__main__:  Test samples needing imputation: 51/6195
INFO:__main__:  Imputing validation features...
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:STRUCTURAL FEATURE IMPUTATION
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:Initialized StructuralFeatureImputer with k=10, threshold=0.50
INFO:preprocessing.structural_feature_imputation:Training samples with history: 50621/50621
INFO:preprocessing.structural_feature_imputation:Fitting StructuralFeatureImputer on training data...
INFO:preprocessing.structural_feature_imputation:  Training samples: 50621
INFO:preprocessing.structural_feature_imputation:  Feature means: [ 8.3889862e+02  2.6135398e-02  2.5333358e-02  2.3399513e-02
  5.8631793e-02  7.9253668e-01 -8.0203859e-04  9.3528038e+01
  8.2692951e-02  4.1863060e+01]
INFO:preprocessing.structural_feature_imputation:  Feature stds: [7.0913098e+02 6.7449272e-02 1.0067850e-01 4.9041599e-02 3.7021261e-01
 1.2968485e+00 8.6382419e-02 2.7006015e+02 2.7541754e-01 1.2270127e+02]
INFO:preprocessing.structural_feature_imputation:  Reference tests (with history): 50621
INFO:preprocessing.structural_feature_imputation:
Test samples needing imputation: 79/6062
INFO:preprocessing.structural_feature_imputation:Imputing features for 79/6062 samples...
INFO:preprocessing.structural_feature_imputation:  Imputation complete:
INFO:preprocessing.structural_feature_imputation:    Semantic-based: 79
INFO:preprocessing.structural_feature_imputation:    Fallback (conservative): 0
INFO:preprocessing.structural_feature_imputation:
Imputation Statistics:
INFO:preprocessing.structural_feature_imputation:  Samples imputed: 79/6062 (1.3%)
INFO:preprocessing.structural_feature_imputation:  Feature means before: [508.83544921875, 0.043842706829309464, 0.043274518102407455, 0.0, 0.025316456332802773, 0.025316456332802773, 0.0, 78.40505981445312, 0.7088607549667358, 35.658226013183594]
INFO:preprocessing.structural_feature_imputation:  Feature means after: [1307.6988525390625, 0.03288509324193001, 0.032611340284347534, 0.02586498111486435, 0.16295191645622253, 0.8064466714859009, -0.0006166615639813244, 98.08698272705078, 0.18984225392341614, 44.35611343383789]
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:__main__:  Imputing test features...
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:STRUCTURAL FEATURE IMPUTATION
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:Initialized StructuralFeatureImputer with k=10, threshold=0.50
INFO:preprocessing.structural_feature_imputation:Training samples with history: 50621/50621
INFO:preprocessing.structural_feature_imputation:Fitting StructuralFeatureImputer on training data...
INFO:preprocessing.structural_feature_imputation:  Training samples: 50621
INFO:preprocessing.structural_feature_imputation:  Feature means: [ 8.3889862e+02  2.6135398e-02  2.5333358e-02  2.3399513e-02
  5.8631793e-02  7.9253668e-01 -8.0203859e-04  9.3528038e+01
  8.2692951e-02  4.1863060e+01]
INFO:preprocessing.structural_feature_imputation:  Feature stds: [7.0913098e+02 6.7449272e-02 1.0067850e-01 4.9041599e-02 3.7021261e-01
 1.2968485e+00 8.6382419e-02 2.7006015e+02 2.7541754e-01 1.2270127e+02]
INFO:preprocessing.structural_feature_imputation:  Reference tests (with history): 50621
INFO:preprocessing.structural_feature_imputation:
Test samples needing imputation: 51/6195
INFO:preprocessing.structural_feature_imputation:Imputing features for 51/6195 samples...
INFO:preprocessing.structural_feature_imputation:  Imputation complete:
INFO:preprocessing.structural_feature_imputation:    Semantic-based: 51
INFO:preprocessing.structural_feature_imputation:    Fallback (conservative): 0
INFO:preprocessing.structural_feature_imputation:
Imputation Statistics:
INFO:preprocessing.structural_feature_imputation:  Samples imputed: 51/6195 (0.8%)
INFO:preprocessing.structural_feature_imputation:  Feature means before: [278.686279296875, 0.08085912466049194, 0.08018329739570618, 0.0, 0.05882352963089943, 0.05882352963089943, 0.0, 58.6274528503418, 0.843137264251709, 25.980392456054688]
INFO:preprocessing.structural_feature_imputation:  Feature means after: [1521.680908203125, 0.03773757815361023, 0.038443390280008316, 0.029763756319880486, 0.19205498695373535, 0.9464510679244995, 9.39177189138718e-05, 113.83965301513672, 0.06127665564417839, 53.2897834777832]
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:__main__:
1.6: Building phylogenetic graph...
INFO:__main__:  Using multi-edge graph builder
INFO:phylogenetic.phylogenetic_graph_builder:======================================================================
INFO:phylogenetic.phylogenetic_graph_builder:BUILDING PHYLOGENETIC GRAPH
INFO:phylogenetic.phylogenetic_graph_builder:======================================================================
INFO:phylogenetic.phylogenetic_graph_builder:Using MultiEdgeGraphBuilder with edge types: ['co_failure', 'co_success', 'semantic']
INFO:phylogenetic.multi_edge_graph_builder:Initialized MultiEdgeGraphBuilder
INFO:phylogenetic.multi_edge_graph_builder:  Edge types: ['co_failure', 'co_success', 'semantic']
INFO:phylogenetic.multi_edge_graph_builder:  Edge weights: {'co_failure': 1.0, 'co_success': 0.5, 'semantic': 0.3}
INFO:phylogenetic.phylogenetic_graph_builder:Loading cached multi-edge graph from cache/multi_edge_graph.pkl
INFO:phylogenetic.multi_edge_graph_builder:Multi-edge graph loaded from cache/multi_edge_graph.pkl
INFO:phylogenetic.multi_edge_graph_builder:  Edge types: ['co_failure', 'co_success', 'semantic']
INFO:phylogenetic.multi_edge_graph_builder:  Nodes: 2347
INFO:phylogenetic.multi_edge_graph_builder:  Combined edges: 334946
INFO:phylogenetic.phylogenetic_graph_builder:
======================================================================
INFO:phylogenetic.phylogenetic_graph_builder:MULTI-EDGE PHYLOGENETIC GRAPH STATISTICS
INFO:phylogenetic.phylogenetic_graph_builder:======================================================================
INFO:phylogenetic.phylogenetic_graph_builder:Nodes: 2347
INFO:phylogenetic.phylogenetic_graph_builder:Edges (combined): 334946
INFO:phylogenetic.phylogenetic_graph_builder:Edge type counts: {'co_failure': 495, 'co_success': 207913, 'semantic': 253085}
INFO:phylogenetic.phylogenetic_graph_builder:Density: 0.121664
INFO:phylogenetic.phylogenetic_graph_builder:Avg Degree: 285.42
INFO:phylogenetic.phylogenetic_graph_builder:======================================================================
INFO:__main__:
âœ“ Data preparation complete!
INFO:__main__:
1.7: Extracting graph structure (edge_index and edge_weights)...
INFO:__main__:Graph structure: 187172 edges among 2347 nodes
INFO:__main__:
1.8: Creating TC_Key to global index mapping...
INFO:__main__:  Mapped 2347 unique TC_Keys to global indices (0-2346)
INFO:__main__:  Train: 50621/50621 in graph
INFO:__main__:  Val: 6006/6062 in graph
INFO:__main__:  Test: 6152/6195 in graph
INFO:__main__:
Creating data loaders...
INFO:__main__:Train embeddings shape: (50621, 1536)
INFO:__main__:Train structural features shape: (50621, 10)
INFO:__main__:Train labels shape: (50621,)
INFO:__main__:Train global indices shape: (50621,)
INFO:__main__:
======================================================================
INFO:__main__:STEP 2: MODEL INITIALIZATION
INFO:__main__:======================================================================
WARNING:models.dual_stream_v8:Semantic and structural hidden dims differ (256 vs 64). Using semantic_hidden=256 for both.
INFO:models.dual_stream_v8:Initialized GAT-based StructuralStreamV8:
INFO:models.dual_stream_v8:  - Input: [N, 10] structural features
INFO:models.dual_stream_v8:  - GAT Layer 1: 4 heads, output [N, 1024]
INFO:models.dual_stream_v8:  - GAT Layer 2: 1 head, output [N, 256]
INFO:models.dual_stream_v8:  - Edge weights: True
INFO:models.dual_stream_v8:Using CrossAttentionFusion (default)
INFO:models.dual_stream_v8:======================================================================
INFO:models.dual_stream_v8:DUAL-STREAM MODEL V8 INITIALIZED
INFO:models.dual_stream_v8:======================================================================
INFO:models.dual_stream_v8:Semantic Stream: [batch, 1024] â†’ [batch, 256]
INFO:models.dual_stream_v8:Structural Stream: [batch, 6] â†’ [batch, 256]
INFO:models.dual_stream_v8:Fusion: [batch, 512]
INFO:models.dual_stream_v8:Classifier: [batch, 512] â†’ [batch, 2]
INFO:models.dual_stream_v8:======================================================================
INFO:__main__:
Initializing loss function...
INFO:__main__:  Loss type: weighted_ce
INFO:__main__:  Using Weighted Cross-Entropy
INFO:__main__:    Class weights: [19.13114135  0.51341839]
INFO:__main__:    Weight ratio: 37.26:1
INFO:__main__:Initializing optimizer...
INFO:__main__:
======================================================================
INFO:__main__:STEP 3: TRAINING
INFO:__main__:======================================================================
INFO:evaluation.metrics:
AUPRC (Macro): 0.5072
INFO:evaluation.metrics:AUPRC (Weighted): 0.9500
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.11      0.06      0.08       170
        Pass       0.97      0.99      0.98      5836

    accuracy                           0.96      6006
   macro avg       0.54      0.52      0.53      6006
weighted avg       0.95      0.96      0.95      6006

INFO:__main__:Epoch 1/50: Train Loss=0.8032, Val Loss=0.5520, Val F1=0.5273, Val Acc=0.9592
INFO:__main__:  â†’ New best model saved! (F1=0.5273)
INFO:evaluation.metrics:
AUPRC (Macro): 0.5076
INFO:evaluation.metrics:AUPRC (Weighted): 0.9500
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.01      0.02       170
        Pass       0.97      1.00      0.98      5836

    accuracy                           0.97      6006
   macro avg       0.52      0.50      0.50      6006
weighted avg       0.95      0.97      0.96      6006

INFO:__main__:Epoch 2/50: Train Loss=0.7951, Val Loss=0.6997, Val F1=0.5020, Val Acc=0.9679
INFO:evaluation.metrics:
AUPRC (Macro): 0.5058
INFO:evaluation.metrics:AUPRC (Weighted): 0.9473
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.00      0.00      0.00       170
        Pass       0.97      1.00      0.99      5836

    accuracy                           0.97      6006
   macro avg       0.49      0.50      0.49      6006
weighted avg       0.94      0.97      0.96      6006

INFO:__main__:Epoch 3/50: Train Loss=0.8346, Val Loss=0.6389, Val F1=0.4928, Val Acc=0.9717
INFO:evaluation.metrics:
AUPRC (Macro): 0.5073
INFO:evaluation.metrics:AUPRC (Weighted): 0.9495
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.08      0.01      0.02       170
        Pass       0.97      1.00      0.98      5836

    accuracy                           0.97      6006
   macro avg       0.52      0.50      0.50      6006
weighted avg       0.95      0.97      0.96      6006

INFO:__main__:Epoch 4/50: Train Loss=0.8189, Val Loss=0.6365, Val F1=0.5021, Val Acc=0.9680
INFO:evaluation.metrics:
AUPRC (Macro): 0.5078
INFO:evaluation.metrics:AUPRC (Weighted): 0.9495
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 5/50: Train Loss=0.8163, Val Loss=0.7096, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5074
INFO:evaluation.metrics:AUPRC (Weighted): 0.9496
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 6/50: Train Loss=0.8342, Val Loss=0.6729, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5055
INFO:evaluation.metrics:AUPRC (Weighted): 0.9478
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 7/50: Train Loss=0.8323, Val Loss=0.5590, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5073
INFO:evaluation.metrics:AUPRC (Weighted): 0.9498
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.07       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 8/50: Train Loss=0.7642, Val Loss=0.6715, Val F1=0.5208, Val Acc=0.9527
INFO:evaluation.metrics:
AUPRC (Macro): 0.5081
INFO:evaluation.metrics:AUPRC (Weighted): 0.9500
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 9/50: Train Loss=0.8542, Val Loss=0.6522, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5070
INFO:evaluation.metrics:AUPRC (Weighted): 0.9494
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 10/50: Train Loss=0.8507, Val Loss=0.6152, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5071
INFO:evaluation.metrics:AUPRC (Weighted): 0.9495
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 11/50: Train Loss=0.8007, Val Loss=0.6505, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5057
INFO:evaluation.metrics:AUPRC (Weighted): 0.9481
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 12/50: Train Loss=0.8497, Val Loss=0.5764, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5076
INFO:evaluation.metrics:AUPRC (Weighted): 0.9496
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 13/50: Train Loss=0.7902, Val Loss=0.5868, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5071
INFO:evaluation.metrics:AUPRC (Weighted): 0.9494
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 14/50: Train Loss=0.7696, Val Loss=0.6221, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5068
INFO:evaluation.metrics:AUPRC (Weighted): 0.9492
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.06      0.06       170
        Pass       0.97      0.98      0.98      5836

    accuracy                           0.95      6006
   macro avg       0.52      0.52      0.52      6006
weighted avg       0.95      0.95      0.95      6006

INFO:__main__:Epoch 15/50: Train Loss=0.8456, Val Loss=0.6655, Val F1=0.5200, Val Acc=0.9519
INFO:evaluation.metrics:
AUPRC (Macro): 0.5075
INFO:evaluation.metrics:AUPRC (Weighted): 0.9497
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.07      0.02      0.03       170
        Pass       0.97      0.99      0.98      5836

    accuracy                           0.97      6006
   macro avg       0.52      0.51      0.50      6006
weighted avg       0.95      0.97      0.96      6006

INFO:__main__:Epoch 16/50: Train Loss=0.8450, Val Loss=0.6255, Val F1=0.5050, Val Acc=0.9650
INFO:__main__:Early stopping at epoch 16
INFO:__main__:
Loading best model...
INFO:__main__:
======================================================================
INFO:__main__:STEP 3.5: THRESHOLD OPTIMIZATION
INFO:__main__:======================================================================
INFO:__main__:
Threshold optimization disabled in config - using default threshold 0.5
INFO:__main__:
ðŸ“Š Classification threshold for test evaluation: 0.5000
INFO:__main__:
======================================================================
INFO:__main__:STEP 4: TEST EVALUATION
INFO:__main__:======================================================================
INFO:evaluation.metrics:
AUPRC (Macro): 0.5106
INFO:evaluation.metrics:AUPRC (Weighted): 0.9558
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.17      0.05      0.08       157
        Pass       0.98      0.99      0.98      5995

    accuracy                           0.97      6152
   macro avg       0.57      0.52      0.53      6152
weighted avg       0.95      0.97      0.96      6152

INFO:__main__:
Test Results with default threshold (0.5):
INFO:__main__:  Loss: 0.5777
INFO:__main__:  Accuracy: 0.9693
INFO:__main__:  F1 (Macro): 0.5312
INFO:__main__:  F1 (Weighted): 0.9612
INFO:__main__:  AUPRC (Macro): 0.5106
INFO:__main__:
ðŸ“ Using default threshold (0.5) - threshold optimization was disabled in config
INFO:__main__:
======================================================================
INFO:__main__:STEP 5: APFD CALCULATION
INFO:__main__:======================================================================
INFO:__main__:  Test DataFrame size: 6195
INFO:__main__:  Probabilities array size: (6195, 2)
INFO:__main__:  Samples with predictions: 6152/6195
INFO:__main__:  Orphan samples (not in graph): 43/6195
INFO:__main__:âœ… TE_Test_Result column found with 2 unique values
INFO:__main__:   Values: {'Pass': 6038, 'Fail': 157}
INFO:__main__:   label_binary distribution: {0: 6038, 1: 157}
INFO:__main__:âœ… Build_ID column found: 307 unique builds
INFO:evaluation.apfd:Ranks calculated per build (rank range: 1-190)
INFO:evaluation.apfd:Prioritized test cases saved to: results/experiment_06_feature_selection/prioritized_test_cases.csv
INFO:__main__:âœ… Prioritized test cases saved to: results/experiment_06_feature_selection/prioritized_test_cases.csv
INFO:evaluation.apfd:Calculating APFD for 307 total builds...
INFO:evaluation.apfd:APFD calculated for 64 builds with 'Fail' results
INFO:evaluation.apfd:   Builds included: 64
INFO:evaluation.apfd:   Builds skipped (no failures): 243
INFO:evaluation.apfd:   Expected: 277 builds (as per project requirements)
WARNING:evaluation.apfd:âš ï¸  WARNING: Expected 277 builds but got 64
WARNING:evaluation.apfd:   This may indicate incorrect filtering or data issues
INFO:evaluation.apfd:APFD per-build report saved to: results/experiment_06_feature_selection/apfd_per_build.csv
INFO:__main__:
âœ… APFD per-build report saved to: results/experiment_06_feature_selection/apfd_per_build.csv
INFO:__main__:ðŸ“Š Mean APFD: 0.5576 (across 64 builds)
WARNING:__main__:âš ï¸  WARNING: Expected 277 builds but got 64
WARNING:__main__:   This may indicate incorrect filtering or data issues
INFO:__main__:
======================================================================
INFO:__main__:STEP 6: PROCESSING FULL TEST.CSV FOR FINAL APFD
INFO:__main__:======================================================================
INFO:__main__:
6.1: Loading FULL test.csv...
INFO:preprocessing.data_loader:Loading FULL test dataset from datasets/test.csv
INFO:preprocessing.data_loader:Loaded 31333 test samples from test.csv
INFO:preprocessing.data_loader:Total builds: 1365
INFO:preprocessing.data_loader:Builds with at least one 'Fail': 277 (expected: 277)
INFO:preprocessing.data_loader:Lightweight cleaning for FULL test.csv (no row drops)
INFO:preprocessing.data_loader:Target distribution (non-strict):
TE_Test_Result
Pass                27805
Delete               1970
Fail                  924
Blocked               484
Conditional Pass      130
Pending                20
Name: count, dtype: int64
INFO:preprocessing.data_loader:Binary classification strategy: pass_vs_fail
INFO:preprocessing.data_loader:Positive class: Pass
INFO:preprocessing.data_loader:Excluded samples with labels other than Pass/Fail
INFO:preprocessing.data_loader:Binary label mapping: {'Fail': 0, 'Pass': 1}
INFO:preprocessing.data_loader:Class distribution after encoding:
label
1    27805
0      924
Name: count, dtype: int64
INFO:preprocessing.data_loader:After preprocessing: 28729 samples
INFO:__main__:âœ… Loaded full test.csv:
INFO:__main__:   Total samples: 28729
INFO:__main__:   Total builds: 1339
INFO:__main__:   Builds with 'Fail': 277
INFO:__main__:
6.2: Generating semantic embeddings for full test set...
WARNING:embeddings.embedding_cache:Data has changed since cache was created
INFO:embeddings.embedding_manager:Cache found but invalid (data changed) - regenerating
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:GENERATING EMBEDDINGS
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_manager:Initializing encoder: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:Initializing SBERT encoder on cuda
INFO:embeddings.sbert_encoder:Model: sentence-transformers/all-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2
INFO:embeddings.sbert_encoder:âœ“ Loaded SBERT model on cuda
INFO:embeddings.sbert_encoder:âœ“ Embedding dimension: 768
INFO:embeddings.sbert_encoder:âœ“ Batch size: 128
INFO:embeddings.sbert_encoder:âœ“ Max sequence length: 384
INFO:embeddings.embedding_manager:Preparing texts...
INFO:embeddings.embedding_manager:  Train TCs: 28729
INFO:embeddings.embedding_manager:  Test TCs: 28729
INFO:embeddings.embedding_manager:  Train Commits: 28729
INFO:embeddings.embedding_manager:  Test Commits: 28729
INFO:embeddings.embedding_manager:Encoding...
INFO:embeddings.sbert_encoder:Encoding 28729 texts in 23 chunks (chunk_size=1280)

======================================================================
APFD PER BUILD - SUMMARY STATISTICS
======================================================================
Total builds analyzed: 64
Total test cases: 1235
Mean TCs per build: 19.3

APFD Statistics:
  Mean:   0.5576 â­ PRIMARY METRIC
  Median: 0.5833
  Std:    0.2385
  Min:    0.0312
  Max:    1.0000

APFD Distribution:
  Builds with APFD = 1.0:    2 (  3.1%)
  Builds with APFD â‰¥ 0.7:   17 ( 26.6%)
  Builds with APFD â‰¥ 0.5:   40 ( 62.5%)
  Builds with APFD < 0.5:   24 ( 37.5%)
======================================================================
Train TCs:   0%|          | 0/23 [00:00<?, ?it/s]Train TCs:   4%|â–         | 1/23 [00:00<00:17,  1.25it/s]Train TCs:   9%|â–Š         | 2/23 [00:01<00:11,  1.84it/s]Train TCs:  13%|â–ˆâ–Ž        | 3/23 [00:01<00:09,  2.18it/s]Train TCs:  17%|â–ˆâ–‹        | 4/23 [00:01<00:08,  2.37it/s]Train TCs:  22%|â–ˆâ–ˆâ–       | 5/23 [00:02<00:07,  2.54it/s]Train TCs:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:02<00:06,  2.64it/s]Train TCs:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:02<00:06,  2.66it/s]Train TCs:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:03<00:05,  2.63it/s]Train TCs:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:03<00:05,  2.69it/s]Train TCs:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:04<00:04,  2.74it/s]Train TCs:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:04<00:04,  2.53it/s]Train TCs:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:04<00:04,  2.57it/s]Train TCs:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:05<00:03,  2.59it/s]Train TCs:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:05<00:03,  2.66it/s]Train TCs:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:05<00:02,  2.70it/s]Train TCs:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:06<00:02,  2.68it/s]Train TCs:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:06<00:02,  2.71it/s]Train TCs:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:07<00:01,  2.72it/s]Train TCs:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:07<00:01,  2.75it/s]Train TCs:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:07<00:01,  2.72it/s]Train TCs:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:08<00:00,  2.46it/s]Train TCs:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:08<00:00,  2.50it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:08<00:00,  3.01it/s]Train TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:08<00:00,  2.59it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 28729 texts â†’ shape: (28729, 768)
INFO:embeddings.sbert_encoder:Encoding 28729 texts in 23 chunks (chunk_size=1280)
Test TCs:   0%|          | 0/23 [00:00<?, ?it/s]Test TCs:   4%|â–         | 1/23 [00:00<00:10,  2.12it/s]Test TCs:   9%|â–Š         | 2/23 [00:00<00:08,  2.39it/s]Test TCs:  13%|â–ˆâ–Ž        | 3/23 [00:01<00:07,  2.51it/s]Test TCs:  17%|â–ˆâ–‹        | 4/23 [00:01<00:07,  2.56it/s]Test TCs:  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:06,  2.63it/s]Test TCs:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:02<00:06,  2.65it/s]Test TCs:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:02<00:06,  2.65it/s]Test TCs:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:03<00:05,  2.67it/s]Test TCs:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:03<00:05,  2.65it/s]Test TCs:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:03<00:04,  2.65it/s]Test TCs:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:04<00:04,  2.44it/s]Test TCs:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:04<00:04,  2.52it/s]Test TCs:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:05<00:03,  2.59it/s]Test TCs:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:05<00:03,  2.64it/s]Test TCs:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:05<00:02,  2.68it/s]Test TCs:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:06<00:02,  2.65it/s]Test TCs:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:06<00:02,  2.65it/s]Test TCs:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:06<00:01,  2.67it/s]Test TCs:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:07<00:01,  2.68it/s]Test TCs:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:07<00:01,  2.69it/s]Test TCs:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:08<00:00,  2.48it/s]Test TCs:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:08<00:00,  2.55it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:08<00:00,  3.05it/s]Test TCs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:08<00:00,  2.65it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 28729 texts â†’ shape: (28729, 768)
INFO:embeddings.sbert_encoder:Encoding 28729 texts in 23 chunks (chunk_size=1280)
Train Commits:   0%|          | 0/23 [00:00<?, ?it/s]Train Commits:   4%|â–         | 1/23 [00:00<00:10,  2.07it/s]Train Commits:   9%|â–Š         | 2/23 [00:00<00:08,  2.43it/s]Train Commits:  13%|â–ˆâ–Ž        | 3/23 [00:01<00:13,  1.45it/s]Train Commits:  17%|â–ˆâ–‹        | 4/23 [00:02<00:10,  1.81it/s]Train Commits:  22%|â–ˆâ–ˆâ–       | 5/23 [00:02<00:08,  2.06it/s]Train Commits:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:02<00:07,  2.26it/s]Train Commits:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:03<00:06,  2.42it/s]Train Commits:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:03<00:05,  2.58it/s]Train Commits:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:03<00:05,  2.68it/s]Train Commits:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:04<00:04,  2.76it/s]Train Commits:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:04<00:04,  2.55it/s]Train Commits:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:05<00:04,  2.64it/s]Train Commits:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:05<00:03,  2.72it/s]Train Commits:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:05<00:03,  2.70it/s]Train Commits:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:06<00:03,  2.64it/s]Train Commits:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:06<00:02,  2.56it/s]Train Commits:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:07<00:02,  2.46it/s]Train Commits:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:07<00:01,  2.58it/s]Train Commits:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:07<00:01,  2.69it/s]Train Commits:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:08<00:01,  2.75it/s]Train Commits:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:08<00:00,  2.54it/s]Train Commits:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:08<00:00,  2.63it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:09<00:00,  3.16it/s]Train Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:09<00:00,  2.53it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 28729 texts â†’ shape: (28729, 768)
INFO:embeddings.sbert_encoder:Encoding 28729 texts in 23 chunks (chunk_size=1280)
Test Commits:   0%|          | 0/23 [00:00<?, ?it/s]Test Commits:   4%|â–         | 1/23 [00:00<00:10,  2.15it/s]Test Commits:   9%|â–Š         | 2/23 [00:00<00:08,  2.54it/s]Test Commits:  13%|â–ˆâ–Ž        | 3/23 [00:01<00:07,  2.68it/s]Test Commits:  17%|â–ˆâ–‹        | 4/23 [00:01<00:06,  2.72it/s]Test Commits:  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:06,  2.74it/s]Test Commits:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:02<00:06,  2.79it/s]Test Commits:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:02<00:05,  2.81it/s]Test Commits:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:02<00:05,  2.84it/s]Test Commits:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:03<00:04,  2.87it/s]Test Commits:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:03<00:04,  2.89it/s]Test Commits:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:04<00:04,  2.62it/s]Test Commits:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:04<00:04,  2.74it/s]Test Commits:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:04<00:03,  2.85it/s]Test Commits:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:05<00:03,  2.93it/s]Test Commits:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:05<00:02,  2.99it/s]Test Commits:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:05<00:02,  3.01it/s]Test Commits:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:05<00:01,  3.05it/s]Test Commits:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:06<00:01,  3.07it/s]Test Commits:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:06<00:01,  3.09it/s]Test Commits:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:06<00:00,  3.08it/s]Test Commits:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:07<00:00,  2.79it/s]Test Commits:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:07<00:00,  2.86it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:07<00:00,  3.42it/s]Test Commits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:07<00:00,  2.92it/s]
INFO:embeddings.sbert_encoder:âœ“ Encoded 28729 texts â†’ shape: (28729, 768)
INFO:embeddings.embedding_manager:======================================================================
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:SAVING EMBEDDINGS TO CACHE
INFO:embeddings.embedding_cache:======================================================================
INFO:embeddings.embedding_cache:âœ“ Saved to: cache/embeddings.npz
INFO:embeddings.embedding_cache:âœ“ File size: 2.0 MB
INFO:embeddings.embedding_cache:âœ“ Metadata: cache/embeddings_metadata.txt
INFO:embeddings.embedding_cache:======================================================================
INFO:__main__:âœ… Generated embeddings: (28729, 1536)
INFO:__main__:
6.3: Extracting structural features for full test set...
INFO:preprocessing.structural_feature_extractor_v2:Transforming 28729 samples into 29 features...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 10000/28729 samples...
INFO:preprocessing.structural_feature_extractor_v2:  Processed 20000/28729 samples...
INFO:preprocessing.structural_feature_extractor_v2:âœ“ Extracted feature matrix: (28729, 29)
INFO:preprocessing.structural_feature_extractor_v2:  Feature means: [1.3999716e+03 4.0820688e-02 3.9231516e-02 2.3361176e-02 4.5173241e+01
 1.0892130e+00 4.4084026e+01 9.2937447e-02 3.2538479e+01 7.1906435e-01]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2:  Feature stds:  [9.9024835e+02 1.0719389e-01 1.3053928e-01 5.2720342e-02 6.9496498e+01
 2.2186613e+00 6.9079300e+01 6.3926554e-01 6.1004280e+01 1.3229167e+00]... (showing first 10)
INFO:preprocessing.structural_feature_extractor_v2_5:âœ“ Selected 10 features from 29
INFO:preprocessing.structural_feature_extractor_v2_5:  Feature matrix: (28729, 10)
INFO:__main__:âœ… Extracted structural features: (28729, 10)
INFO:__main__:   Samples needing imputation: 6530/28729
INFO:__main__:   Imputing features...
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:STRUCTURAL FEATURE IMPUTATION
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:preprocessing.structural_feature_imputation:Initialized StructuralFeatureImputer with k=10, threshold=0.50
INFO:preprocessing.structural_feature_imputation:Training samples with history: 50621/50621
INFO:preprocessing.structural_feature_imputation:Fitting StructuralFeatureImputer on training data...
INFO:preprocessing.structural_feature_imputation:  Training samples: 50621
INFO:preprocessing.structural_feature_imputation:  Feature means: [ 8.3889862e+02  2.6135398e-02  2.5333358e-02  2.3399513e-02
  5.8631793e-02  7.9253668e-01 -8.0203859e-04  9.3528038e+01
  8.2692951e-02  4.1863060e+01]
INFO:preprocessing.structural_feature_imputation:  Feature stds: [7.0913098e+02 6.7449272e-02 1.0067850e-01 4.9041599e-02 3.7021261e-01
 1.2968485e+00 8.6382419e-02 2.7006015e+02 2.7541754e-01 1.2270127e+02]
INFO:preprocessing.structural_feature_imputation:  Reference tests (with history): 50621
INFO:preprocessing.structural_feature_imputation:
Test samples needing imputation: 6530/28729
INFO:preprocessing.structural_feature_imputation:Imputing features for 6530/28729 samples...
INFO:preprocessing.structural_feature_imputation:  Imputation complete:
INFO:preprocessing.structural_feature_imputation:    Semantic-based: 6530
INFO:preprocessing.structural_feature_imputation:    Fallback (conservative): 0
INFO:preprocessing.structural_feature_imputation:
Imputation Statistics:
INFO:preprocessing.structural_feature_imputation:  Samples imputed: 6530/28729 (22.7%)
INFO:preprocessing.structural_feature_imputation:  Feature means before: [8.277182579040527, 0.026621462777256966, 0.02582232467830181, 0.0, 0.0006125574000179768, 0.0006125574000179768, 0.0, 64.31025695800781, 0.9950995445251465, 30.172740936279297]
INFO:preprocessing.structural_feature_imputation:  Feature means after: [24.76511573791504, 0.0029941999819129705, 0.0025060689076781273, 0.001830250839702785, 0.0018123174086213112, 0.30766019225120544, -0.0016149415168911219, 2.3352303504943848, 0.9952692985534668, 1.0119158029556274]
INFO:preprocessing.structural_feature_imputation:======================================================================
INFO:__main__:
6.3b: Mapping TC_Keys to global indices...
INFO:__main__:   Samples in training graph: 22231/28729 (77.4%)
INFO:__main__:   Orphan samples: 6498/28729
INFO:__main__:
6.4: Generating predictions on full test set...
INFO:evaluation.metrics:
AUPRC (Macro): 0.5000
INFO:evaluation.metrics:AUPRC (Weighted): 1.0000
INFO:evaluation.metrics:
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       1.00      0.03      0.06     22231
        Pass       0.00      0.00      0.00         0

    accuracy                           0.03     22231
   macro avg       0.50      0.02      0.03     22231
weighted avg       1.00      0.03      0.06     22231

INFO:__main__:âœ… Predictions generated: (28729, 2)
INFO:__main__:   Samples with actual predictions: 22216
INFO:__main__:   Orphan samples (default prob 0.5): 6513
INFO:__main__:
6.5: Preparing data for APFD calculation...
INFO:__main__:   Failures (TE_Test_Result=='Fail'): 924
INFO:__main__:   Passes: 27805
INFO:__main__:
6.6: Generating prioritized test cases CSV...
INFO:evaluation.apfd:Ranks calculated per build (rank range: 1-836)
INFO:evaluation.apfd:Prioritized test cases saved to: results/experiment_06_feature_selection/prioritized_test_cases_FULL_testcsv.csv
INFO:__main__:âœ… Prioritized test cases (FULL) saved to: results/experiment_06_feature_selection/prioritized_test_cases_FULL_testcsv.csv
INFO:__main__:
6.7: Calculating APFD per build on FULL test.csv...
INFO:evaluation.apfd:Calculating APFD for 1339 total builds...
INFO:evaluation.apfd:APFD calculated for 277 builds with 'Fail' results
INFO:evaluation.apfd:   Builds included: 277
INFO:evaluation.apfd:   Builds skipped (no failures): 1062
INFO:evaluation.apfd:   Expected: 277 builds (as per project requirements)
INFO:evaluation.apfd:APFD per-build report saved to: results/experiment_06_feature_selection/apfd_per_build_FULL_testcsv.csv
INFO:__main__:
======================================================================
INFO:__main__:FINAL APFD RESULTS - FULL TEST.CSV (277 BUILDS)
INFO:__main__:======================================================================
INFO:__main__:
======================================================================
INFO:__main__:VALIDATION
INFO:__main__:======================================================================
INFO:__main__:âœ… SUCCESS: Found exactly 277 builds with failures!
INFO:__main__:âœ… Mean APFD: 0.6171
INFO:__main__:
âœ… All results saved to: results/experiment_06_feature_selection/
INFO:__main__:   - prioritized_test_cases.csv (test split)
INFO:__main__:   - apfd_per_build.csv (test split)
INFO:__main__:   - prioritized_test_cases_FULL_testcsv.csv (all 277 builds)
INFO:__main__:   - apfd_per_build_FULL_testcsv.csv (all 277 builds)
INFO:__main__:
======================================================================
INFO:__main__:TRAINING COMPLETE!
INFO:__main__:======================================================================
INFO:__main__:Best Val F1: 0.5273
INFO:__main__:Test F1: 0.5312
INFO:__main__:Mean APFD (test split): 0.5576
INFO:__main__:Mean APFD (FULL test.csv, 277 builds): 0.6171

======================================================================
APFD PER BUILD - SUMMARY STATISTICS
======================================================================
Total builds analyzed: 277
Total test cases: 5085
Mean TCs per build: 18.4

APFD Statistics:
  Mean:   0.6171 â­ PRIMARY METRIC
  Median: 0.6111
  Std:    0.2552
  Min:    0.0278
  Max:    1.0000

APFD Distribution:
  Builds with APFD = 1.0:   23 (  8.3%)
  Builds with APFD â‰¥ 0.7:  113 ( 40.8%)
  Builds with APFD â‰¥ 0.5:  184 ( 66.4%)
  Builds with APFD < 0.5:   93 ( 33.6%)
======================================================================
[W1114 22:26:37.272514397 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
