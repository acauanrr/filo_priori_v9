"""
Intelligent Embedding Cache Manager

Automatically handles embedding generation and caching:
- Detects if embeddings already exist
- Reuses cached embeddings when available
- Supports forced regeneration via flag
- Thread-safe and efficient
"""

import os
import logging
import hashlib
from pathlib import Path
from typing import Dict, Tuple, Optional
import numpy as np
import pandas as pd
from datetime import datetime

logger = logging.getLogger(__name__)


class EmbeddingCache:
    """
    Manages embedding cache with automatic detection and reuse

    Features:
    - Auto-detection of cached embeddings
    - Hash-based validation (detects data changes)
    - Metadata tracking (creation date, data sizes)
    - Force regeneration option
    """

    def __init__(self, cache_dir: str = 'cache'):
        """
        Initialize cache manager

        Args:
            cache_dir: Directory for cache storage
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / 'embeddings.npz'
        self.metadata_file = self.cache_dir / 'embeddings_metadata.txt'

    def _compute_data_hash(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> str:
        """
        Compute hash of data to detect changes

        Args:
            train_df: Training dataframe
            test_df: Test dataframe

        Returns:
            hash_str: MD5 hash of data
        """
        # Hash based on data shape and first/last rows
        hash_input = f"{len(train_df)}_{len(test_df)}"

        # Add sample of data to hash
        if len(train_df) > 0:
            first_row = str(train_df.iloc[0].to_dict())
            last_row = str(train_df.iloc[-1].to_dict())
            hash_input += first_row + last_row

        if len(test_df) > 0:
            first_row = str(test_df.iloc[0].to_dict())
            last_row = str(test_df.iloc[-1].to_dict())
            hash_input += first_row + last_row

        return hashlib.md5(hash_input.encode()).hexdigest()

    def _write_metadata(self, data_hash: str, train_size: int, test_size: int,
                       embedding_dim: int, model_name: str):
        """Write cache metadata"""
        metadata = f"""Embedding Cache Metadata
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Model: {model_name}
Embedding Dimension: {embedding_dim}
Train Samples: {train_size}
Test Samples: {test_size}
Data Hash: {data_hash}
"""
        self.metadata_file.write_text(metadata)

    def _read_metadata(self) -> Optional[Dict]:
        """Read and parse cache metadata"""
        if not self.metadata_file.exists():
            return None

        try:
            content = self.metadata_file.read_text()
            metadata = {}
            for line in content.split('\n'):
                if ':' in line and not line.startswith('Embedding'):
                    key, value = line.split(':', 1)
                    metadata[key.strip()] = value.strip()
            return metadata
        except Exception as e:
            logger.warning(f"Failed to read metadata: {e}")
            return None

    def exists(self) -> bool:
        """Check if cache exists"""
        return self.cache_file.exists() and self.metadata_file.exists()

    def is_valid(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> bool:
        """
        Check if cache is valid for current data

        Args:
            train_df: Current training data
            test_df: Current test data

        Returns:
            valid: True if cache matches current data
        """
        if not self.exists():
            return False

        # Check data hash
        current_hash = self._compute_data_hash(train_df, test_df)
        metadata = self._read_metadata()

        if metadata is None:
            return False

        cached_hash = metadata.get('Data Hash', '')

        if current_hash != cached_hash:
            logger.warning("Data has changed since cache was created")
            return False

        return True

    def load(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, str]:
        """
        Load embeddings from cache

        Returns:
            train_tc_emb: Training TC embeddings
            test_tc_emb: Test TC embeddings
            train_commit_emb: Training commit embeddings
            test_commit_emb: Test commit embeddings
            embedding_dim: Embedding dimension
            model_name: Model name used
        """
        logger.info("="*70)
        logger.info("LOADING CACHED EMBEDDINGS")
        logger.info("="*70)

        if not self.exists():
            raise FileNotFoundError("Cache does not exist")

        # Load embeddings
        data = np.load(str(self.cache_file))

        train_tc_emb = data['train_tc_embeddings']
        test_tc_emb = data['test_tc_embeddings']
        train_commit_emb = data['train_commit_embeddings']
        test_commit_emb = data['test_commit_embeddings']
        embedding_dim = int(data['embedding_dim'])
        model_name = str(data['model_name'])

        logger.info(f"✓ Train TC embeddings: {train_tc_emb.shape}")
        logger.info(f"✓ Test TC embeddings: {test_tc_emb.shape}")
        logger.info(f"✓ Train Commit embeddings: {train_commit_emb.shape}")
        logger.info(f"✓ Test Commit embeddings: {test_commit_emb.shape}")
        logger.info(f"✓ Embedding dimension: {embedding_dim}")
        logger.info(f"✓ Model: {model_name}")

        # Show metadata
        metadata = self._read_metadata()
        if metadata:
            logger.info(f"✓ Generated: {metadata.get('Generated', 'Unknown')}")

        logger.info("="*70)

        return train_tc_emb, test_tc_emb, train_commit_emb, test_commit_emb, embedding_dim, model_name

    def save(self, train_tc_emb: np.ndarray, test_tc_emb: np.ndarray,
             train_commit_emb: np.ndarray, test_commit_emb: np.ndarray,
             embedding_dim: int, model_name: str,
             train_df: pd.DataFrame, test_df: pd.DataFrame):
        """
        Save embeddings to cache

        Args:
            train_tc_emb: Training TC embeddings
            test_tc_emb: Test TC embeddings
            train_commit_emb: Training commit embeddings
            test_commit_emb: Test commit embeddings
            embedding_dim: Embedding dimension
            model_name: Model name
            train_df: Training dataframe (for hash)
            test_df: Test dataframe (for hash)
        """
        logger.info("="*70)
        logger.info("SAVING EMBEDDINGS TO CACHE")
        logger.info("="*70)

        # Save embeddings
        np.savez_compressed(
            str(self.cache_file),
            train_tc_embeddings=train_tc_emb,
            test_tc_embeddings=test_tc_emb,
            train_commit_embeddings=train_commit_emb,
            test_commit_embeddings=test_commit_emb,
            embedding_dim=embedding_dim,
            model_name=model_name
        )

        # Compute hash and save metadata
        data_hash = self._compute_data_hash(train_df, test_df)
        self._write_metadata(
            data_hash=data_hash,
            train_size=len(train_df),
            test_size=len(test_df),
            embedding_dim=embedding_dim,
            model_name=model_name
        )

        file_size_mb = self.cache_file.stat().st_size / (1024 ** 2)

        logger.info(f"✓ Saved to: {self.cache_file}")
        logger.info(f"✓ File size: {file_size_mb:.1f} MB")
        logger.info(f"✓ Metadata: {self.metadata_file}")
        logger.info("="*70)

    def clear(self):
        """Delete cache files"""
        if self.cache_file.exists():
            self.cache_file.unlink()
            logger.info(f"✓ Deleted cache: {self.cache_file}")

        if self.metadata_file.exists():
            self.metadata_file.unlink()
            logger.info(f"✓ Deleted metadata: {self.metadata_file}")

    def info(self) -> str:
        """Get cache information as string"""
        if not self.exists():
            return "Cache: Not found"

        metadata = self._read_metadata()
        if not metadata:
            return "Cache: Exists but metadata missing"

        info = [
            "Cache Information:",
            f"  Location: {self.cache_file}",
            f"  Generated: {metadata.get('Generated', 'Unknown')}",
            f"  Model: {metadata.get('Model', 'Unknown')}",
            f"  Embedding Dim: {metadata.get('Embedding Dimension', 'Unknown')}",
            f"  Train Samples: {metadata.get('Train Samples', 'Unknown')}",
            f"  Test Samples: {metadata.get('Test Samples', 'Unknown')}",
            f"  Size: {self.cache_file.stat().st_size / (1024**2):.1f} MB"
        ]

        return '\n'.join(info)
