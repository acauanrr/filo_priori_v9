# PHASE II: ARCHITECTURAL REFINEMENT - Experiment 016
# Objective: Optimize classification performance (F1 Macro >76%, APFD >57%)
# Changes from Exp 015:
#   - Loss: focal_alpha=[0.15, 0.85], use_class_weights=true, label_smoothing=0.10
#   - Rewiring: k=15, keep_original_ratio=0.2
# This config is designed to be used by run_experiment_016.sh

data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_016_optimized"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_fail"  # Changed from pass_vs_all - only Pass and Fail samples
  use_smote: false
  use_resampling: false

text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

  # Multi-field embedding configuration
  use_multi_field: true  # Enable multi-field embeddings (separate encoding per field)
  field_embedding_dim: 1024  # Dimension of individual field embeddings (BGE-large-en-v1.5 outputs 1024d)

  # Fusion strategy: "projection_concat" (recommended), "weighted", "attention", "hierarchical"
  fusion_strategy: "projection_concat"
  fusion_num_heads: 4  # For attention fusion

  # Field definitions
  fields:
    - name: "summary"
      source: "TE_Summary"
      weight: 0.40  # Weight for weighted fusion (summary is critical)

    - name: "steps"
      source: "TC_Steps"
      weight: 0.10  # Steps less informative for failure prediction

    - name: "commits"
      source: "commit_processed"
      weight: 0.35  # Commits very informative (recent changes)

    - name: "CR"
      source: ["CR_Type_processed", "CR_Component_processed"]
      weight: 0.15  # CR context helpful

phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

  # Rewired graph integration
  use_rewired_graph: true
  rewired_graph_path: "results/experiment_016_optimized/rewiring/rewired_graph.pt"

model:
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"

  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 10
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"
    num_heads: 4
    use_residual: false

    # Denoising disabled by default for rewired scenario
    use_denoising_gate: false
    denoising_gate_type: "mlp"
    denoising_gate_mode: "basic"
    denoising_hard_threshold: null
    denoising_neighbor_dropout: 0.0

  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "concatenation"

  classifier:
    hidden_dim: 128
    num_classes: 2
    dropout: 0.5
    classifier_type: "simple"

training:
  num_epochs: 20       # Increased from 5 for proper training
  batch_size: 64
  learning_rate: 5.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 100
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: false

  loss:
    type: "focal"
    focal_alpha: [0.15, 0.85]  # CHANGED: More weight on Fail class (0.75 → 0.85)
    use_class_weights: true    # CHANGED: Enable class weights (false → true)
    focal_gamma: 2.0
    label_smoothing: 0.10      # CHANGED: Doubled for better calibration (0.05 → 0.10)

  optimizer: "adamw"
  scheduler: "cosine"

  # Ranking-aligned training (pairwise within build)
  ranking:
    enabled: true
    weight: 0.5              # strength of ranking signal
    loss_type: margin        # options: margin|hinge|logistic
    score_type: logit        # options: logit|prob
    margin: 0.5              # for prob, consider 0.05–0.1
    hard_negative_top_k: 5   # hardest Pass per build
    hard_negative_percent: 0.2
    max_pairs_per_build: 100
    metrics:
      ndcg_ks: [5, 10]
      recall_at_percent: [0.1]

  early_stopping:
    patience: 4
    min_delta: 5.0e-4
    monitor: "f1_macro"
    mode: "max"

  save_every_n_epochs: 0
  save_best_only: false

  # NEW: choose how to build edges during training
  graph_mode: "global_rewired"   # options: batch_knn (default), global_rewired

evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"

hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

logging:
  use_tensorboard: false
  log_every_n_steps: 50
  log_dir: "results/experiment_016_optimized/logs"
  save_predictions: true

experiment:
  phase: "II"
  phase_name: "Architectural Refinement"
  experiment_id: "016"
  description: "Optimize F1 Macro & APFD: Better class balance + Improved rewiring (k=15, keep_ratio=0.2)"
