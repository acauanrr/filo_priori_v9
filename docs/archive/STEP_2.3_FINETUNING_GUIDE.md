# Step 2.3: Fine-Tuning BGE for Software Engineering Domain

**Date:** 2025-11-06
**Status:** ðŸš€ READY TO EXECUTE
**Hardware:** Optimized for Quadro RTX 8000 (48GB VRAM)

---

## ðŸ“‹ Overview

This guide covers the contrastive fine-tuning of BGE-Large embeddings to capture domain-specific semantic relationships in software engineering, specifically the relationship between test cases and code changes.

### Scientific Motivation

**Problem with Generic Embeddings (BGE-Large):**
- BGE understands general semantics: "login" â‰ˆ "credentials"
- But misses SE-specific causality: "Fix auth bug" â†’ causes â†’ "Login test fails"
- Research shows domain-specific models can outperform generic BERT by 13% on SE tasks

**Our Solution:**
- Fine-tune BGE with contrastive learning using triplets from test execution history
- Train embedder to recognize: **test case + failure-causing commit = semantically close**
- Expected improvement: 10-15% better semantic similarity for SE domain

---

## ðŸŽ¯ Triplet Strategy

### Triplet Structure
```
Anchor:   Test case text (TE_Summary + TC_Steps)
Positive: Commit text from builds where test FAILED
Negative: Commit text from builds where test PASSED
```

### Example
```
Anchor:   "TE - TC - Login: User login with valid credentials"
Positive: "Fix authentication bug in login module" âœ“ Should be CLOSE
Negative: "Update README documentation"           âœ“ Should be FAR
```

### Training Objective
Minimize distance(anchor, positive) while maximizing distance(anchor, negative).

**Loss Function:** Triplet Loss with margin
```
loss = max(0, d(anchor, positive) - d(anchor, negative) + margin)
```

---

## ðŸ–¥ï¸ Hardware Configuration

### Your Server Specs
```
CPU:  Intel Xeon W-2235 @ 3.80GHz (12 threads)
RAM:  125GB
GPU:  Quadro RTX 8000 (48GB VRAM) ðŸ”¥
Disk: 3.6TB available (/home)
CUDA: 12.2
```

### Optimized Settings
```yaml
batch_size: 96           # Fits comfortably in 48GB VRAM
num_epochs: 5            # Good starting point
learning_rate: 3e-5      # Conservative for fine-tuning
use_amp: true            # Mixed precision for speed
```

**Expected GPU Usage:** 35-40GB VRAM at batch_size=96

---

## ðŸ“‚ Files Created

### Core Implementation
1. **`src/embeddings/triplet_generator.py`** (330 lines)
   - Generates triplets from test execution history
   - Filters tests with sufficient fail/pass history
   - Caches triplets for faster re-runs

2. **`scripts/finetune_bge.py`** (300 lines)
   - Main fine-tuning script using sentence-transformers
   - TripletLoss with cosine distance
   - Checkpointing and validation

3. **`configs/finetune_bge.yaml`** (140 lines)
   - Complete configuration optimized for Quadro RTX 8000
   - Adjustable hyperparameters

4. **`scripts/test_triplet_generation.py`** (80 lines)
   - Quick test script for triplet generation
   - Validates data before full training

---

## ðŸš€ Quick Start

### Step 1: Test Triplet Generation (5 minutes)
```bash
# Test with small sample
python scripts/test_triplet_generation.py

# Expected output:
# - Generated N triplets from M test cases
# - Examples of anchor/positive/negative
# - Saved to cache/triplets_test.csv
```

### Step 2: Quick Fine-Tuning Test (30 minutes)
```bash
# Edit config to use sample
nano configs/finetune_bge.yaml
# Set: sample_size: 10000

# Run fine-tuning
python scripts/finetune_bge.py --config configs/finetune_bge.yaml

# Expected:
# - ~10K-20K triplets generated
# - Training time: ~30 minutes
# - Model saved to: models/finetuned_bge_v1/
```

### Step 3: Full Dataset Fine-Tuning (10-15 hours)
```bash
# Edit config for full dataset
nano configs/finetune_bge.yaml
# Set: sample_size: null  # Use all data

# Run full fine-tuning
nohup python scripts/finetune_bge.py --config configs/finetune_bge.yaml > logs/finetune_full.log 2>&1 &

# Monitor progress
tail -f logs/finetune_full.log

# Or monitor GPU
watch -n 1 nvidia-smi
```

---

## ðŸ“Š Expected Results

### Triplet Generation
```
From ~400K training samples:
- Valid test cases: ~50K-100K (with both fail/pass history)
- Total triplets: ~500K-1M
- Generation time: ~10-15 minutes
- Disk space: ~50-100MB CSV
```

### Training Progress
```
Epoch 1/5:
- Batches: ~10,000 (with batch_size=96, 1M triplets)
- Time per epoch: ~2-3 hours
- GPU usage: 35-40GB / 48GB
- Total training: ~10-15 hours
```

### Validation Results
```
Before fine-tuning (BGE-Large baseline):
  Test case vs Failure commit: similarity ~0.50
  Test case vs Unrelated commit: similarity ~0.45
  Î” = 0.05 (weak discrimination)

After fine-tuning (Expected):
  Test case vs Failure commit: similarity ~0.75
  Test case vs Unrelated commit: similarity ~0.40
  Î” = 0.35 (strong discrimination) âœ“
```

---

## ðŸ”„ Integration with V8 Pipeline

### After Fine-Tuning is Complete

1. **Update V8 Configuration:**
```yaml
# configs/experiment_v8_baseline.yaml

semantic:
  model_name: "models/finetuned_bge_v1"  # Use fine-tuned model
  # Old: model_name: "BAAI/bge-large-en-v1.5"

  embedding_dim: 1024
  # ... rest stays the same
```

2. **Run V8 Training with Fine-Tuned Embeddings:**
```bash
python main_v8.py --config configs/experiment_v8_baseline.yaml --device cuda
```

3. **Compare Results:**
```
V8 with BGE-Large (baseline):
  Test F1 Macro: 0.55
  Mean APFD: 0.60

V8 with Fine-Tuned BGE (expected):
  Test F1 Macro: 0.60-0.65 (+5-10pp)
  Mean APFD: 0.65-0.70 (+5-10pp)
```

---

## ðŸ“ˆ Monitoring Training

### GPU Monitoring
```bash
# Real-time GPU usage
watch -n 1 nvidia-smi

# Expected output:
# GPU: Quadro RTX 8000
# Memory: 35GB / 48GB
# Utilization: 95-100%
# Temperature: 70-85Â°C
```

### Training Logs
```bash
# View training progress
tail -f logs/finetune_bge.log

# Check for errors
grep "ERROR" logs/finetune_bge.log

# Check epoch progress
grep "Epoch" logs/finetune_bge.log
```

### TensorBoard (Optional)
```bash
# Start TensorBoard
tensorboard --logdir runs/finetune_bge_v1

# Open browser: http://localhost:6006
# View: loss curves, learning rate schedule
```

---

## âš™ï¸ Hyperparameter Tuning

### Key Parameters to Adjust

#### 1. Batch Size
```yaml
batch_size: 96   # Default for 48GB VRAM
# Increase if GPU underutilized: 128, 160
# Decrease if OOM: 64, 48
```

#### 2. Number of Epochs
```yaml
num_epochs: 5    # Good starting point
# Increase if loss not plateaued: 7, 10
# Decrease if overfitting: 3
```

#### 3. Learning Rate
```yaml
learning_rate: 3e-5   # Conservative
# Increase for faster convergence: 5e-5
# Decrease if training unstable: 2e-5, 1e-5
```

#### 4. Triplet Margin
```yaml
triplet_margin: 0.5   # Standard
# Increase for stricter separation: 0.7, 1.0
# Decrease if training too slow: 0.3
```

#### 5. Triplets per Test
```yaml
max_triplets_per_test: 10   # Balanced
# Increase for more data: 15, 20
# Decrease for faster generation: 5
```

---

## ðŸ› Troubleshooting

### Issue: Out of Memory (OOM)
```
Error: CUDA out of memory
```
**Solution:**
```yaml
# Reduce batch size
batch_size: 64  # or 48, 32
```

### Issue: Training Too Slow
```
Expected: 2-3 hours/epoch
Actual: 5+ hours/epoch
```
**Solution:**
```yaml
# Reduce data or increase batch size
max_triplets_per_test: 5  # Generate fewer triplets
batch_size: 128           # Use larger batches
```

### Issue: No Triplets Generated
```
Error: No triplets generated! Check your data...
```
**Solution:**
- Check data has both Pass and Fail results
- Reduce `min_fail_builds` and `min_pass_builds` to 1
- Ensure commit field is populated

### Issue: Loss Not Decreasing
```
Epoch 1: Loss = 1.5
Epoch 2: Loss = 1.5
Epoch 3: Loss = 1.5
```
**Solution:**
```yaml
# Increase learning rate
learning_rate: 5e-5  # from 3e-5

# Check data quality
# Ensure triplets are meaningful
```

---

## ðŸ“Š Experiment Tracking

### Comparison Matrix

| Configuration | Triplets | Epochs | Time | Val Similarity | Notes |
|--------------|----------|--------|------|----------------|-------|
| Quick Test | 10K | 3 | 30min | TBD | Initial validation |
| Medium | 100K | 5 | 4h | TBD | Good balance |
| Full | 1M | 5 | 15h | TBD | Best performance |
| Full + More Epochs | 1M | 10 | 30h | TBD | If underfitting |

### Save Results
```bash
# After each run, copy model and logs
cp -r models/finetuned_bge_v1 models/finetuned_bge_v1_run1
cp logs/finetune_bge.log logs/finetune_bge_run1.log

# Document in experiments log
echo "Run 1: 1M triplets, 5 epochs, lr=3e-5" >> experiments.log
```

---

## ðŸŽ“ Scientific Contribution

### Novel Aspects

1. **Domain-Specific Contrastive Learning for TCP**
   - First application of triplet learning for test case prioritization
   - Novel triplet generation strategy from test execution history

2. **Causality-Aware Embeddings**
   - Embeddings capture causal relationship: commit â†’ test failure
   - Goes beyond semantic similarity to functional impact

3. **End-to-End SE-Specific Pipeline**
   - Fine-tuned embeddings + structural features
   - Complete solution for TCP with domain knowledge

### Publication Potential

**Title Ideas:**
1. "Domain-Specific Contrastive Learning for Test Case Prioritization"
2. "Causality-Aware Embeddings for Software Testing"
3. "Fine-Tuning Transformers for Test-Code Semantic Relationships"

**Expected Results:**
- 10-15% improvement over generic BGE
- Ablation study: Generic BGE vs Fine-tuned BGE vs Fine-tuned BGE + Structural
- Qualitative analysis: Semantic similarity examples

---

## ðŸ“š References

1. **CodeBERT:** Feng et al. "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
2. **SE-BERT:** Wang et al. "Domain-Specific BERT for Software Engineering Tasks"
3. **Contrastive Learning:** Chen et al. "A Simple Framework for Contrastive Learning"
4. **Triplet Loss:** Schroff et al. "FaceNet: A Unified Embedding for Face Recognition"

---

## âœ… Checklist

### Pre-Training
- [ ] Install sentence-transformers: `pip install sentence-transformers`
- [ ] Test triplet generation: `python scripts/test_triplet_generation.py`
- [ ] Verify GPU available: `nvidia-smi`
- [ ] Create directories: `mkdir -p models logs cache`

### Quick Test (30 minutes)
- [ ] Set sample_size=10000 in config
- [ ] Run: `python scripts/finetune_bge.py --config configs/finetune_bge.yaml`
- [ ] Check model saved: `ls models/finetuned_bge_v1/`
- [ ] Validate similarity improvements

### Full Training (10-15 hours)
- [ ] Set sample_size=null in config
- [ ] Run in background: `nohup python scripts/finetune_bge.py ... &`
- [ ] Monitor GPU: `watch -n 1 nvidia-smi`
- [ ] Check logs: `tail -f logs/finetune_bge.log`
- [ ] Wait for completion

### Integration
- [ ] Update experiment_v8_baseline.yaml with fine-tuned model path
- [ ] Run V8 training: `python main_v8.py ...`
- [ ] Compare metrics: Fine-tuned vs Baseline
- [ ] Document improvements

---

## ðŸŽ¯ Next Steps

1. **Run Quick Test** (30 min) to validate pipeline
2. **Launch Full Training** (overnight) on server
3. **Integrate with V8** and compare results
4. **Document improvements** for publication
5. **Consider CodeBERT** as alternative base model

---

**Status:** ðŸš€ **READY TO EXECUTE**

**Command to start:**
```bash
# Quick test first
python scripts/test_triplet_generation.py

# Then full fine-tuning
nohup python scripts/finetune_bge.py --config configs/finetune_bge.yaml > logs/finetune_full.log 2>&1 &
```

**Expected completion:** 10-15 hours from start

---

*For questions or issues, check logs/finetune_bge.log or contact the team.*
