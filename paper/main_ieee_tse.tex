%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FILO-PRIORI V9 - IEEE TRANSACTIONS ON SOFTWARE ENGINEERING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Target: IEEE Transactions on Software Engineering (IEEE TSE)
% Template: IEEEtran (IEEE Computer Society Transactions)
%
% Structure:
%   - sections/introduction.tex
%   - sections/background.tex
%   - sections/related_work.tex
%   - sections/approach.tex
%   - sections/experimental_design.tex
%   - sections/results.tex
%   - sections/discussion.tex
%   - sections/threats.tex
%   - sections/conclusion.tex
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,journal,compsoc]{IEEEtran}

%------------------------------------------------------------------------------
% PACKAGES
%------------------------------------------------------------------------------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{balance}

% For code listings
\usepackage{listings}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  breaklines=true,
  frame=single
}

% For subfigures
\usepackage{subcaption}

% For table notes
\usepackage{threeparttable}

% For colored boxes (findings summary)
\usepackage[most]{tcolorbox}

%------------------------------------------------------------------------------
% CUSTOM COMMANDS
%------------------------------------------------------------------------------
\newcommand{\filopriori}{\textsc{Filo-Priori}}
\newcommand{\apfd}{\textsc{APFD}}
\newcommand{\gatv}{GATv2}

% Highlight for revision
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\rev}[1]{\textcolor{blue}{#1}}

%------------------------------------------------------------------------------
% DOCUMENT START
%------------------------------------------------------------------------------
\begin{document}

%------------------------------------------------------------------------------
% TITLE
%------------------------------------------------------------------------------
\title{Filo-Priori: A Dual-Stream Deep Learning Approach to Test Case Prioritization}

%------------------------------------------------------------------------------
% AUTHORS
%------------------------------------------------------------------------------
\author{
  \IEEEauthorblockN{Acauan C. Ribeiro}
  \IEEEauthorblockA{
    Instituto de Computa\c{c}\~{a}o (IComp)\\
    Universidade Federal do Amazonas (UFAM)\\
    Manaus, AM, Brazil\\
    acauan@icomp.ufam.edu.br
  }
}

%------------------------------------------------------------------------------
% HEADERS
%------------------------------------------------------------------------------
\markboth{IEEE Transactions on Software Engineering, Vol. XX, No. X, Month 2026}%
{Ribeiro: Filo-Priori: Deep Learning-based Test Case Prioritization}

%------------------------------------------------------------------------------
% ABSTRACT
%------------------------------------------------------------------------------
\IEEEtitleabstractindextext{%
\begin{abstract}
Test Case Prioritization (TCP) aims to order test cases to maximize early fault
detection in Continuous Integration (CI) environments. Existing approaches treat
software versions as linear time series, failing to capture the complex relationships
between test cases that characterize real-world testing. We propose a
\textbf{dual-stream architecture} that combines semantic understanding of test
cases with structural patterns learned from execution history.

We present \filopriori{}, a deep learning approach that introduces:
(1) a \textbf{Multi-Edge Test Relationship Graph} encoding co-failure,
co-success, and semantic similarity relationships between test cases;
(2) a \textbf{Dual-Stream Architecture} combining semantic embeddings (SBERT)
with Graph Attention Networks (GAT) for structural feature learning;
(3) \textbf{Cross-Attention Fusion} for dynamic modality combination; and
(4) \textbf{Weighted Focal Loss} for handling severe class imbalance (37:1).

We evaluate \filopriori{} on two complementary datasets: an industrial dataset
(52,102 test executions, 277 builds with failures) and the RTPTorrent open-source
benchmark (20 Java projects, 1,250 test builds). On the industrial dataset, results
show a mean APFD of 0.6413, representing 14.6\% improvement over random ordering
($p < 0.001$) and 2.0\% over the strongest baseline. On RTPTorrent, \filopriori{}
achieves APFD of 0.8376 across all 20 projects, outperforming 6 of 7 baselines
including the strongest heuristic (recently\_failed) by +2.02\%. Ablation reveals
Graph Attention contributes +17.0\% to performance. Temporal validation confirms
robustness (APFD: 0.619--0.663).

This work demonstrates the effectiveness of combining semantic and structural
information for TCP through graph neural networks. Our replication package is publicly available.
\end{abstract}

%------------------------------------------------------------------------------
% KEYWORDS
%------------------------------------------------------------------------------
\begin{IEEEkeywords}
Test Case Prioritization, Graph Attention Networks, Deep Learning,
Continuous Integration, Dual-Stream Architecture, Class Imbalance
\end{IEEEkeywords}
}

\maketitle

%------------------------------------------------------------------------------
% SECTION 1: INTRODUCTION
%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{C}{ontinuous} Integration (CI) has become a fundamental practice
in modern software development, enabling teams to integrate code changes frequently
and detect defects early~\cite{hilton2016usage, fowler2006continuous}. A key
challenge in CI environments is managing the growing test suite: as software
evolves, the number of test cases increases, making it impractical to execute
all tests for every commit~\cite{memon2017taming}.

Test Case Prioritization (TCP) addresses this challenge by ordering test cases
to maximize early fault detection~\cite{rothermel2001prioritizing, elbaum2002test}.
The goal is to execute tests most likely to fail first, providing faster feedback
to developers. The effectiveness of TCP is typically measured using the Average
Percentage of Faults Detected (APFD) metric~\cite{rothermel1999test}.

\textbf{The Challenge of Test Relationships.} Existing TCP approaches, whether based
on coverage~\cite{rothermel2001prioritizing}, historical failure~\cite{kim2002history},
or machine learning~\cite{spieker2017reinforcement, pan2022test}, often treat
test cases as independent entities. This assumption ignores the rich relationships
between test cases: tests that fail together often indicate related functionality,
tests with similar descriptions target similar code, and historical patterns
reveal systematic dependencies~\cite{german2009evolution}.

\textbf{A Dual-Stream Approach.} We propose combining two complementary information
sources for TCP: (1) \emph{semantic information} from test descriptions and commit
messages, and (2) \emph{structural information} from test execution history and
test relationships. This dual-stream approach captures both what tests do
(semantics) and how they behave (structure).

Table~\ref{tab:feature_types} presents the two types of features we combine:

\begin{table}[h]
\centering
\caption{Dual-Stream Feature Types}
\label{tab:feature_types}
\begin{tabular}{ll}
\toprule
\textbf{Semantic Features} & \textbf{Structural Features} \\
\midrule
Test case summary & Historical failure rate \\
Test case steps & Recent failure trend \\
Commit messages & Test age (builds since first run) \\
Code changes (diff) & Flakiness rate \\
-- & Co-failure relationships \\
-- & Consecutive failure streaks \\
\bottomrule
\end{tabular}
\end{table}

The key insight is that semantic similarity alone is insufficient---tests with
similar descriptions may have very different failure patterns. By combining
semantic and structural information through Graph Attention Networks, we can
learn which tests are likely to fail based on both their content and their history.

In this paper, we present \filopriori{}, a deep learning approach for
Test Case Prioritization that combines semantic and structural information through
four key innovations:

\begin{enumerate}
    \item \textbf{Multi-Edge Test Relationship Graph}: We construct a graph that
    captures multiple types of relationships between test cases: co-failure edges
    (tests that fail together), co-success edges (tests that pass together), and
    semantic similarity edges (tests with similar descriptions). This multi-edge
    approach increases graph density and captures complementary relationships.

    \item \textbf{Dual-Stream Architecture}: We employ a dual-stream model that
    processes semantic features (SBERT embeddings of test descriptions and commit
    messages) and structural features (historical execution patterns) through
    separate neural networks before fusion.

    \item \textbf{Graph Attention Networks for Structural Learning}: We use Graph
    Attention Networks (GAT)~\cite{velickovic2018graph} to learn representations
    that capture test relationships, allowing the model to propagate information
    between related test cases with learned attention weights.

    \item \textbf{Weighted Focal Loss for Class Imbalance}: We employ Weighted
    Focal Loss~\cite{lin2017focal} to address the severe class imbalance (37:1
    Pass:Fail ratio) inherent in test execution data, focusing training on
    hard-to-classify examples while down-weighting easy negatives.
\end{enumerate}

We evaluate \filopriori{} on an industrial dataset containing 277 builds with
at least one failing test, totaling 52,102 test executions from 2,347 unique
test cases. Our evaluation addresses four research questions:

\begin{itemize}
    \item \textbf{RQ1}: How effective is \filopriori{} compared to baseline methods?
    \item \textbf{RQ2}: What is the contribution of each architectural component?
    \item \textbf{RQ3}: How robust is \filopriori{} across different time periods?
    \item \textbf{RQ4}: How sensitive is \filopriori{} to hyperparameter choices?
\end{itemize}

Our results show that \filopriori{} achieves a mean APFD of 0.6413, significantly
outperforming random ordering by 14.6\% ($p < 0.001$) and the strongest baseline
(FailureRate) by 2.0\%. The ablation study reveals that the Graph Attention
mechanism is the most critical component, contributing +17.0\% to performance.

\textbf{Contributions.} This paper makes the following contributions:
\begin{itemize}
    \item \textbf{Architectural}: We propose a dual-stream neural architecture
    that combines semantic embeddings (SBERT) with Graph Attention Networks for
    learning test relationships, using cross-attention fusion for modality combination.
    \item \textbf{Graph Construction}: We introduce a multi-edge test relationship
    graph that captures co-failure, co-success, and semantic similarity relationships,
    providing richer structure than single-edge approaches.
    \item \textbf{Feature Engineering}: We identify 10 discriminative structural
    features from an initial set of 29, selected through importance analysis and
    correlation filtering.
    \item \textbf{Empirical}: We demonstrate effectiveness on an industrial dataset,
    achieving 14.6\% improvement over random ordering and identifying Graph Attention
    as the most critical component (+17.0\% contribution).
    \item \textbf{Practical}: We provide a complete replication package enabling
    reproducibility and extension of our approach.
\end{itemize}

\textbf{Paper Organization.} Section~\ref{sec:background} presents background
concepts. Section~\ref{sec:related} discusses related work. Section~\ref{sec:approach}
describes our approach. Section~\ref{sec:experimental} presents the experimental
design. Section~\ref{sec:results} reports results. Section~\ref{sec:discussion}
discusses findings. Section~\ref{sec:threats} addresses threats to validity.
Section~\ref{sec:conclusion} concludes.

%------------------------------------------------------------------------------
% SECTION 2: BACKGROUND
%------------------------------------------------------------------------------
\section{Background}
\label{sec:background}

This section introduces fundamental concepts underlying our approach.

\subsection{Test Case Prioritization}
\label{sec:bg_tcp}

Test Case Prioritization (TCP) is the process of ordering test cases for execution
to achieve certain objectives, such as maximizing early fault detection~\cite{rothermel2001prioritizing}.
Formally, given a test suite $T$ and a permutation function $PT$ that produces
ordered sequences of $T$, TCP aims to find an optimal ordering $T' \in PT$ that
maximizes a given objective function~\cite{elbaum2002test}.

In Continuous Integration environments, TCP is particularly important because:
(1) test suites grow over time, making exhaustive testing impractical~\cite{memon2017taming},
(2) developers need rapid feedback on code changes~\cite{hilton2016usage}, and
(3) computing resources for testing are often limited~\cite{spieker2017reinforcement}.

\subsection{APFD Metric}
\label{sec:bg_apfd}

The Average Percentage of Faults Detected (APFD) is the standard metric for
evaluating TCP effectiveness~\cite{rothermel1999test}. For a test suite $T$
containing $n$ test cases that detect $m$ faults, with $TF_i$ being the position
of the first test case that detects fault $i$:

\begin{equation}
    \text{APFD} = 1 - \frac{\sum_{i=1}^{m} TF_i}{n \times m} + \frac{1}{2n}
\end{equation}

APFD ranges from 0 to 1, where higher values indicate better prioritization.
An APFD of 0.5 corresponds to random ordering, while 1.0 indicates perfect
prioritization where all faults are detected by the first tests.

\subsection{Graph Attention Networks}
\label{sec:bg_gat}

Graph Attention Networks (GAT)~\cite{velickovic2018graph} extend Graph Neural
Networks by incorporating attention mechanisms to weigh the importance of
neighboring nodes. For a node $i$ with neighbors $\mathcal{N}_i$, GAT computes:

\begin{equation}
    \vec{h}_i' = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W} \vec{h}_j\right)
\end{equation}

where $\alpha_{ij}$ are attention coefficients computed as:

\begin{equation}
    \alpha_{ij} = \text{softmax}_j\left(\text{LeakyReLU}\left(\vec{a}^T[\mathbf{W}\vec{h}_i \| \mathbf{W}\vec{h}_j]\right)\right)
\end{equation}

Brody et al.~\cite{brody2022attentive} showed that standard GAT computes a
restricted form of ``static'' attention where the ranking of attention scores
is independent of the query node. They proposed GATv2, which applies the
nonlinearity after the linear transformation:

\begin{equation}
    \alpha_{ij} = \text{softmax}_j\left(\vec{a}^T \cdot \text{LeakyReLU}\left(\mathbf{W}[\vec{h}_i \| \vec{h}_j]\right)\right)
\end{equation}

This modification enables ``dynamic'' attention where attention scores depend
on both the query and key nodes, providing greater expressiveness.

\subsection{Class Imbalance and Focal Loss}
\label{sec:bg_focal}

Test execution data typically exhibits severe class imbalance, with far more
passing tests than failing tests. Standard cross-entropy loss is dominated by
the majority class, leading to models that predict ``pass'' for nearly all tests.

\textbf{Focal Loss}~\cite{lin2017focal} addresses this by down-weighting easy
examples and focusing on hard ones:

\begin{equation}
    \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

where $p_t$ is the predicted probability of the true class, $\alpha_t$ is a
class-balancing weight, and $\gamma$ is the focusing parameter. When $\gamma > 0$,
the $(1 - p_t)^\gamma$ term reduces the loss for well-classified examples,
focusing training on hard negatives.

\subsection{Cross-Attention Mechanisms}
\label{sec:bg_cross_attn}

Cross-attention allows one sequence (or modality) to attend to another, enabling
information exchange between different representations. Given queries $Q$ from
one modality and keys/values $K, V$ from another:

\begin{equation}
    \text{CrossAttn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

In multi-modal fusion, bidirectional cross-attention allows each modality to
selectively attend to relevant parts of the other, combining information in a
learned, adaptive manner.

%------------------------------------------------------------------------------
% SECTION 3: RELATED WORK
%------------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

We conducted a systematic literature review following the guidelines of
Kitchenham and Charters~\cite{kitchenham2007guidelines}. Our search targeted
IEEE Xplore and ACM Digital Library using the query: \texttt{(Test Case
Prioritization OR Regression Testing) AND (Graph Neural Network OR Deep Learning
OR Software Evolution)}. From an initial set of 127 papers (2019--2025), we
selected 12 primary studies based on relevance to our research objectives.
Table~\ref{tab:rsl_studies} summarizes these studies across three themes:
(1) GNNs for software engineering, (2) deep learning for TCP, and
(3) software evolution analysis.

\input{tables/tab_rsl_studies}

\subsection{Traditional TCP Approaches}

Early TCP research focused on code coverage-based techniques. Rothermel et al.~\cite{rothermel2001prioritizing}
proposed total and additional coverage prioritization, while Elbaum et al.~\cite{elbaum2002test}
conducted extensive empirical studies comparing different strategies.

History-based approaches leverage past test execution results. Kim and Porter~\cite{kim2002history}
proposed using historical failure information, showing that tests that failed
recently are more likely to fail again. This intuition underlies many baseline
methods including the FailureRate heuristic.

\subsection{Machine Learning for TCP}

Machine learning has been increasingly applied to TCP. Spieker et al.~\cite{spieker2017reinforcement}
introduced RETECS, using reinforcement learning for TCP in CI environments.
Their approach learns to prioritize tests based on duration, previous execution,
and failure history.

Bertolino et al.~\cite{bertolino2020learning} compared learning-to-rank and
ranking-to-learn strategies, demonstrating the importance of aligning training
objectives with evaluation metrics.

Bagherzadeh et al.~\cite{bagherzadeh2022reinforcement} extended reinforcement
learning approaches with improved reward functions and demonstrated effectiveness
on industrial datasets.

\subsection{Deep Learning for TCP}

Deep learning approaches have shown promising results. Pan et al.~\cite{pan2022test}
used neural networks to learn test case representations from historical data.
Chen et al.~\cite{chen2023deeporder} proposed DeepOrder, using deep neural
networks for TCP in CI environments.

TCP-Net~\cite{abdelkarim2022tcp} introduced an end-to-end deep neural network
approach that learns directly from test execution data. Recent work by
Khan et al.~\cite{khan2024hyperparameter} demonstrated the importance of
hyperparameter optimization in ML-based TCP.

\subsection{Graph Neural Networks in Software Engineering}

GNNs have been applied to various software engineering tasks.
Allamanis et al.~\cite{allamanis2018learning} used GNNs for program representation.
In testing, GraphPrior~\cite{wang2023graphprior} applied GNNs for test input
prioritization in DNN testing. Lou et al.~\cite{lou2024towards} applied GNNs
to fault localization with enhanced code representations.

For temporal graphs, Rossi et al.~\cite{rossi2020temporal} proposed Temporal
Graph Networks for dynamic graphs, and Xu et al.~\cite{xu2020inductive} introduced
inductive methods for temporal representations.

\subsection{Software Evolution Analysis}

Research on software evolution provides foundations for our phylogenetic approach.
German et al.~\cite{german2009evolution} introduced change impact graphs for
analyzing the effects of prior code changes. Godfrey and Zou~\cite{godfrey2005evolution}
developed origin analysis for detecting entity evolution across versions.
Dig and Johnson~\cite{dig2006automated} automated detection of refactorings in
evolving components.

Recent work on code intelligence~\cite{niu2024deeplearning, feng2020codebert,
guo2021graphcodebert} provides semantic understanding of code that complements
evolutionary analysis. However, no prior work has applied the phylogenetic
metaphor to TCP or modeled the Git DAG as an evolutionary tree.

\subsection{Comparison with Our Approach}

\filopriori{} differs from prior work in several key aspects (Table~\ref{tab:comparison}):

\begin{table}[h]
\centering
\caption{Comparison with Related Approaches}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Approach} & \textbf{Git DAG} & \textbf{GNN} & \textbf{Ranking} \\
\midrule
RETECS~\cite{spieker2017reinforcement} & No & No & RL \\
DeepOrder~\cite{chen2023deeporder} & No & No & DL \\
NodeRank~\cite{vansoest2024noderank} & No & Yes & Heuristic \\
GraphPrior~\cite{wang2023graphprior} & No & Yes & Mutation \\
\textbf{\filopriori{}} & \textbf{Yes} & \textbf{Yes} & \textbf{LTR} \\
\bottomrule
\end{tabular}
\end{table}

Key differentiators:
\begin{itemize}
    \item \textbf{Phylogenetic modeling}: We are the first to treat the Git DAG
    as a phylogenetic tree, enabling principled distance-weighted propagation.
    \item \textbf{Hierarchical attention}: We introduce multi-scale attention
    (micro/meso/macro) not found in prior TCP approaches.
    \item \textbf{Evolutionary regularization}: Our loss function includes a
    phylogenetic regularization term penalizing predictions inconsistent with
    evolutionary structure.
\end{itemize}

%------------------------------------------------------------------------------
% SECTION 4: APPROACH
%------------------------------------------------------------------------------
\section{Approach: \filopriori{}}
\label{sec:approach}

This section describes the \filopriori{} approach for Test Case Prioritization.

\subsection{Overview}

The \filopriori{} architecture is a dual-stream system that
takes as input: (1) test case descriptions and commit messages (semantic),
(2) historical test execution patterns (structural), and (3) test relationship
graph. It outputs a ranking of test cases by predicted failure probability.

The approach consists of three main modules:
\begin{enumerate}
    \item \textbf{Semantic Stream}: A feed-forward network that processes SBERT
    embeddings of test descriptions and commit messages, capturing textual
    similarity between test cases and code changes.
    \item \textbf{Structural Stream}: A Graph Attention Network (GAT) that processes
    historical features (failure rate, recency, flakiness) over the test relationship
    graph, learning to propagate failure signals between related tests.
    \item \textbf{Cross-Attention Fusion}: Bidirectional cross-attention that
    fuses semantic and structural representations, allowing each modality to
    attend to the other for final classification.
\end{enumerate}

This dual-stream design captures both what tests do (semantics) and how they
behave (structure), combining complementary information sources.

\subsection{Semantic Feature Extraction}

We use Sentence-BERT (SBERT)~\cite{reimers2019sentence} with the \texttt{all-mpnet-base-v2}
model to encode textual information. For each test case, we concatenate:
\begin{itemize}
    \item Test case summary (TC\_Summary)
    \item Test case steps (TC\_Steps)
\end{itemize}

For each commit, we encode:
\begin{itemize}
    \item Commit message
    \item Code diff (truncated to 2000 characters)
\end{itemize}

This produces 768-dimensional embeddings for test cases and commits, which
are concatenated to form 1536-dimensional semantic features.

\subsection{Structural Feature Extraction}

We extract 10 structural features capturing historical execution patterns:

\begin{enumerate}
    \item \textbf{test\_age}: Number of builds since first appearance
    \item \textbf{failure\_rate}: Historical failure percentage
    \item \textbf{recent\_failure\_rate}: Failure rate in last 5 builds
    \item \textbf{flakiness\_rate}: Pass/fail oscillation frequency
    \item \textbf{commit\_count}: Number of associated commits
    \item \textbf{test\_novelty}: Binary flag for first appearance
    \item \textbf{consecutive\_failures}: Current failure streak
    \item \textbf{max\_consecutive\_failures}: Maximum observed streak
    \item \textbf{failure\_trend}: Trend analysis (-1/0/+1)
    \item \textbf{cr\_count}: Associated change request count
\end{enumerate}

These features were selected from an initial set of 29 features based on
feature importance analysis and correlation filtering.

\subsection{Test Relationship Graph Construction}

We construct a multi-edge graph where nodes represent test cases and edges
capture different types of relationships:

\textbf{Edge Types}: We define three complementary edge types:
\begin{enumerate}
    \item \textbf{Co-Failure Edges} (weight 1.0): Connect tests that fail
    together in the same build, capturing fault-related dependencies. If tests
    $t_i$ and $t_j$ both fail in build $b$, we add edge $(t_i, t_j)$.
    \item \textbf{Co-Success Edges} (weight 0.5): Connect tests that pass
    together, capturing functional similarity. Lower weight reflects that
    passing together is less informative than failing together.
    \item \textbf{Semantic Edges} (weight 0.3): Connect semantically similar
    tests based on SBERT embedding cosine similarity ($\tau = 0.75$). This
    captures tests targeting similar functionality even without co-occurrence.
\end{enumerate}

\textbf{Graph Construction Algorithm}:
\begin{enumerate}
    \item For each build in training data, identify all test pairs that fail together
    \item Count co-occurrences across all builds
    \item Filter edges with fewer than 1 co-occurrence (min\_co\_occurrences = 1)
    \item Compute semantic similarity for all test pairs, add edges above threshold
    \item Combine edges with type-specific weights
\end{enumerate}

This multi-edge approach increases graph density from 0.02\% (co-failure only)
to 0.5-1.0\% (all edge types), providing richer connectivity for message passing.

\subsection{Semantic Stream}

The Semantic Stream processes text embeddings using a feed-forward network
with residual connections:

\textbf{Input}: SBERT embeddings of concatenated test case description and
commit message (1536 dimensions = 768 TC + 768 Commit).

\textbf{Architecture}: The stream consists of:
\begin{enumerate}
    \item Input projection: Linear layer projecting 1536-dim to 256-dim
    \item Two residual FFN blocks, each containing:
    \begin{itemize}
        \item Linear (256 $\rightarrow$ 1024)
        \item GELU activation
        \item Dropout (0.3)
        \item Linear (1024 $\rightarrow$ 256)
        \item Dropout (0.3)
        \item LayerNorm
    \end{itemize}
    \item Output LayerNorm
\end{enumerate}

The output is a 256-dimensional semantic representation for each test case.

\subsection{Structural Stream (Graph Attention Network)}

The Structural Stream processes historical features over the test relationship
graph using Graph Attention Networks~\cite{velickovic2018graph}:

\textbf{Input}: 10 structural features per test case (failure\_rate,
recent\_failure\_rate, test\_age, flakiness\_rate, etc.).

\textbf{GAT Architecture}: We use two GAT layers:
\begin{enumerate}
    \item \textbf{Layer 1}: Multi-head attention (4 heads) with concatenation
    \begin{equation}
        h_i^{(1)} = \|_{k=1}^{4} \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k h_j^{(0)}\right)
    \end{equation}
    where $\alpha_{ij}^k$ are learned attention coefficients for head $k$.
    Output dimension: $256 \times 4 = 1024$.

    \item \textbf{Layer 2}: Single-head attention with averaging, producing
    256-dimensional output per test case.
\end{enumerate}

\textbf{Edge Weights}: The GAT layers incorporate edge weights from the multi-edge
graph, allowing the model to learn that co-failure relationships (weight 1.0)
are more important than semantic similarity (weight 0.3).

\subsection{Cross-Attention Fusion}

The Cross-Attention Fusion module combines semantic and structural representations
through bidirectional attention:

\textbf{Bidirectional Cross-Attention}: Each modality attends to the other,
allowing semantic features to be informed by structural patterns and vice versa:

\begin{enumerate}
    \item \textbf{Semantic $\rightarrow$ Structural}: Semantic features query
    structural features:
    \begin{equation}
        h_{sem}' = h_{sem} + \text{MHA}(h_{sem}, h_{struct}, h_{struct})
    \end{equation}

    \item \textbf{Structural $\rightarrow$ Semantic}: Structural features query
    semantic features:
    \begin{equation}
        h_{struct}' = h_{struct} + \text{MHA}(h_{struct}, h_{sem}, h_{sem})
    \end{equation}
\end{enumerate}

where $\text{MHA}(Q, K, V)$ denotes multi-head attention with query $Q$,
key $K$, and value $V$.

Each attention uses 4 heads with 256-dimensional queries/keys/values, followed
by LayerNorm for stability.

\textbf{Feature Concatenation}: The attended features are concatenated:
\begin{equation}
    h_{fused} = [h_{sem}' \| h_{struct}']
\end{equation}

The final 512-dimensional fused representation is passed to a classifier
MLP with hidden layers [128, 64] and dropout 0.4.

\subsection{Training with Weighted Focal Loss}

We use Weighted Focal Loss~\cite{lin2017focal} to address the severe class
imbalance (37:1 Pass:Fail ratio) in test execution data:

\begin{equation}
    \mathcal{L} = -\alpha \cdot w_t \cdot (1 - p_t)^\gamma \cdot \log(p_t)
\end{equation}

where:
\begin{itemize}
    \item $p_t$ is the predicted probability of the true class
    \item $\alpha = 0.75$ weights the loss toward the minority class
    \item $\gamma = 2.5$ is the focusing parameter that down-weights easy examples
    \item $w_t$ is the class weight (computed as inverse frequency)
\end{itemize}

\textbf{Why Weighted Focal Loss?} Standard cross-entropy is dominated by the
majority class (Pass), leading to models that predict ``Pass'' for nearly all
test cases. Focal Loss addresses this by reducing the contribution of easy
examples (confident Pass predictions) while focusing on hard examples (uncertain
predictions and Fail cases). The additional class weighting further rebalances
the loss toward the minority class.

\textbf{Training Configuration}:
\begin{itemize}
    \item Optimizer: AdamW with learning rate $3 \times 10^{-5}$
    \item Weight decay: $1 \times 10^{-4}$
    \item Scheduler: Cosine annealing with $\eta_{min} = 1 \times 10^{-6}$
    \item Early stopping: Patience 15, monitoring validation F1-macro
    \item Gradient clipping: Max norm 1.0
\end{itemize}

%------------------------------------------------------------------------------
% SECTION 5: EXPERIMENTAL DESIGN
%------------------------------------------------------------------------------
\section{Experimental Design}
\label{sec:experimental}

\subsection{Research Questions}

\begin{itemize}
    \item \textbf{RQ1 (Effectiveness)}: How effective is \filopriori{} compared to baseline methods?
    \item \textbf{RQ2 (Components)}: What is the contribution of each architectural component?
    \item \textbf{RQ3 (Robustness)}: How robust is \filopriori{} across different time periods?
    \item \textbf{RQ4 (Sensitivity)}: How sensitive is \filopriori{} to hyperparameter choices?
\end{itemize}

\subsection{Datasets}

We evaluate \filopriori{} on two datasets with complementary characteristics:

\textbf{Dataset 1: Industrial QTA Dataset.} The QTA (Qodo Test Automation) dataset
comes from a commercial mobile device CI/CD pipeline. This dataset provides rich
semantic information including detailed test descriptions, test steps, and commit
messages. Table~\ref{tab:dataset_qta} summarizes its statistics.

\begin{table}[h]
\centering
\caption{Industrial QTA Dataset Statistics}
\label{tab:dataset_qta}
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Total test executions & 52,102 \\
Unique builds & 1,339 \\
Builds with failures & 277 (20.7\%) \\
Unique test cases & 2,347 \\
Pass:Fail ratio & 37:1 \\
Semantic info & Rich (descriptions, commits) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset 2: RTPTorrent Open-Source Dataset.} To evaluate generalization,
we use the RTPTorrent dataset~\cite{mattis2020rtptorrent}, an open-source benchmark
from MSR 2020 containing test execution histories from 20 Java projects on GitHub
with over 100,000 Travis CI build logs. This dataset has limited semantic information
(test names only) but provides diverse projects for cross-domain evaluation.

\begin{table}[h]
\centering
\caption{RTPTorrent Dataset Statistics}
\label{tab:dataset_rtptorrent}
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Projects & 20 Java projects \\
Source & Travis CI build logs \\
Semantic info & Limited (test names) \\
License & CC BY 4.0 \\
\bottomrule
\end{tabular}
\end{table}

The complementary nature of these datasets enables comprehensive evaluation:
the industrial dataset tests semantic feature utilization while RTPTorrent
tests structural feature effectiveness and cross-project generalization.

\subsection{Baselines}

We compare against eight baselines:

\textbf{Heuristic Baselines}:
\begin{itemize}
    \item \textbf{Random}: Random ordering (expected APFD $\approx$ 0.5)
    \item \textbf{Recency}: Prioritize recently failed tests
    \item \textbf{RecentFailureRate}: Failure rate in last 5 builds
    \item \textbf{FailureRate}: Historical failure rate
    \item \textbf{GreedyHistorical}: Combined heuristics
\end{itemize}

\textbf{ML Baselines}:
\begin{itemize}
    \item \textbf{Logistic Regression}
    \item \textbf{Random Forest}
    \item \textbf{XGBoost}
\end{itemize}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{APFD}: Primary metric, computed per build
    \item \textbf{Statistical tests}: Wilcoxon signed-rank test ($\alpha = 0.05$)
    \item \textbf{Confidence intervals}: 95\% bootstrap CI (1000 iterations)
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
    \item \textbf{Framework}: PyTorch 2.0, PyTorch Geometric 2.3
    \item \textbf{Hardware}: NVIDIA RTX 3090 (24GB VRAM)
    \item \textbf{Training}: 50 epochs, batch size 32, AdamW optimizer
    \item \textbf{Learning rate}: $3 \times 10^{-5}$ with cosine annealing
    \item \textbf{Early stopping}: Patience 15, monitoring val\_f1\_macro
\end{itemize}

%------------------------------------------------------------------------------
% SECTION 6: RESULTS
%------------------------------------------------------------------------------
\section{Results}
\label{sec:results}

\input{sections/results_ieee}

%------------------------------------------------------------------------------
% SECTION 7: DISCUSSION
%------------------------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}

\input{sections/discussion_ieee}

%------------------------------------------------------------------------------
% SECTION 8: THREATS TO VALIDITY
%------------------------------------------------------------------------------
\section{Threats to Validity}
\label{sec:threats}

\input{sections/threats_ieee}

%------------------------------------------------------------------------------
% SECTION 9: CONCLUSION
%------------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

We presented \filopriori{}, a dual-stream deep learning approach for Test Case
Prioritization that combines semantic understanding with structural pattern
learning. By modeling test relationships through multi-edge graphs and using
Graph Attention Networks for message passing, we capture complex dependencies
between test cases that simpler approaches ignore.

Our key contributions include:
\begin{itemize}
    \item A dual-stream architecture combining SBERT semantic embeddings with
    Graph Attention Networks for structural feature learning.
    \item Multi-edge test relationship graphs capturing co-failure, co-success,
    and semantic similarity relationships.
    \item Careful feature engineering identifying 10 discriminative structural
    features from historical execution patterns.
    \item Empirical validation on industrial data showing 14.6\% improvement over
    random ordering ($p < 0.001$) with Graph Attention contributing +17.0\%.
    \item Cross-dataset validation on RTPTorrent benchmark achieving APFD=0.8376
    across 20 Java projects, outperforming 6 of 7 baselines.
    \item Robust temporal generalization (APFD: 0.619--0.663 across time periods).
\end{itemize}

The ablation study reveals that Graph Attention is the most critical component,
validating our hypothesis that modeling test relationships through graphs is
more effective than treating tests as independent entities.

\textbf{Future Work.} We plan to: (1) investigate dynamic graph construction that
evolves with the test suite, (2) explore cross-project transfer learning using
pre-trained code embeddings (CodeBERT, GraphCodeBERT), (3) develop real-time
prioritization integrated with CI/CD pipelines, and (4) extend evaluation to
additional industrial datasets with diverse testing patterns.

\textbf{Data Availability.} Our replication package, including source code,
trained models, configuration files, and anonymized dataset, is available at:
\url{https://github.com/[anonymized]/filo-priori-v9}

%------------------------------------------------------------------------------
% ACKNOWLEDGMENTS
%------------------------------------------------------------------------------
\section*{Acknowledgments}
This work was supported by [funding information]. We thank [acknowledgments].

%------------------------------------------------------------------------------
% REFERENCES
%------------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{references_ieee}

%------------------------------------------------------------------------------
% BIOGRAPHY (Optional for TSE)
%------------------------------------------------------------------------------
\begin{IEEEbiographynophoto}{Acauan C. Ribeiro}
is a researcher at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil. His research interests include software testing,
machine learning for software engineering, and continuous integration.
\end{IEEEbiographynophoto}

\end{document}
