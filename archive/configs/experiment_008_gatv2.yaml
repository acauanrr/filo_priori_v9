# PHASE II: ARCHITECTURAL REFINEMENT - Experiment 008
# Objective: Improve structural stream with GATv2 (dynamic attention)
# Expected: Test F1 Macro 0.67 → 0.68-0.70 (+1-3 pp)
#
# CHANGE from Experiment 007:
# - Structural stream: GAT → GATv2 (more expressive attention mechanism)
#
# Baseline: Experiment 007 (F1 Macro = 0.6677)

# Data Configuration (SAME AS EXP 007)
data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_008_gatv2"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  # Binary Classification
  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_all"

  # NO DATA AUGMENTATION (maintain baseline conditions)
  use_smote: false
  use_resampling: false

# Text Processing (SAME AS EXP 007)
text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

# Embedding Configuration (SAME AS EXP 007)
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

# Phylogenetic Structure (SAME AS EXP 007)
phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

# Model Architecture - SINGLE CHANGE: GATv2
model:
  # Semantic Stream - NO CHANGE
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"
    pooling_type: "mean"  # Explicit (no change from 007)

  # Structural Stream - CHANGE: Use GATv2 (dynamic attention)
  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 10
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"  # CHANGED: Use GATv2 for dynamic attention
    num_heads: 4  # Multi-head attention (256 / 4 = 64 dim per head)
    use_residual: false  # Use manual residual in forward pass

  # Cross-Attention Fusion - NO CHANGE
  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "concatenation"  # Explicit (no change from 007)

  # Classification Head - NO CHANGE
  classifier:
    hidden_dim: 128
    num_classes: 2
    dropout: 0.5
    classifier_type: "simple"  # Explicit (no change from 007)

# Training Configuration (SAME AS EXP 007)
training:
  num_epochs: 80
  batch_size: 64
  learning_rate: 5.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 1000
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: true
  threshold_search_range: [0.20, 0.80]

  # Loss Configuration (SAME AS EXP 007)
  loss:
    type: "focal"
    focal_alpha: [0.25, 0.75]
    use_class_weights: false
    focal_gamma: 2.0
    label_smoothing: 0.05

  # Optimization (SAME AS EXP 007)
  optimizer: "adamw"
  scheduler: "cosine"

  # Early Stopping (SAME AS EXP 007)
  early_stopping:
    patience: 12
    min_delta: 5.0e-4
    monitor: "f1_macro"
    mode: "max"

  # Checkpointing
  save_every_n_epochs: 5
  save_best_only: true

# Evaluation Configuration (SAME AS EXP 007)
evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"
    - "roc_auc"
    - "auprc"

# Hardware Configuration (SAME AS EXP 007)
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Logging
logging:
  use_tensorboard: true
  log_every_n_steps: 50
  log_dir: "results/experiment_008_gatv2/logs"
  save_predictions: true

# ADVANCED MONITORING (SAME AS EXP 007)
monitoring:
  enabled: true
  check_every_n_batches: 50
  prediction_diversity_threshold: 0.05
  gradient_norm_max: 100.0
  gradient_norm_min: 1e-6
  loss_spike_multiplier: 5.0
  auto_terminate_on_collapse: true

# SUCCESS CRITERIA (Phase II: Architectural Refinement)
success_criteria:
  phase: "architectural_refinement"
  required_metrics:
    f1_macro: 0.68  # Target: +1-3 pp over baseline (0.6677)
    accuracy: 0.60  # Maintain at least baseline level
    recall_pass: 0.50
    prediction_diversity: 0.30
  blocking_failures:
    - "prediction_collapse"
    - "performance_regression"  # F1 < 0.65 (below baseline - tolerance)

# Experiment Metadata
experiment:
  phase: "II"
  phase_name: "Architectural Refinement"
  experiment_id: "008"
  description: "Add GATv2 to structural stream for more expressive graph attention"
  hypothesis: "GATv2's dynamic attention mechanism will better capture structural relationships, improving F1 Macro by 1-3 pp"
  baseline_experiment: "007"
  baseline_metrics:
    f1_macro: 0.6677
    accuracy: 0.8739
    pass_recall: 0.94
    not_pass_recall: 0.37
  expected_improvements:
    - "F1 Macro: 0.6677 → 0.68-0.70 (+1-3 pp)"
    - "Not-Pass Recall: 37% → 39-42% (+2-5 pp)"
    - "Better structural feature learning"
  rationale: |
    GATv2 improves upon GAT by applying LeakyReLU AFTER the linear projection,
    allowing for more dynamic and expressive attention weights. This should
    better capture complex structural relationships in the k-NN graph.
