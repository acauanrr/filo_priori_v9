# PHASE II: ARCHITECTURAL REFINEMENT - Experiment 011 (FINAL)
# Objective: Improve classifier with gradual reduction + BatchNorm + Residual
# Expected: Test F1 Macro 0.72-0.75 → 0.75-0.78 (+3-5 pp cumulative)
#
# CHANGES from Experiment 007 (ALL Phase II improvements):
# - Structural stream: GAT → GATv2 (from Exp 008)
# - Semantic stream: Mean → Attention Pooling (from Exp 009)
# - Fusion: Concatenation → Bidirectional Cross-Attention (from Exp 010)
# - Classifier: Simple MLP → Improved Classifier (NEW in Exp 011)
#
# Baseline: Experiment 010 (Expected F1 Macro ≈ 0.72-0.75)
# This is the FINAL Phase II experiment with ALL architectural improvements.

# Data Configuration (SAME AS EXP 007)
data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_011_improved_classifier"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  # Binary Classification
  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_all"

  # NO DATA AUGMENTATION
  use_smote: false
  use_resampling: false

# Text Processing (SAME AS EXP 007)
text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

# Embedding Configuration (SAME AS EXP 007)
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

# Phylogenetic Structure (SAME AS EXP 007)
phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

# Model Architecture - ALL Phase II improvements
model:
  # Semantic Stream - CHANGE: Attention Pooling (from Exp 009)
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"
    pooling_type: "attention"
    attention_pooling:
      dropout: 0.1
      use_layer_norm: true

  # Structural Stream - CHANGE: GATv2 (from Exp 008)
  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 10
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"
    use_residual: true

  # Cross-Attention Fusion - CHANGE: Bidirectional (from Exp 010)
  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "bidirectional_cross_attention"
    use_feedforward: true
    use_gating: true

  # Classification Head - CHANGE: Improved Classifier
  classifier:
    classifier_type: "improved"  # ⭐ NEW: Changed from "simple"
    hidden_dim: 256              # Input dimension (from fusion)
    num_classes: 2

    # Improved Classifier Architecture (256→192→128→64→2)
    architecture: "gradual_reduction"  # ⭐ NEW
    use_batch_norm: true               # ⭐ NEW: Add BatchNorm for stability
    use_residual: true                 # ⭐ NEW: Add skip connection
    dropout_base: 0.3                  # ⭐ NEW: Base dropout rate
    dropout_scaling: 1.5               # ⭐ NEW: Dropout increases in deeper layers
    # Effective dropout: Layer1=0.3, Layer2=0.36, Layer3=0.45

# Training Configuration (ADJUSTED for stability with BatchNorm)
training:
  num_epochs: 80
  batch_size: 64
  learning_rate: 4.0e-5  # Same as Exp 010 (complex model)
  weight_decay: 5.0e-5   # ⚠️ REDUCED: BatchNorm already regularizes
  warmup_steps: 1000
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: true
  threshold_search_range: [0.20, 0.80]

  # Loss Configuration (SAME AS EXP 007)
  loss:
    type: "focal"
    focal_alpha: [0.25, 0.75]
    use_class_weights: false
    focal_gamma: 2.0
    label_smoothing: 0.05

  # Optimization (SAME AS EXP 007)
  optimizer: "adamw"
  scheduler: "cosine"

  # Early Stopping (SAME AS EXP 010)
  early_stopping:
    patience: 15  # Keep increased patience for complex model
    min_delta: 5.0e-4
    monitor: "f1_macro"
    mode: "max"

  # Checkpointing
  save_every_n_epochs: 5
  save_best_only: true

# Evaluation Configuration (SAME AS EXP 007)
evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"
    - "roc_auc"
    - "auprc"

# Hardware Configuration (SAME AS EXP 007)
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Logging
logging:
  use_tensorboard: true
  log_every_n_steps: 50
  log_dir: "results/experiment_011_improved_classifier/logs"
  save_predictions: true

# ADVANCED MONITORING (SAME AS EXP 007)
monitoring:
  enabled: true
  check_every_n_batches: 50
  prediction_diversity_threshold: 0.05
  gradient_norm_max: 100.0
  gradient_norm_min: 1e-6
  loss_spike_multiplier: 5.0
  auto_terminate_on_collapse: true

# SUCCESS CRITERIA (Phase II: Architectural Refinement - FINAL TARGET)
success_criteria:
  phase: "architectural_refinement"
  required_metrics:
    f1_macro: 0.75  # Target: 0.72-0.75 + 0.03-0.05 = 0.75-0.78
    accuracy: 0.65  # Expect improvement over baseline
    recall_pass: 0.50
    not_pass_recall: 0.40  # Target improvement for minority class
    prediction_diversity: 0.30
  blocking_failures:
    - "prediction_collapse"
    - "performance_regression"  # F1 < 0.71 (below Exp 010 - tolerance)

# Experiment Metadata
experiment:
  phase: "II"
  phase_name: "Architectural Refinement (FINAL)"
  experiment_id: "011"
  description: "Add Improved Classifier - FINAL Phase II experiment with ALL architectural improvements"
  hypothesis: "Gradual dimensionality reduction with BatchNorm and residual connections will improve classification, achieving F1 Macro ≥ 0.75"
  baseline_experiment: "010"
  baseline_metrics:
    f1_macro: 0.73  # Expected from 010 (midpoint of 0.72-0.75)
    accuracy: 0.88
  expected_improvements:
    - "F1 Macro: 0.73 → 0.75-0.78 (+2-5 pp over Exp 010)"
    - "F1 Macro: 0.6677 → 0.75-0.78 (+8-11 pp over Exp 007 baseline)"
    - "Not-Pass Recall: 37% → 40-45% (+3-8 pp)"
    - "Not-Pass Precision: 44% → 50-55% (+6-11 pp)"
    - "Better stability with BatchNorm"
    - "Better gradient flow with residual connections"
  rationale: |
    The improved classifier uses:
    1. Gradual reduction (256→192→128→64→2) instead of abrupt (256→128→2)
       - Preserves more information through the layers
       - Smoother transition reduces information bottleneck

    2. Batch Normalization:
       - Stabilizes training by normalizing activations
       - Reduces internal covariate shift
       - Acts as mild regularizer

    3. Residual Connection (skip connection from input to layer 2):
       - Helps gradient flow during backpropagation
       - Preserves original feature information
       - Mitigates vanishing gradient problem

    4. Adaptive Dropout (increasing in deeper layers):
       - Early layers: less dropout (preserve features)
       - Deep layers: more dropout (prevent overfitting)

    Together, these improvements should create a more powerful and stable
    classifier that can better leverage the rich fused features from the
    bidirectional cross-attention fusion layer.

  training_notes: |
    - Reduced weight_decay (5e-5) because BatchNorm provides regularization
    - Kept LR at 4e-5 (same as Exp 010) for stability
    - Patience=15 to allow full convergence of complex model
    - Expected training time: ~55-60 min (similar to Exp 007)

  phase_ii_summary: |
    This is the FINAL experiment of Phase II, combining ALL architectural improvements:

    Exp 007 (Baseline):         0.6677 F1 Macro
    Exp 008 (+GATv2):          ~0.69   (+0.02)
    Exp 009 (+AttentionPool):  ~0.71   (+0.02)
    Exp 010 (+BiDirectional):  ~0.73   (+0.02)
    Exp 011 (+ImprovedClass):  ~0.76   (+0.03) ⭐ TARGET

    Total expected improvement: +0.09 F1 Macro points (13.5% relative improvement)

    If Exp 011 achieves F1 Macro ≥ 0.75, Phase II is COMPLETE and successful.
    Next step would be Phase III: Hyperparameter Optimization.
