%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: RESULTS (IEEE TSE Format)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: Section header is in main_ieee_tse.tex

This section presents the experimental results organized by research questions.
All experiments were conducted on the QTA dataset containing 277 builds with
at least one failing test case, totaling 8,847 test case executions with failures
from 52,102 total executions.

%------------------------------------------------------------------------------
% RQ1: Effectiveness
%------------------------------------------------------------------------------

\subsection{RQ1: Effectiveness Comparison}
\label{sec:rq1}

Table~\ref{tab:tcp_comparison} presents the comparison of \filopriori{} against
eight baseline methods. We evaluate effectiveness using the Average Percentage
of Faults Detected (APFD) metric, with 95\% bootstrap confidence intervals
(1,000 iterations) and Wilcoxon signed-rank tests for statistical significance.

\begin{table}[!t]
\centering
\caption{Comparison with State-of-the-Art TCP Methods}
\label{tab:tcp_comparison}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{APFD} & \textbf{95\% CI} & \textbf{p-value} & \textbf{$\Delta$ vs Random} \\
\midrule
DeepOrder~\cite{chen2023deeporder} & 0.6500 & {[}0.627, 0.673{]} & 0.434 & +22.0\% \\
\textbf{Filo-Priori} & \textbf{0.6413} & {[}0.611, 0.671{]} & -- & \textbf{+20.3\%} \\
FailureRate & 0.5549 & {[}0.531, 0.579{]} & 0.003** & +4.1\% \\
Random & 0.5329 & {[}0.512, 0.554{]} & $<$.001*** & baseline \\
RETECS~\cite{spieker2017reinforcement} & 0.4583 & {[}0.436, 0.480{]} & $<$.001*** & -14.0\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001 (Wilcoxon signed-rank test vs \filopriori{})
\item DeepOrder and \filopriori{} show no statistically significant difference.
\end{tablenotes}
\end{table}

\textbf{Key Findings for RQ1:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.6413} (95\% CI: [0.611, 0.671]),
    representing a \textbf{20.3\%} improvement over random ordering ($p < 0.001$).
    \item DeepOrder~\cite{chen2023deeporder} achieves the highest APFD (0.6500), but
    the difference from \filopriori{} is \textbf{not statistically significant}
    ($p = 0.434$, Wilcoxon signed-rank test).
    \item \filopriori{} significantly outperforms the FailureRate heuristic
    (0.5549) with $p = 0.003$, demonstrating that our approach exceeds well-tuned
    history-based baselines.
    \item RETECS~\cite{spieker2017reinforcement} performs below random ordering
    (-14.0\%), suggesting that reinforcement learning may require more training
    data or domain-specific reward engineering for our dataset.
    \item Both deep learning approaches (DeepOrder, \filopriori{}) significantly
    outperform heuristic and RL-based methods, validating the effectiveness of
    neural network-based TCP.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ1]
\filopriori{} achieves APFD=0.6413, outperforming random ordering by 20.3\%
($p < 0.001$) and FailureRate by 15.6\% ($p = 0.003$). \filopriori{} is
statistically equivalent to DeepOrder (0.6500, $p = 0.434$), while offering
interpretability through graph-based test relationships.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ1b: Cross-Dataset Validation (RTPTorrent)
%------------------------------------------------------------------------------

\subsection{Cross-Dataset Validation: RTPTorrent}
\label{sec:rq1_rtptorrent}

To evaluate generalization beyond the industrial dataset, we conducted a comprehensive
experiment on the RTPTorrent open-source benchmark~\cite{mattis2020rtptorrent}, which
contains test execution histories from 20 Java projects with over 100,000 Travis CI builds.
We adapted \filopriori{} using a LightGBM LambdaRank model with 16 ranking-optimized
features, trained independently on each project.

\begin{table}[!t]
\centering
\caption{RTPTorrent Cross-Dataset Results (20 Projects)}
\label{tab:rtptorrent_results}
\footnotesize
\begin{tabular}{lccr}
\toprule
\textbf{Baseline} & \textbf{APFD} & \textbf{Model Improv.} & \textbf{p-value} \\
\midrule
\textbf{Filo-Priori V10} & \textbf{0.8376} & -- & -- \\
\midrule
recently\_failed & 0.8209 & +2.02\% & sig.* \\
random & 0.4940 & +69.56\% & $<$0.001*** \\
untreated & 0.3574 & +134.32\% & $<$0.001*** \\
matrix\_naive & 0.5693 & +47.11\% & $<$0.001*** \\
matrix\_conditional & 0.5132 & +63.21\% & $<$0.001*** \\
optimal\_duration & 0.5934 & +41.15\% & $<$0.001*** \\
optimal\_failure (oracle) & 0.9249 & -9.45\% & -- \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Aggregate results over 1,250 test builds from 20 Java projects.
\item * Statistically significant improvement over strongest baseline.
\end{tablenotes}
\end{table}

Table~\ref{tab:rtptorrent_top5} shows the top-5 performing projects:

\begin{table}[!t]
\centering
\caption{Top-5 Projects by APFD (RTPTorrent)}
\label{tab:rtptorrent_top5}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Project} & \textbf{APFD} & \textbf{Builds} & \textbf{vs recent.} \\
\midrule
apache/sling & 0.9922 & 163 & +2.18\% \\
neuland/jade4j & 0.9799 & 20 & +0.88\% \\
eclipse/jetty.project & 0.9789 & 66 & +1.27\% \\
facebook/buck & 0.9722 & 69 & +1.97\% \\
deeplearning4j/dl4j & 0.9277 & 114 & +0.46\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for Cross-Dataset Validation:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.8376} across 20 diverse
    Java projects (1,250 test builds), demonstrating strong cross-domain generalization.
    \item The model \textbf{outperforms 6 out of 7 baselines}, including the strongest
    heuristic baseline (recently\_failed) by +2.02\%.
    \item Only the oracle baseline (optimal\_failure) achieves higher APFD (0.9249),
    representing the theoretical upper bound with perfect failure knowledge.
    \item Performance varies across projects (APFD: 0.29--0.99), with best results
    on projects with consistent failure patterns (e.g., apache/sling).
    \item The most important features are \texttt{novelty\_score}, \texttt{base\_risk},
    \texttt{execution\_frequency}, and \texttt{time\_decay\_score}.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Cross-Dataset Finding]
\filopriori{} generalizes effectively to open-source projects, achieving APFD=0.8376
across 20 RTPTorrent projects and outperforming 6 of 7 baselines. This validates
the approach on a well-known benchmark with limited semantic information.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ2: Component Contributions
%------------------------------------------------------------------------------

\subsection{RQ2: Ablation Study}
\label{sec:rq2}

To understand the contribution of each architectural component, we conducted
an ablation study by systematically removing components and measuring the
impact on APFD. Table~\ref{tab:ablation} shows the results.

\begin{table}[!t]
\centering
\caption{Ablation Study Results}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{APFD} & \textbf{$\Delta$} & \textbf{Contrib.} & \textbf{p-value} \\
\midrule
Full Model & 0.6397 & -- & -- & -- \\
\midrule
w/o Graph Attention & 0.5467 & -0.093 & +17.0\% & $<$0.001*** \\
w/o Structural Stream & 0.6073 & -0.032 & +5.3\% & $<$0.001*** \\
w/o Focal Loss (use CE) & 0.6115 & -0.028 & +4.6\% & $<$0.001*** \\
w/o Class Weighting & 0.6179 & -0.022 & +3.5\% & 0.002** \\
w/o Semantic Stream & 0.6280 & -0.012 & +1.9\% & 0.087 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item *** p $<$ 0.001, ** p $<$ 0.01 (Wilcoxon signed-rank test)
\item Contrib. = relative contribution to full model performance
\end{tablenotes}
\end{table}

\textbf{Key Findings for RQ2:}
\begin{itemize}
    \item \textbf{Graph Attention (GAT)} is the most critical component,
    contributing \textbf{+17.0\%} to performance. Removing it causes the
    largest drop in APFD (-0.093), demonstrating the importance of modeling
    test case relationships through graph neural networks.
    \item The \textbf{Structural Stream} contributes +5.3\%, showing that
    historical execution features provide valuable information beyond
    semantic content.
    \item \textbf{Focal Loss} contributes +4.6\%, validating its effectiveness
    for handling the severe class imbalance. Using standard cross-entropy
    significantly degrades performance.
    \item \textbf{Class Weighting} contributes +3.5\%, showing that additional
    reweighting by class frequency further improves handling of imbalance.
    \item The \textbf{Semantic Stream} shows the smallest contribution (+1.9\%),
    suggesting that for this dataset, structural patterns are more predictive
    than textual content.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ2]
Graph Attention Networks are the most critical component (+17.0\%), followed
by structural features (+5.3\%) and Focal Loss (+4.6\%). Class weighting
provides an additional +3.5\% improvement.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ3: Temporal Robustness
%------------------------------------------------------------------------------

\subsection{RQ3: Temporal Validation}
\label{sec:rq3}

Software projects evolve over time, and a TCP model trained on historical data
must generalize to future builds. We evaluated temporal robustness using three
validation strategies that respect temporal ordering.

\begin{table}[!t]
\centering
\caption{Temporal Cross-Validation Results}
\label{tab:temporal_cv}
\begin{tabular}{lccc}
\toprule
\textbf{Validation Method} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{Folds} \\
\midrule
Temporal 5-Fold CV & 0.6629 & [0.627, 0.698] & 5 \\
Sliding Window CV & 0.6279 & [0.595, 0.661] & 10 \\
Concept Drift Test & 0.6187 & [0.574, 0.661] & 3 \\
\midrule
\textbf{Average} & \textbf{0.6365} & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Validation Methods:}
\begin{itemize}
    \item \textbf{Temporal 5-Fold CV}: Chronologically split data into 5 folds,
    training on past folds and testing on future folds.
    \item \textbf{Sliding Window CV}: Use a fixed training window that slides
    forward in time, testing on the immediately following period.
    \item \textbf{Concept Drift Test}: Train on early builds, test on late
    builds to detect performance degradation over time.
\end{itemize}

\textbf{Key Findings for RQ3:}
\begin{itemize}
    \item Performance remains stable across all temporal validation methods,
    with APFD ranging from 0.619 to 0.663.
    \item The concept drift test shows only a 3\% degradation compared to
    temporal CV, indicating robustness to evolving test patterns.
    \item No significant performance degradation over time, demonstrating
    the model's ability to generalize to future builds.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ3]
\filopriori{} demonstrates robust performance across temporal splits, with
consistent APFD in the range 0.619--0.663. The model generalizes well to
future builds without significant performance degradation.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ4: Sensitivity Analysis
%------------------------------------------------------------------------------

\subsection{RQ4: Hyperparameter Sensitivity}
\label{sec:rq4}

We analyzed the sensitivity of \filopriori{} to key hyperparameters by comparing
results across multiple experimental configurations.

\begin{table}[!t]
\centering
\caption{Hyperparameter Sensitivity Analysis}
\label{tab:sensitivity}
\begin{tabular}{llcr}
\toprule
\textbf{Parameter} & \textbf{Values Tested} & \textbf{Best} & \textbf{$\Delta$} \\
\midrule
Loss Function & CE, Focal, W.~Focal & W.~Focal & 0.036 \\
Focal Gamma & 1.5, 2.0, 2.5, 3.0 & 2.5 & 0.034 \\
Learning Rate & 1e-5, 3e-5, 5e-5, 1e-4 & 3e-5 & 0.027 \\
GNN Layers & 1, 2, 3 & 1 & 0.027 \\
GNN Heads & 1, 2, 4, 8 & 2 & 0.018 \\
Structural Feat. & 6, 10, 29 & 10 & 0.015 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for RQ4:}
\begin{itemize}
    \item \textbf{Loss Function}: Weighted Focal Loss performs best; the choice
    of loss function has the largest impact ($\Delta$ = 0.036).
    \item \textbf{Focal Gamma}: A gamma of 2.5 provides optimal focus on hard
    examples ($\Delta$ = 0.034), second largest impact after loss type.
    \item \textbf{Learning Rate}: Lower rate (3e-5) outperforms higher rates,
    suggesting careful optimization is beneficial.
    \item \textbf{GNN Architecture}: Simpler architecture (1 layer, 2 heads)
    performs best, avoiding overfitting on the test relationship graph.
    \item \textbf{Structural Features}: 10 selected features outperform both
    minimal (6) and expanded (29) feature sets.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ4]
\filopriori{} shows moderate sensitivity to hyperparameters, with loss function
choice having the largest impact (5.9\% relative variation). The optimal
configuration uses Weighted Focal Loss with $\gamma=2.5$, learning rate 3e-5,
1-layer GAT with 2 heads, and 10 structural features.
\end{tcolorbox}
