% FILO-PRIORI V9 - ALL LATEX CONTENT
% Generated: 2025-11-26 13:27:18
%
% This file contains all LaTeX tables and sections for the paper.
%


% === results/baselines/comparison_table.tex ===
\begin{table}
\caption{Comparison of TCP Methods on QTA Dataset}
\label{tab:tcp_comparison}
\begin{tabular}{lccccc}
\toprule
Method & Mean APFD & p-value & Cohen's d & Effect & 95% CI \\
\midrule
FailureRate & 0.6289 & 0.3626 & -0.046 & negligible & [0.597, 0.660] \\
XGBoost & 0.6171 & 0.5771 & -0.000 & negligible & [0.583, 0.649] \\
Filo-Priori & 0.6171 & 1.0000 & 0.000 & - & [0.586, 0.648] \\
GreedyHistorical & 0.6138 & 0.0962 & 0.013 & negligible & [0.582, 0.647] \\
LogisticRegression & 0.5964 & 0.1855 & 0.064 & negligible & [0.559, 0.631] \\
RandomForest & 0.5910 & 0.0937 & 0.082 & negligible & [0.556, 0.627] \\
Random & 0.5596 & 2.05e-04 & 0.200 & small & [0.533, 0.584] \\
RecentFailureRate & 0.5454 & 7.21e-05 & 0.226 & small & [0.515, 0.578] \\
Recency & 0.5240 & 1.44e-08 & 0.326 & small & [0.490, 0.557] \\
\bottomrule
\end{tabular}
\end{table}


% === results/ablation/ablation_study_final.tex ===

\begin{table}[htbp]
\centering
\caption{Ablation Study: Component Contributions to Filo-Priori v9 Performance}
\label{tab:ablation}
\begin{tabular}{llccccc}
\toprule
\textbf{ID} & \textbf{Removed Component} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{$\Delta$ APFD} & \textbf{Contrib.} & \textbf{Sig.} \\
\midrule
A3 & Graph Attention & 0.5311 & [0.506, 0.557] & -0.1086 & +17.0\% & *** \\
A2 & Structural Stream & 0.6060 & [0.580, 0.632] & -0.0337 & +5.3\% & *** \\
A5 & Class Weighting & 0.6100 & [0.581, 0.640] & -0.0297 & +4.6\% & *** \\
A7 & Ensemble & 0.6171 & [0.588, 0.651] & -0.0226 & +3.5\% & *** \\
A1 & Semantic Stream & 0.6276 & [0.595, 0.658] & -0.0121 & +1.9\% &  \\
\textbf{A0} & \textbf{All (Full Model)} & \textbf{0.6397} & [0.608, 0.669] & - & - & \\
A6 & Cross-Attention & 0.6466 & [0.615, 0.674] & +0.0069 & -1.1\% &  \\

\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Note: $\Delta$ APFD shows performance change when component is removed (negative = performance drop).
\item Contribution shows how much each component adds to the full model's performance.
\item *** indicates statistical significance (paired t-test, $p < 0.05$).
\end{tablenotes}
\end{table}


% === results/temporal_cv/temporal_cv_table.tex ===

\begin{table}[htbp]
\centering
\caption{Temporal Cross-Validation Results for Filo-Priori v9}
\label{tab:temporal_cv}
\begin{tabular}{lccc}
\toprule
\textbf{Validation Method} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{N Builds} \\
\midrule
Temporal 5-Fold CV & 0.6629 & [0.627, 0.698] & 215 \\
Sliding Window CV & 0.6279 & [0.595, 0.661] & 248 \\
Concept Drift Test & 0.6187 & [0.574, 0.661] & 152 \\

\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Temporal 5-Fold: Train on past builds, test on future builds.
\item Sliding Window: 100-build training window, 20-build test window.
\item Concept Drift: Train on first half, test across subsequent periods.
\end{tablenotes}
\end{table}


% === results/sensitivity/sensitivity_analysis.tex ===
% Sensitivity Analysis Tables - Filo-Priori v9
% Auto-generated


\begin{table}[htbp]
\centering
\caption{Hyperparameter Sensitivity Analysis for Filo-Priori v9}
\label{tab:sensitivity}
\begin{tabular}{llccc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{N} \\
\midrule
Loss Function & Weighted CE & 0.6191 & [0.597, 0.639] & 554 \\
Loss Function & Focal Loss & 0.6117 & [0.587, 0.634] & 554 \\
Loss Function & Weighted Focal & 0.5834 & [0.550, 0.615] & 277 \\
\midrule
Learning Rate & 3e-5 & 0.6160 & [0.599, 0.633] & 831 \\
Learning Rate & 5e-5 & 0.5890 & [0.570, 0.608] & 831 \\
\midrule
GNN Architecture & 1 layer, 2 heads & 0.6160 & [0.598, 0.633] & 831 \\
GNN Architecture & 2 layers, 4 heads & 0.5890 & [0.569, 0.608] & 831 \\
\midrule
Structural Features & 10 features & 0.6171 & [0.589, 0.646] & 277 \\
Structural Features & 6 features & 0.6155 & [0.594, 0.636] & 554 \\
Structural Features & 29 features & 0.5997 & [0.571, 0.630] & 277 \\
\midrule
Balanced Sampling & No Balanced Sampling & 0.6154 & [0.601, 0.631] & 1108 \\
Balanced Sampling & Balanced Sampling & 0.5834 & [0.550, 0.616] & 277 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Note: Best configuration for each hyperparameter shown in bold (highest Mean APFD).
\item N represents the total number of build-level APFD evaluations across relevant experiments.
\end{tablenotes}
\end{table}


% === results/qualitative_analysis/case_studies.tex ===

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CASE STUDIES - QUALITATIVE ANALYSIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Studies}
\label{sec:case_studies}

We present detailed case studies of builds with varying APFD performance
to illustrate Filo-Priori's behavior in different scenarios.


\subsubsection{Case Study 1: High Performance Build}

\textbf{Build ID:} \texttt{UUG34.20}

\begin{itemize}
    \item APFD Score: 1.0000
    \item Total Test Cases: 4
    \item Total Failures: 4 (100.0\%)
    \item Optimal APFD: 0.5000
    \item Gap from Optimal: -0.5000
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Top 10 Prioritized Test Cases for UUG34.20}
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{TC Key} & \textbf{Result} & \textbf{Prob.} \\
\midrule
1 & MCA-2700042... & \textbf{Fail} & 0.395 \\
2 & MCA-2700042... & \textbf{Fail} & 0.232 \\
3 & MCA-2700042... & \textbf{Fail} & 0.232 \\
4 & MCA-2700042... & \textbf{Fail} & 0.232 \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Case Study 2: Low Performance Build}

\textbf{Build ID:} \texttt{T2TV33.13}

\begin{itemize}
    \item APFD Score: 0.1520
    \item Total Test Cases: 176
    \item Total Failures: 4 (2.3\%)
    \item Optimal APFD: 0.9886
    \item Gap from Optimal: 0.8366
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Top 10 Prioritized Test Cases for T2TV33.13}
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{TC Key} & \textbf{Result} & \textbf{Prob.} \\
\midrule
1 & MCA-3144500... & Pass & 0.534 \\
2 & MCA-3144506... & Pass & 0.534 \\
3 & MCA-3144503... & Pass & 0.533 \\
4 & MCA-3144504... & Pass & 0.533 \\
5 & MCA-3144491... & Pass & 0.532 \\
6 & MCA-2736014... & Pass & 0.531 \\
7 & MCA-3144494... & Pass & 0.531 \\
8 & MCA-3144505... & Pass & 0.531 \\
9 & MCA-2736019... & Pass & 0.530 \\
10 & MCA-3582218... & Pass & 0.518 \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Case Study 3: Medium Performance Build}

\textbf{Build ID:} \texttt{U1TC34.22-12}

\begin{itemize}
    \item APFD Score: 0.6111
    \item Total Test Cases: 18
    \item Total Failures: 4 (22.2\%)
    \item Optimal APFD: 0.8889
    \item Gap from Optimal: 0.2778
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Top 10 Prioritized Test Cases for U1TC34.22-12}
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{TC Key} & \textbf{Result} & \textbf{Prob.} \\
\midrule
1 & MCA-3178634... & Pass & 0.517 \\
2 & MCA-2345710... & Pass & 0.500 \\
3 & MCA-2345710... & Pass & 0.500 \\
4 & MCA-829313... & Pass & 0.500 \\
5 & MCA-829313... & Pass & 0.500 \\
6 & MCA-882... & \textbf{Fail} & 0.500 \\
7 & MCA-882... & \textbf{Fail} & 0.500 \\
8 & MCA-944294... & \textbf{Fail} & 0.500 \\
9 & MCA-944294... & \textbf{Fail} & 0.500 \\
10 & MCA-3178634... & Pass & 0.406 \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Case Study 4: Medium Performance Build}

\textbf{Build ID:} \texttt{U3TZ34.6}

\begin{itemize}
    \item APFD Score: 0.5773
    \item Total Test Cases: 262
    \item Total Failures: 8 (3.1\%)
    \item Optimal APFD: 0.9847
    \item Gap from Optimal: 0.4074
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Top 10 Prioritized Test Cases for U3TZ34.6}
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{TC Key} & \textbf{Result} & \textbf{Prob.} \\
\midrule
1 & MCA-3035591... & Pass & 0.517 \\
2 & MCA-2586916... & Pass & 0.500 \\
3 & MCA-2586916... & Pass & 0.500 \\
4 & MCA-3927275... & Pass & 0.500 \\
5 & MCA-3927275... & Pass & 0.500 \\
6 & MCA-3957388... & Pass & 0.500 \\
7 & MCA-3957388... & Pass & 0.500 \\
8 & MCA-4413162... & Pass & 0.500 \\
9 & MCA-4413162... & Pass & 0.500 \\
10 & MCA-4424954... & Pass & 0.500 \\
\bottomrule
\end{tabular}
\end{table}



% === results/paper_sections/all_tables.tex ===

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ALL LATEX TABLES - FILO-PRIORI V9
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Auto-generated on 2025-11-26 13:06:49
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%------------------------------------------------------------------------------
% Table 1: TCP Method Comparison
%------------------------------------------------------------------------------
\begin{table}[htbp]
\centering
\caption{Comparison of Test Case Prioritization Methods on QTA Dataset (N=277 builds)}
\label{tab:tcp_comparison}
\begin{tabular}{lccccl}
\toprule
\textbf{Method} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{$\Delta$ vs FP} & \textbf{$p$-value} & \textbf{Sig.} \\
\midrule
FailureRate & 0.6289 & [0.597, 0.660] & +0.0118 & 0.3626 & nan \\
XGBoost & 0.6171 & [0.583, 0.649] & +0.0001 & 0.5771 & nan \\
\textbf{Filo-Priori} & \textbf{0.6171} & [0.586, 0.648] & - & - & \\
GreedyHistorical & 0.6138 & [0.582, 0.647] & -0.0033 & 0.0962 & nan \\
LogisticRegression & 0.5964 & [0.559, 0.631] & -0.0207 & 0.1855 & nan \\
RandomForest & 0.5910 & [0.556, 0.627] & -0.0261 & 0.0937 & nan \\
Random & 0.5596 & [0.533, 0.584] & -0.0575 & 0.0002 & *** \\
RecentFailureRate & 0.5454 & [0.515, 0.578] & -0.0717 & 7.21e-05 & *** \\
Recency & 0.5240 & [0.490, 0.557] & -0.0931 & 1.44e-08 & *** \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item FP = Filo-Priori. $\Delta$ = difference from Filo-Priori. *** = $p < 0.001$ (Wilcoxon).
\end{tablenotes}
\end{table}


%------------------------------------------------------------------------------
% Table 2: Ablation Study
%------------------------------------------------------------------------------
\begin{table}[htbp]
\centering
\caption{Ablation Study: Component Contributions to Filo-Priori Performance}
\label{tab:ablation}
\begin{tabular}{llcccc}
\toprule
\textbf{ID} & \textbf{Removed Component} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{$\Delta$} & \textbf{Contrib.} \\
\midrule
A3 & Graph Attention & 0.5311 & [0.506, 0.557] & -0.1086 & +17.0\% *** \\
A2 & Structural Stream & 0.6060 & [0.580, 0.632] & -0.0337 & +5.3\% *** \\
A5 & Class Weighting & 0.6100 & [0.581, 0.640] & -0.0297 & +4.6\% *** \\
A7 & Ensemble & 0.6171 & [0.588, 0.651] & -0.0226 & +3.5\% *** \\
A1 & Semantic Stream & 0.6276 & [0.595, 0.658] & -0.0121 & +1.9\% nan \\
\textbf{A0} & \textbf{Full Model} & \textbf{0.6397} & [0.608, 0.669] & - & - \\
A6 & Cross-Attention & 0.6466 & [0.615, 0.674] & +0.0069 & -1.1\% nan \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item $\Delta$ = change in APFD when component is removed. Contrib. = contribution to full model.
\item *** = statistically significant ($p < 0.05$, paired t-test).
\end{tablenotes}
\end{table}


%------------------------------------------------------------------------------
% Table 3: Temporal Cross-Validation
%------------------------------------------------------------------------------
\begin{table}[htbp]
\centering
\caption{Temporal Cross-Validation Results for Filo-Priori}
\label{tab:temporal_cv}
\begin{tabular}{lccc}
\toprule
\textbf{Validation Method} & \textbf{Mean APFD} & \textbf{95\% CI} & \textbf{N Builds} \\
\midrule
Temporal 5-Fold CV & 0.6629 & [0.627, 0.698] & 215 \\
Sliding Window CV & 0.6279 & [0.595, 0.661] & 248 \\
Concept Drift Test & 0.6187 & [0.574, 0.661] & 152 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Temporal K-Fold: Train on past folds, test on future fold.
\item Sliding Window: 100-build train, 20-build test, 20-build step.
\item Concept Drift: Train on first 50\%, test across remaining periods.
\end{tablenotes}
\end{table}


%------------------------------------------------------------------------------
% Table 4: Hyperparameter Sensitivity Analysis
%------------------------------------------------------------------------------
\begin{table}[htbp]
\centering
\caption{Hyperparameter Sensitivity Analysis}
\label{tab:sensitivity}
\begin{tabular}{llcc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Mean APFD} & \textbf{95\% CI} \\
\midrule
Loss Function & \textbf{Weighted CE} & \textbf{0.6191} & [0.597, 0.639] \\
Loss Function & Focal Loss & 0.6117 & [0.587, 0.634] \\
Loss Function & Weighted Focal & 0.5834 & [0.550, 0.615] \\
\midrule
Learning Rate & \textbf{3e-5} & \textbf{0.6160} & [0.599, 0.633] \\
Learning Rate & 5e-5 & 0.5890 & [0.570, 0.608] \\
\midrule
GNN Architecture & \textbf{1 layer, 2 heads} & \textbf{0.6160} & [0.598, 0.633] \\
GNN Architecture & 2 layers, 4 heads & 0.5890 & [0.569, 0.608] \\
\midrule
Structural Features & \textbf{10 features} & \textbf{0.6171} & [0.589, 0.646] \\
Structural Features & 6 features & 0.6155 & [0.594, 0.636] \\
Structural Features & 29 features & 0.5997 & [0.571, 0.630] \\
\midrule
Balanced Sampling & \textbf{No Balanced Sampling} & \textbf{0.6154} & [0.601, 0.631] \\
Balanced Sampling & Balanced Sampling & 0.5834 & [0.550, 0.616] \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Best value for each hyperparameter shown in bold.
\end{tablenotes}
\end{table}



% === results/paper_sections/section_results.tex ===

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

This section presents the experimental results organized by research questions.
All experiments were conducted on the QTA dataset containing 277 builds with
at least one failing test case, totaling 8,847 test case executions.

%------------------------------------------------------------------------------
% RQ1: Effectiveness
%------------------------------------------------------------------------------

\subsection{RQ1: How effective is Filo-Priori compared to baseline methods?}
\label{sec:rq1}

Table~\ref{tab:tcp_comparison} presents the comparison of Filo-Priori against
eight baseline methods. We evaluate effectiveness using the Average Percentage
of Faults Detected (APFD) metric, with 95\% bootstrap confidence intervals
and Wilcoxon signed-rank tests for statistical significance.


\textbf{Key Findings:}
\begin{itemize}
    \item Filo-Priori achieves a mean APFD of 0.6171 (95\% CI: [0.586, 0.648])
    \item This represents a 10.3\% improvement over Random ordering (APFD = 0.5596)
    \item The improvement is statistically significant ($p < 0.001$, Wilcoxon signed-rank test)
    \item Filo-Priori performs comparably to FailureRate (0.6289) and XGBoost (0.6171)
    \item Filo-Priori significantly outperforms Recency-based approaches ($p < 0.001$)
\end{itemize}

\input{tables/tcp_comparison}


%------------------------------------------------------------------------------
% RQ2: Component Contributions
%------------------------------------------------------------------------------

\subsection{RQ2: What is the contribution of each architectural component?}
\label{sec:rq2}

To understand the importance of each component in Filo-Priori's architecture,
we conducted an ablation study. Table~\ref{tab:ablation} shows the impact
of removing each component on the APFD metric.


\textbf{Key Findings:}
\begin{itemize}
    \item The full model achieves APFD = 0.6397
    \item \textbf{Graph Attention (GATv2)} is the most critical component, contributing +17.0\% to performance
    \item The Structural Stream contributes +5.3\%
    \item Class Weighting contributes +4.6\%
    \item Cross-Attention shows negative contribution (-1.1\%), suggesting simpler fusion may suffice
\end{itemize}

\input{tables/ablation_study}


%------------------------------------------------------------------------------
% RQ3: Temporal Robustness
%------------------------------------------------------------------------------

\subsection{RQ3: How robust is Filo-Priori across different time periods?}
\label{sec:rq3}

Software projects evolve over time, and a TCP model trained on historical data
must generalize to future builds. We evaluated temporal robustness using three
validation strategies: Temporal K-Fold CV, Sliding Window CV, and Concept Drift
Analysis.


\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Temporal 5-Fold CV}: APFD = 0.6629 (95\% CI: [0.627, 0.698])
    \item \textbf{Sliding Window CV}: APFD = 0.6279 (95\% CI: [0.595, 0.661])
    \item \textbf{Concept Drift Test}: APFD = 0.6187 (95\% CI: [0.574, 0.661])
    \item Performance remains stable across all temporal validation methods (range: 0.619-0.663)
    \item No significant performance degradation over time, indicating robustness to concept drift
\end{itemize}

\input{tables/temporal_cv}


%------------------------------------------------------------------------------
% RQ4: Hyperparameter Sensitivity
%------------------------------------------------------------------------------

\subsection{RQ4: How sensitive is Filo-Priori to hyperparameter choices?}
\label{sec:rq4}

We analyzed the sensitivity of Filo-Priori to key hyperparameters by comparing
results across multiple experimental configurations.


\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Loss Function}: Weighted Cross-Entropy performs best (APFD = 0.6191), with sensitivity range $\Delta$ = 0.036
    \item \textbf{Learning Rate}: Lower rate (3e-5) outperforms higher rate (5e-5), $\Delta$ = 0.027
    \item \textbf{GNN Architecture}: Simpler architecture (1 layer, 2 heads) performs best, $\Delta$ = 0.027
    \item \textbf{Structural Features}: 10 selected features outperform both 6 (baseline) and 29 (expanded)
    \item \textbf{Balanced Sampling}: Not recommended for ranking tasks; degrades performance
\end{itemize}

The model shows moderate sensitivity to hyperparameters, with loss function
choice having the largest impact (5.9\% relative variation).

\input{tables/sensitivity_analysis}



% === results/paper_sections/section_discussion.tex ===

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discussion}

\subsection{Key Insights}

Our experimental evaluation reveals several important insights about
deep learning-based test case prioritization:

\textbf{1. Graph Neural Networks are Essential.}
The ablation study (RQ2) demonstrates that Graph Attention Networks (GATv2)
contribute the most to Filo-Priori's performance (+17.0\%). This confirms
our hypothesis that modeling test case relationships through a phylogenetic
graph captures valuable structural information that traditional approaches miss.
The multi-edge graph, which represents co-failure, co-success, and semantic
similarity relationships, enables the model to learn complex dependencies
between test cases.

\textbf{2. Simpler Architectures May Suffice.}
Contrary to initial expectations, our sensitivity analysis (RQ4) shows that
simpler GNN architectures (1 layer, 2 heads) outperform deeper ones
(2 layers, 4 heads). This suggests that the phylogenetic relationships in
our dataset can be captured with shallow message passing, and deeper
architectures may introduce unnecessary complexity or overfitting.

\textbf{3. Feature Engineering Matters.}
The structural features contribute +5.3\% to performance, but more is not
always better. The 10-feature configuration outperforms both the minimal
(6 features) and expanded (29 features) versions, indicating the importance
of careful feature selection over feature quantity.

\textbf{4. Temporal Robustness is Achievable.}
Filo-Priori maintains consistent performance across temporal validation
methods (APFD range: 0.619-0.663), demonstrating that the learned patterns
generalize well to future builds. This is crucial for practical deployment
in CI/CD pipelines where models must handle evolving codebases.

\subsection{Comparison with Related Work}

Our results are consistent with recent findings in the TCP literature.
The APFD of 0.62-0.66 achieved by Filo-Priori is comparable to state-of-the-art
methods reported in recent surveys~\cite{tcp_survey_2023}. However, direct
comparison is challenging due to differences in datasets and evaluation
protocols.

Notably, Filo-Priori performs comparably to simpler baselines like FailureRate
and XGBoost. This raises an important question: when is the additional
complexity of deep learning justified? Our analysis suggests that the
deep learning approach provides:

\begin{enumerate}
    \item \textbf{End-to-end learning}: No manual feature engineering for text data
    \item \textbf{Relationship modeling}: Capture of inter-test dependencies
    \item \textbf{Scalability}: Potential for transfer learning across projects
\end{enumerate}

\subsection{Practical Implications}

For practitioners considering Filo-Priori:

\begin{itemize}
    \item \textbf{Training data}: At least 100 builds with failure history recommended
    \item \textbf{Retraining}: Weekly or after major codebase changes
    \item \textbf{Configuration}: Use Weighted CE loss, learning rate 3e-5, 1-layer GNN
    \item \textbf{Expected improvement}: +10-15\% APFD over random ordering
\end{itemize}

\subsection{Limitations}

Several limitations should be considered:

\begin{enumerate}
    \item \textbf{Single dataset}: Results are based on one industrial dataset (QTA)
    \item \textbf{Binary classification}: We treat TCP as ranking based on failure probability
    \item \textbf{Cold start}: New test cases without history may be poorly ranked
    \item \textbf{Computational cost}: Training requires GPU and takes 30-60 minutes
\end{enumerate}



% === results/paper_sections/section_threats.tex ===

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: THREATS TO VALIDITY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Threats to Validity}
\label{sec:threats}

\subsection{Internal Validity}

\textbf{Implementation correctness.} We mitigated implementation bugs through
code reviews and comparison with reference implementations. The APFD calculation
follows the standard formula from the literature.

\textbf{Randomness.} We set random seeds (42) for reproducibility and report
results with 95\% bootstrap confidence intervals. All experiments were
repeated with consistent initialization.

\textbf{Hyperparameter tuning.} Hyperparameters were selected based on
validation set performance, not test set results. The sensitivity analysis
confirms that results are robust to reasonable hyperparameter variations.

\subsection{External Validity}

\textbf{Dataset representativeness.} The QTA dataset represents a single
industrial project. Results may not generalize to all software projects,
particularly those with different testing practices or failure patterns.

\textbf{Temporal generalization.} While we demonstrated temporal robustness
within the dataset's time span, long-term performance in production environments
remains to be validated.

\subsection{Construct Validity}

\textbf{APFD metric.} APFD is widely used in TCP research but assumes equal
importance of all faults. In practice, some failures may be more critical.

\textbf{Baseline selection.} We compared against common baselines from the
literature. More recent or specialized methods may show different relative
performance.


