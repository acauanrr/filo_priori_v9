# PHASE II: ARCHITECTURAL REFINEMENT - Experiment 009
# Objective: Improve semantic stream with Attention Pooling
# Expected: Test F1 Macro 0.68-0.70 → 0.70-0.72 (+2-3 pp cumulative)
#
# CHANGES from Experiment 007:
# - Structural stream: GAT → GATv2 (from Exp 008)
# - Semantic stream: Mean Pooling → Attention Pooling (NEW in Exp 009)
#
# Baseline: Experiment 008 (Expected F1 Macro ≈ 0.68-0.70)

# Data Configuration (SAME AS EXP 007)
data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_009_attention_pooling"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  # Binary Classification
  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_all"

  # NO DATA AUGMENTATION
  use_smote: false
  use_resampling: false

# Text Processing (SAME AS EXP 007)
text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

# Embedding Configuration (SAME AS EXP 007)
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

# Phylogenetic Structure (SAME AS EXP 007)
phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

# Model Architecture - CHANGES: GATv2 + Attention Pooling
model:
  # Semantic Stream - CHANGE: Attention Pooling
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"
    pooling_type: "attention"  # ⭐ NEW: Changed from "mean" to "attention"
    attention_pooling:          # ⭐ NEW: Attention pooling config
      dropout: 0.1
      use_layer_norm: true

  # Structural Stream - CHANGE: GATv2 (from Exp 008)
  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 10
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"  # From Exp 008
    use_residual: true

  # Cross-Attention Fusion - NO CHANGE
  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "concatenation"

  # Classification Head - NO CHANGE
  classifier:
    hidden_dim: 128
    num_classes: 2
    dropout: 0.5
    classifier_type: "simple"

# Training Configuration (SAME AS EXP 007)
training:
  num_epochs: 80
  batch_size: 64
  learning_rate: 5.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 1000
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: true
  threshold_search_range: [0.20, 0.80]

  # Loss Configuration (SAME AS EXP 007)
  loss:
    type: "focal"
    focal_alpha: [0.25, 0.75]
    use_class_weights: false
    focal_gamma: 2.0
    label_smoothing: 0.05

  # Optimization (SAME AS EXP 007)
  optimizer: "adamw"
  scheduler: "cosine"

  # Early Stopping (SAME AS EXP 007)
  early_stopping:
    patience: 12
    min_delta: 5.0e-4
    monitor: "f1_macro"
    mode: "max"

  # Checkpointing
  save_every_n_epochs: 5
  save_best_only: true

# Evaluation Configuration (SAME AS EXP 007)
evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"
    - "roc_auc"
    - "auprc"

# Hardware Configuration (SAME AS EXP 007)
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Logging
logging:
  use_tensorboard: true
  log_every_n_steps: 50
  log_dir: "results/experiment_009_attention_pooling/logs"
  save_predictions: true

# ADVANCED MONITORING (SAME AS EXP 007)
monitoring:
  enabled: true
  check_every_n_batches: 50
  prediction_diversity_threshold: 0.05
  gradient_norm_max: 100.0
  gradient_norm_min: 1e-6
  loss_spike_multiplier: 5.0
  auto_terminate_on_collapse: true

# SUCCESS CRITERIA (Phase II: Architectural Refinement)
success_criteria:
  phase: "architectural_refinement"
  required_metrics:
    f1_macro: 0.70  # Target: 0.68-0.70 + 0.02-0.03
    accuracy: 0.60
    recall_pass: 0.50
    prediction_diversity: 0.30
  blocking_failures:
    - "prediction_collapse"
    - "performance_regression"  # F1 < 0.67 (below Exp 008 - tolerance)

# Experiment Metadata
experiment:
  phase: "II"
  phase_name: "Architectural Refinement"
  experiment_id: "009"
  description: "Add Attention Pooling to semantic stream (cumulative with GATv2)"
  hypothesis: "Learned token importance via attention pooling will better aggregate semantic features, improving F1 Macro by additional 2-3 pp"
  baseline_experiment: "008"
  baseline_metrics:
    f1_macro: 0.69  # Expected from 008 (midpoint of 0.68-0.70)
    accuracy: 0.88
  expected_improvements:
    - "F1 Macro: 0.69 → 0.70-0.72 (+1-3 pp over Exp 008)"
    - "F1 Macro: 0.6677 → 0.70-0.72 (+3-5 pp over Exp 007)"
    - "Better semantic feature extraction"
    - "More interpretable (attention weights available)"
  rationale: |
    Attention pooling learns which tokens in the test description are most
    important for prediction, rather than treating all tokens equally.
    This should improve semantic feature quality, especially for long texts.
