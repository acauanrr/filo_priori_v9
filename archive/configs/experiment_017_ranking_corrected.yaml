# PHASE II: PIPELINE CORRECTION - Experiment 017
# Objective: Fix fundamental pipeline issues from Exp 016_01
#
# KEY CORRECTIONS:
# 1. GROUP-AWARE SPLIT (Anti-leakage): GroupShuffleSplit by Build_ID
# 2. GROUPED BATCH SAMPLER (Pro-ranking): Batches contain samples from same build
# 3. CURRICULUM LEARNING: Ranking loss starts after classification is learned
# 4. IMPROVED MONITORING: AUPRC macro + probability calibration
#
# Expected Impact:
# - Better generalization to unseen builds
# - Effective ranking loss within builds
# - Smoother training convergence
# - Better calibrated probabilities for APFD

data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_017_ranking_corrected"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_fail"  # Only Pass and Fail samples
  use_smote: false
  use_resampling: false

text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

  # Multi-field embedding configuration
  use_multi_field: true
  field_embedding_dim: 1024

  # Fusion strategy
  fusion_strategy: "projection_concat"
  fusion_num_heads: 4

  # Field definitions
  fields:
    - name: "summary"
      source: "TE_Summary"
      weight: 0.40

    - name: "steps"
      source: "TC_Steps"
      weight: 0.10

    - name: "commits"
      source: "commit_processed"
      weight: 0.35

    - name: "CR"
      source: ["CR_Type_processed", "CR_Component_processed"]
      weight: 0.15

phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

  # Rewired graph integration
  use_rewired_graph: true
  rewired_graph_path: "results/experiment_017_ranking_corrected/rewiring/rewired_graph.pt"

model:
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"

  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 20        # UPDATED: 10 → 20 (more graph context)
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"
    num_heads: 4
    use_residual: false

    use_denoising_gate: false
    denoising_gate_type: "mlp"
    denoising_gate_mode: "basic"
    denoising_hard_threshold: null
    denoising_neighbor_dropout: 0.0

  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "concatenation"

  classifier:
    hidden_dim: 128
    num_classes: 2
    dropout: 0.5
    classifier_type: "simple"

training:
  num_epochs: 20
  batch_size: 64
  learning_rate: 5.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 100
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: true       # ENABLED: Optimize decision threshold for imbalanced data
  calibrate_probabilities: true  # Enable temperature scaling for better APFD

  loss:
    type: "focal"
    focal_alpha: [0.05, 0.95]  # CRITICAL FIX: Heavy weight on minority class (Fail)
    use_class_weights: true    # RE-ENABLED: Necessary to prevent collapse
    focal_gamma: 2.0
    label_smoothing: 0.05      # CORRECTED: Reduced from 016_01 (0.10 → 0.05)

  optimizer: "adamw"
  scheduler: "cosine"

  # CORRECTED RANKING CONFIGURATION
  ranking:
    enabled: true
    use_grouped_sampler: true  # NEW: Use GroupedBatchSampler instead of WeightedRandomSampler

    # CURRICULUM LEARNING: Start ranking LATER to allow classification learning
    start_epoch: 8             # DELAYED: Start ranking loss at epoch 8 (was 5)
    ramp_epochs: 4             # NEW: Linear ramp from epoch 8 to 12

    weight: 0.10               # REDUCED: 0.15 → 0.10 (even less conflict with class weights)
    loss_type: logistic        # CORRECTED: logistic (smooth) instead of margin
    score_type: prob           # CORRECTED: prob (aligns with APFD) instead of logit
    margin: 0.05               # For prob-based logistic loss
    hard_negative_top_k: 5
    hard_negative_percent: 0.2
    max_pairs_per_build: 100
    metrics:
      ndcg_ks: [5, 10]
      recall_at_percent: [0.1]

  early_stopping:
    patience: 8                # INCREASED: More patience for delayed curriculum
    min_delta: 5.0e-4
    monitor: "auprc_macro"     # CORRECTED: Monitor AUPRC instead of F1
    mode: "max"

  save_every_n_epochs: 0
  save_best_only: false

  # NEW: choose how to build edges during training
  graph_mode: "global_rewired"

evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"

hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

logging:
  use_tensorboard: false
  log_every_n_steps: 50
  log_dir: "results/experiment_017_ranking_corrected/logs"
  save_predictions: true

experiment:
  phase: "II"
  phase_name: "Pipeline Correction"
  experiment_id: "017"
  description: "Fix pipeline issues from 016_01: Group-aware split, Grouped batching, Curriculum learning"
