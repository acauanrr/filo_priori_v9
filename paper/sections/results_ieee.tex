%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: RESULTS (IEEE TSE Format)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: Section header is in main_ieee_tse.tex

This section presents experimental results organized by research questions.
All experiments were conducted on the industrial QTA dataset containing
4,552 builds with 100,502 test executions (277 builds with failures).

%------------------------------------------------------------------------------
% RQ1: Effectiveness
%------------------------------------------------------------------------------

\subsection{RQ1: Effectiveness Comparison}
\label{sec:rq1}

Table~\ref{tab:tcp_comparison} presents the comparison of \filopriori{} against
state-of-the-art TCP methods. We use APFD as the primary metric with Wilcoxon
signed-rank tests for statistical significance and Cliff's delta for effect size.

\begin{table}[!t]
\centering
\caption{Comparison with State-of-the-Art TCP Methods}
\label{tab:tcp_comparison}
\footnotesize
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{APFD} & \textbf{Std} & \textbf{p-value} & \textbf{Effect} & \textbf{$\Delta$} \\
\midrule
\textbf{\filopriori{}} & \textbf{0.7595} & \textbf{0.189} & -- & -- & -- \\
DeepOrder~\cite{chen2023deeporder} & 0.6919 & 0.267 & $<$0.001*** & small & +9.8\% \\
NodeRank~\cite{li2024noderank} & 0.6623 & 0.270 & $<$0.001*** & small & +14.7\% \\
Random & 0.5000 & -- & $<$0.001*** & large & +51.9\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item *** p $<$ 0.001 (Wilcoxon signed-rank test vs \filopriori{})
\item Effect sizes measured using Cliff's delta.
\end{tablenotes}
\end{table}

\textbf{Key Findings for RQ1:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.7595} (std: 0.189),
    representing a \textbf{51.9\%} improvement over random ordering.
    \item \filopriori{} \textbf{significantly outperforms} NodeRank~\cite{li2024noderank}
    by \textbf{+14.7\%} ($p < 0.001$, Cliff's delta = 0.179, small effect).
    \item \filopriori{} also outperforms DeepOrder~\cite{chen2023deeporder}
    by \textbf{+9.8\%} ($p < 0.001$), demonstrating the effectiveness of our
    dual-stream graph attention architecture.
    \item Per-build analysis shows \filopriori{} outperforms NodeRank in
    \textbf{52.7\%} of builds (vs 31.0\% for NodeRank, 16.2\% ties).
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ1]
\filopriori{} achieves APFD=0.7595, significantly outperforming state-of-the-art
methods: NodeRank ($+14.7\%$, $p < 0.001$) and DeepOrder ($+9.8\%$, $p < 0.001$).
The dual-stream GAT architecture with semantic and structural features provides
superior fault detection compared to existing GNN-based approaches.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ1b: Cross-Dataset Validation (RTPTorrent)
%------------------------------------------------------------------------------

\subsection{Cross-Dataset Validation: RTPTorrent}
\label{sec:rq1_rtptorrent}

To evaluate generalization beyond the industrial dataset, we conducted a comprehensive
experiment on the RTPTorrent open-source benchmark~\cite{mattis2020rtptorrent}, which
contains test execution histories from 20 Java projects with over 100,000 Travis CI builds.
We adapted \filopriori{} using a LightGBM LambdaRank model with 16 ranking-optimized
features, trained independently on each project.

\begin{table}[!t]
\centering
\caption{RTPTorrent Cross-Dataset Results (20 Projects)}
\label{tab:rtptorrent_results}
\footnotesize
\begin{tabular}{lccr}
\toprule
\textbf{Baseline} & \textbf{APFD} & \textbf{Model Improv.} & \textbf{p-value} \\
\midrule
\textbf{Filo-Priori V10} & \textbf{0.8376} & -- & -- \\
\midrule
recently\_failed & 0.8209 & +2.02\% & sig.* \\
random & 0.4940 & +69.56\% & $<$0.001*** \\
untreated & 0.3574 & +134.32\% & $<$0.001*** \\
matrix\_naive & 0.5693 & +47.11\% & $<$0.001*** \\
matrix\_conditional & 0.5132 & +63.21\% & $<$0.001*** \\
optimal\_duration & 0.5934 & +41.15\% & $<$0.001*** \\
optimal\_failure (oracle) & 0.9249 & -9.45\% & -- \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Aggregate results over 1,250 test builds from 20 Java projects.
\item * Statistically significant improvement over strongest baseline.
\end{tablenotes}
\end{table}

Table~\ref{tab:rtptorrent_top5} shows the top-5 performing projects:

\begin{table}[!t]
\centering
\caption{Top-5 Projects by APFD (RTPTorrent)}
\label{tab:rtptorrent_top5}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Project} & \textbf{APFD} & \textbf{Builds} & \textbf{vs recent.} \\
\midrule
apache/sling & 0.9922 & 163 & +2.18\% \\
neuland/jade4j & 0.9799 & 20 & +0.88\% \\
eclipse/jetty.project & 0.9789 & 66 & +1.27\% \\
facebook/buck & 0.9722 & 69 & +1.97\% \\
deeplearning4j/dl4j & 0.9277 & 114 & +0.46\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for Cross-Dataset Validation:}
\begin{itemize}
    \item \filopriori{} achieves a mean APFD of \textbf{0.8376} across 20 diverse
    Java projects (1,250 test builds), demonstrating strong cross-domain generalization.
    \item The model \textbf{outperforms 6 out of 7 baselines}, including the strongest
    heuristic baseline (recently\_failed) by +2.02\%.
    \item Only the oracle baseline (optimal\_failure) achieves higher APFD (0.9249),
    representing the theoretical upper bound with perfect failure knowledge.
    \item Performance varies across projects (APFD: 0.29--0.99), with best results
    on projects with consistent failure patterns (e.g., apache/sling).
    \item The most important features are \texttt{novelty\_score}, \texttt{base\_risk},
    \texttt{execution\_frequency}, and \texttt{time\_decay\_score}.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Cross-Dataset Finding]
\filopriori{} generalizes effectively to open-source projects, achieving APFD=0.8376
across 20 RTPTorrent projects and outperforming 6 of 7 baselines. This validates
the approach on a well-known benchmark with limited semantic information.
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ2: Component Contributions
%------------------------------------------------------------------------------

\subsection{RQ2: Ablation Study}
\label{sec:rq2}

To understand the contribution of each component, we conducted an ablation study
isolating five key innovations. Table~\ref{tab:ablation} shows the impact of
removing each component from the full system (APFD=0.7595).

\begin{table}[!t]
\centering
\caption{Ablation Study: Component Contributions to APFD}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{APFD} & \textbf{$\Delta$} & \textbf{p-value} \\
\midrule
Full Model & 0.7595 & -- & -- \\
\midrule
w/o Dense Multi-Edge Graph & 0.6835 & -10.0\% & $<$0.001*** \\
w/o Orphan KNN Scoring & 0.7145 & -5.9\% & $<$0.001*** \\
w/o Single Balancing & 0.7291 & -4.0\% & $<$0.001*** \\
w/o DeepOrder Features & 0.7443 & -2.0\% & 0.003** \\
w/o Threshold Optimization & 0.7519 & -1.0\% & 0.042* \\
\midrule
Baseline (V1) & 0.6503 & -14.4\% & $<$0.001*** \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001 (Wilcoxon signed-rank test)
\end{tablenotes}
\end{table}

\textbf{Component Analysis:}

\textbf{(1) Dense Multi-Edge Graph (+10.0\%):} The largest contribution comes from
increasing graph connectivity. Key changes: semantic threshold 0.75$\rightarrow$0.65,
top-k neighbors 5$\rightarrow$10, and adding temporal/component edges. This increased
in-graph coverage from 50--60\% to \textbf{77.4\%}.

\textbf{(2) Orphan KNN Scoring (+5.9\%):} Orphan tests (22.6\% of total) previously
received uniform scores, destroying ranking. Our KNN pipeline with Euclidean distance,
structural blending ($w=0.35$), and temperature scaling ($T=0.7$) restores meaningful
score variance (std=0.046 vs. 0.000).

\textbf{(3) Single Balancing (+4.0\%):} Prior work used triple compensation (class weights
+ balanced sampling + focal alpha), causing mode collapse. Using \emph{only} balanced
sampling (15:1 ratio) with neutral $\alpha=0.5$ stabilizes training and prevents
over-prediction of the minority class.

\textbf{(4) DeepOrder Features (+2.0\%):} Temporal execution features (execution\_status\_last\_[1,2,3,5,10],
cycles\_since\_last\_fail) capture recent failure patterns that static features miss.

\textbf{(5) Threshold Optimization (+1.0\%):} Two-phase search finding threshold 0.28
(vs. default 0.50) improves minority class recall from 5\% to 30\%.

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ2]
The five key innovations contribute cumulatively to the +16.8\% improvement over
baseline: Dense Multi-Edge Graph (+10.0\%), Orphan KNN Scoring (+5.9\%),
Single Balancing (+4.0\%), DeepOrder Features (+2.0\%), and Threshold Optimization (+1.0\%).
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ3: Temporal Robustness
%------------------------------------------------------------------------------

\subsection{RQ3: Temporal Validation}
\label{sec:rq3}

To evaluate generalization to future builds, we used temporal cross-validation
where training data always precedes test data chronologically.

\begin{table}[!t]
\centering
\caption{Temporal Cross-Validation Results}
\label{tab:temporal_cv}
\begin{tabular}{lcc}
\toprule
\textbf{Validation Method} & \textbf{APFD} & \textbf{Folds} \\
\midrule
Temporal 5-Fold CV & 0.7821 & 5 \\
Sliding Window CV & 0.7412 & 10 \\
Concept Drift Test & 0.7298 & 3 \\
\midrule
\textbf{Average} & \textbf{0.7510} & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for RQ3:}
\begin{itemize}
    \item Performance remains stable across temporal validation methods,
    with APFD ranging from 0.73 to 0.78.
    \item Concept drift causes only 3\% degradation, indicating robustness
    to evolving test patterns over time.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ3]
\filopriori{} demonstrates robust temporal generalization (APFD: 0.73--0.78),
with minimal concept drift degradation (3\%).
\end{tcolorbox}

%------------------------------------------------------------------------------
% RQ4: Sensitivity Analysis
%------------------------------------------------------------------------------

\subsection{RQ4: Hyperparameter Sensitivity}
\label{sec:rq4}

We analyzed sensitivity to key hyperparameters across multiple configurations.

\begin{table}[!t]
\centering
\caption{Hyperparameter Sensitivity Analysis}
\label{tab:sensitivity}
\begin{tabular}{llcc}
\toprule
\textbf{Parameter} & \textbf{Values} & \textbf{Best} & \textbf{Impact} \\
\midrule
Loss Function & CE, Focal, W.~Focal & W.~Focal & 4.7\% \\
Focal Gamma & 1.5, 2.0, 2.5, 3.0 & 2.5 & 4.5\% \\
Learning Rate & 1e-5, 3e-5, 5e-5, 1e-4 & 3e-5 & 3.6\% \\
GNN Layers & 1, 2, 3 & 1 & 3.6\% \\
GNN Heads & 1, 2, 4, 8 & 2 & 2.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings for RQ4:}
\begin{itemize}
    \item \textbf{Loss Function}: Weighted Focal Loss has the largest impact (4.7\%).
    \item \textbf{Architecture}: Simpler configuration (1 layer, 2 heads) performs
    best, avoiding overfitting on the test graph.
\end{itemize}

\begin{tcolorbox}[colback=gray!5,colframe=gray!50,title=Answer to RQ4]
Optimal configuration: Weighted Focal Loss ($\gamma=2.5$), learning rate 3e-5,
1-layer GAT with 2 heads. Loss function choice has the largest impact (4.7\%).
\end{tcolorbox}
