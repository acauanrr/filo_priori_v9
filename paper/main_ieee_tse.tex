%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FILO-PRIORI V9 - IEEE TRANSACTIONS ON SOFTWARE ENGINEERING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Target: IEEE Transactions on Software Engineering (IEEE TSE)
% Template: IEEEtran (IEEE Computer Society Transactions)
%
% Structure:
%   - sections/introduction.tex
%   - sections/background.tex
%   - sections/related_work.tex
%   - sections/approach.tex
%   - sections/experimental_design.tex
%   - sections/results.tex
%   - sections/discussion.tex
%   - sections/threats.tex
%   - sections/conclusion.tex
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,journal,compsoc]{IEEEtran}

%------------------------------------------------------------------------------
% PACKAGES
%------------------------------------------------------------------------------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{balance}

% For code listings
\usepackage{listings}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  breaklines=true,
  frame=single
}

% For subfigures
\usepackage{subcaption}

% For table notes
\usepackage{threeparttable}

% For colored boxes (findings summary)
\usepackage[most]{tcolorbox}

%------------------------------------------------------------------------------
% CUSTOM COMMANDS
%------------------------------------------------------------------------------
\newcommand{\filopriori}{\textsc{Filo-Priori}}
\newcommand{\apfd}{\textsc{APFD}}
\newcommand{\gatv}{GATv2}

% Highlight for revision
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\rev}[1]{\textcolor{blue}{#1}}

%------------------------------------------------------------------------------
% DOCUMENT START
%------------------------------------------------------------------------------
\begin{document}

%------------------------------------------------------------------------------
% TITLE
%------------------------------------------------------------------------------
\title{Filo-Priori: A Dual-Stream Deep Learning Approach to Test Case Prioritization}

%------------------------------------------------------------------------------
% AUTHORS
%------------------------------------------------------------------------------
\author{
  \IEEEauthorblockN{Acauan C. Ribeiro\IEEEauthorrefmark{1},
    Eduardo L. Feitosa\IEEEauthorrefmark{1},
    Andre L. da Costa Carvalho\IEEEauthorrefmark{1},\\
    Eulanda M. dos Santos\IEEEauthorrefmark{1},
    Bruno F. Gadelha\IEEEauthorrefmark{1},
    Yan R. Soares\IEEEauthorrefmark{1},
    and Jose Nascimento\IEEEauthorrefmark{2}}
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Instituto de Computa\c{c}\~{a}o (IComp)\\
    Universidade Federal do Amazonas (UFAM)\\
    Manaus, AM, Brazil\\
    \{acauan.ribeiro, efeitosa, andre, emsantos, bruno, yan.soares\}@icomp.ufam.edu.br}
  \IEEEauthorblockA{\IEEEauthorrefmark{2}Motorola Mobility LLC\\
    Manaus, AM, Brazil\\
    josern@motorola.com}
}

%------------------------------------------------------------------------------
% HEADERS
%------------------------------------------------------------------------------
\markboth{IEEE Transactions on Software Engineering, Vol. XX, No. X, Month 2026}%
{Ribeiro \MakeLowercase{\textit{et al.}}: Filo-Priori: Deep Learning-based Test Case Prioritization}

%------------------------------------------------------------------------------
% ABSTRACT
%------------------------------------------------------------------------------
\IEEEtitleabstractindextext{%
\begin{abstract}
Test Case Prioritization (TCP) aims to order test cases to maximize early fault
detection in Continuous Integration (CI) environments. Existing approaches treat
software versions as linear time series, failing to capture the complex relationships
between test cases that characterize real-world testing. We propose a
\textbf{dual-stream architecture} that combines semantic understanding of test
cases with structural patterns learned from execution history.

We present \filopriori{}, a deep learning approach that introduces:
(1) a \textbf{Multi-Edge Test Relationship Graph} encoding co-failure,
co-success, and semantic similarity relationships between test cases;
(2) a \textbf{Dual-Stream Architecture} combining semantic embeddings (SBERT)
with Graph Attention Networks (GAT) for structural feature learning;
(3) \textbf{Cross-Attention Fusion} for dynamic modality combination; and
(4) \textbf{Optimized Loss Functions} for handling severe class imbalance (37:1).

We evaluate \filopriori{} on three complementary datasets: (1) an industrial
dataset with 100,502 test executions, (2) GNN benchmark datasets (Cora, CiteSeer,
PubMed), and (3) RTPTorrent with 20 open-source Java projects. Results show that
\filopriori{} achieves: (a) APFD=\textbf{0.7595} on industrial data, outperforming
NodeRank by $+14.7\%$ ($p < 0.001$) and DeepOrder by $+9.8\%$; (b) APFD=\textbf{0.796}
on GNN benchmarks, surpassing NodeRank on 2/3 datasets (Cora: $+3.4\%$, CiteSeer:
$+1.3\%$); and (c) APFD=\textbf{0.838} on RTPTorrent, outperforming 6/7 baselines.

This work demonstrates the effectiveness of combining semantic and structural
information for test prioritization through graph neural networks, achieving
state-of-the-art results across diverse domains. Our replication package is publicly available.
\end{abstract}

%------------------------------------------------------------------------------
% KEYWORDS
%------------------------------------------------------------------------------
\begin{IEEEkeywords}
Test Case Prioritization, Graph Attention Networks, Deep Learning,
Continuous Integration, Dual-Stream Architecture, Class Imbalance
\end{IEEEkeywords}
}

\maketitle

%------------------------------------------------------------------------------
% SECTION 1: INTRODUCTION
%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{C}{ontinuous} Integration (CI) has become a fundamental practice
in modern software development, enabling teams to integrate code changes frequently
and detect defects early~\cite{hilton2016usage, fowler2006continuous}. A key
challenge in CI environments is managing the growing test suite: as software
evolves, the number of test cases increases, making it impractical to execute
all tests for every commit~\cite{memon2017taming}.

Test Case Prioritization (TCP) addresses this challenge by ordering test cases
to maximize early fault detection~\cite{rothermel2001prioritizing, elbaum2002test}.
The goal is to execute tests most likely to fail first, providing faster feedback
to developers. The effectiveness of TCP is typically measured using the Average
Percentage of Faults Detected (APFD) metric~\cite{rothermel1999test}.

\textbf{The Challenge of Test Relationships.} Existing TCP approaches, whether based
on coverage~\cite{rothermel2001prioritizing}, historical failure~\cite{kim2002history},
or machine learning~\cite{spieker2017reinforcement, pan2022test}, often treat
test cases as independent entities. This assumption ignores the rich relationships
between test cases: tests that fail together often indicate related functionality,
tests with similar descriptions target similar code, and historical patterns
reveal systematic dependencies~\cite{german2009evolution}.

\textbf{A Dual-Stream Approach.} We propose combining two complementary information
sources for TCP: (1) \emph{semantic information} from test descriptions and commit
messages, and (2) \emph{structural information} from test execution history and
test relationships. This dual-stream approach captures both what tests do
(semantics) and how they behave (structure).

Table~\ref{tab:feature_types} presents the two types of features we combine:

\begin{table}[h]
\centering
\caption{Dual-Stream Feature Types}
\label{tab:feature_types}
\begin{tabular}{ll}
\toprule
\textbf{Semantic Features} & \textbf{Structural Features} \\
\midrule
Test case summary & Historical failure rate \\
Test case steps & Recent failure trend \\
Commit messages & Test age (builds since first run) \\
Code changes (diff) & Flakiness rate \\
-- & Co-failure relationships \\
-- & Consecutive failure streaks \\
\bottomrule
\end{tabular}
\end{table}

The key insight is that semantic similarity alone is insufficient---tests with
similar descriptions may have very different failure patterns. By combining
semantic and structural information through Graph Attention Networks, we can
learn which tests are likely to fail based on both their content and their history.

In this paper, we present \filopriori{}, a deep learning approach for
Test Case Prioritization that combines semantic and structural information through
four key innovations:

\begin{enumerate}
    \item \textbf{Multi-Edge Test Relationship Graph}: We construct a graph that
    captures multiple types of relationships between test cases: co-failure edges
    (tests that fail together), co-success edges (tests that pass together), and
    semantic similarity edges (tests with similar descriptions). This multi-edge
    approach increases graph density and captures complementary relationships.

    \item \textbf{Dual-Stream Architecture}: We employ a dual-stream model that
    processes semantic features (SBERT embeddings of test descriptions and commit
    messages) and structural features (historical execution patterns) through
    separate neural networks before fusion.

    \item \textbf{Graph Attention Networks for Structural Learning}: We use Graph
    Attention Networks (GAT)~\cite{velickovic2018graph} to learn representations
    that capture test relationships, allowing the model to propagate information
    between related test cases with learned attention weights.

    \item \textbf{Weighted Focal Loss for Class Imbalance}: We employ Weighted
    Focal Loss~\cite{lin2017focal} to address the severe class imbalance (37:1
    Pass:Fail ratio) inherent in test execution data, focusing training on
    hard-to-classify examples while down-weighting easy negatives.
\end{enumerate}

We evaluate \filopriori{} on three complementary datasets: (1) an industrial
dataset with 277 builds and 52,102 test executions, (2) GNN benchmark datasets
(Cora, CiteSeer, PubMed) for direct comparison with NodeRank, and (3) RTPTorrent
with 20 open-source Java projects. Our evaluation addresses four research questions:

\begin{itemize}
    \item \textbf{RQ1}: How effective is \filopriori{} compared to baseline methods?
    \item \textbf{RQ2}: What is the contribution of each architectural component?
    \item \textbf{RQ3}: How robust is \filopriori{} across different time periods?
    \item \textbf{RQ4}: How sensitive is \filopriori{} to hyperparameter choices?
\end{itemize}

Our results show that \filopriori{} achieves consistent state-of-the-art performance:
(a) \textbf{APFD=0.7595} on industrial data, outperforming NodeRank by 14.7\%
($p < 0.001$) and DeepOrder by 9.8\%; (b) \textbf{APFD=0.796} on GNN benchmarks,
surpassing NodeRank on Cora (+3.4\%) and CiteSeer (+1.3\%); and (c) \textbf{APFD=0.838}
on RTPTorrent, outperforming 6/7 baselines. The ablation study reveals that the
multi-edge graph construction is the most critical component (+10.0\%).

\textbf{Contributions.} This paper makes the following contributions:
\begin{itemize}
    \item \textbf{Architectural}: We propose a dual-stream neural architecture
    that combines semantic embeddings (SBERT) with Graph Attention Networks for
    learning test relationships, using cross-attention fusion for modality combination.
    \item \textbf{Graph Construction}: We introduce a multi-edge test relationship
    graph that captures co-failure, co-success, and semantic similarity relationships,
    providing richer structure than single-edge approaches.
    \item \textbf{Feature Engineering}: We identify 10 discriminative structural
    features from an initial set of 29, selected through importance analysis and
    correlation filtering.
    \item \textbf{Empirical}: We demonstrate state-of-the-art performance across
    three domains: industrial TCP (APFD=0.7595, +14.7\% over NodeRank), GNN benchmarks
    (APFD=0.796, beating NodeRank on 2/3 datasets), and open-source projects (APFD=0.838).
    \item \textbf{Practical}: We provide a complete replication package enabling
    reproducibility and extension of our approach.
\end{itemize}

\textbf{Paper Organization.} Section~\ref{sec:background} presents background
concepts. Section~\ref{sec:related} discusses related work. Section~\ref{sec:approach}
describes our approach. Section~\ref{sec:experimental} presents the experimental
design. Section~\ref{sec:results} reports results. Section~\ref{sec:discussion}
discusses findings. Section~\ref{sec:threats} addresses threats to validity.
Section~\ref{sec:conclusion} concludes.

%------------------------------------------------------------------------------
% SECTION 2: BACKGROUND
%------------------------------------------------------------------------------
\section{Background}
\label{sec:background}

This section introduces fundamental concepts underlying our approach.

\subsection{Test Case Prioritization}
\label{sec:bg_tcp}

Test Case Prioritization (TCP) is the process of ordering test cases for execution
to achieve certain objectives, such as maximizing early fault detection~\cite{rothermel2001prioritizing}.
Formally, given a test suite $T$ and a permutation function $PT$ that produces
ordered sequences of $T$, TCP aims to find an optimal ordering $T' \in PT$ that
maximizes a given objective function~\cite{elbaum2002test}.

In Continuous Integration environments, TCP is particularly important because:
(1) test suites grow over time, making exhaustive testing impractical~\cite{memon2017taming},
(2) developers need rapid feedback on code changes~\cite{hilton2016usage}, and
(3) computing resources for testing are often limited~\cite{spieker2017reinforcement}.

\subsection{APFD Metric}
\label{sec:bg_apfd}

The Average Percentage of Faults Detected (APFD) is the standard metric for
evaluating TCP effectiveness~\cite{rothermel1999test}. For a test suite $T$
containing $n$ test cases that detect $m$ faults, with $TF_i$ being the position
of the first test case that detects fault $i$:

\begin{equation}
    \text{APFD} = 1 - \frac{\sum_{i=1}^{m} TF_i}{n \times m} + \frac{1}{2n}
\end{equation}

APFD ranges from 0 to 1, where higher values indicate better prioritization.
An APFD of 0.5 corresponds to random ordering, while 1.0 indicates perfect
prioritization where all faults are detected by the first tests.

\subsection{Graph Attention Networks}
\label{sec:bg_gat}

Graph Attention Networks (GAT)~\cite{velickovic2018graph} extend Graph Neural
Networks by incorporating attention mechanisms to weigh the importance of
neighboring nodes. For a node $i$ with neighbors $\mathcal{N}_i$, GAT computes:

\begin{equation}
    \vec{h}_i' = \sigma\left(\sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W} \vec{h}_j\right)
\end{equation}

where $\alpha_{ij}$ are attention coefficients computed as:

\begin{equation}
    \alpha_{ij} = \text{softmax}_j\left(\text{LeakyReLU}\left(\vec{a}^T[\mathbf{W}\vec{h}_i \| \mathbf{W}\vec{h}_j]\right)\right)
\end{equation}

Brody et al.~\cite{brody2022attentive} showed that standard GAT computes a
restricted form of ``static'' attention where the ranking of attention scores
is independent of the query node. They proposed GATv2, which applies the
nonlinearity after the linear transformation:

\begin{equation}
    \alpha_{ij} = \text{softmax}_j\left(\vec{a}^T \cdot \text{LeakyReLU}\left(\mathbf{W}[\vec{h}_i \| \vec{h}_j]\right)\right)
\end{equation}

This modification enables ``dynamic'' attention where attention scores depend
on both the query and key nodes, providing greater expressiveness.

\subsection{Class Imbalance and Focal Loss}
\label{sec:bg_focal}

Test execution data typically exhibits severe class imbalance, with far more
passing tests than failing tests. Standard cross-entropy loss is dominated by
the majority class, leading to models that predict ``pass'' for nearly all tests.

\textbf{Focal Loss}~\cite{lin2017focal} addresses this by down-weighting easy
examples and focusing on hard ones:

\begin{equation}
    \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

where $p_t$ is the predicted probability of the true class, $\alpha_t$ is a
class-balancing weight, and $\gamma$ is the focusing parameter. When $\gamma > 0$,
the $(1 - p_t)^\gamma$ term reduces the loss for well-classified examples,
focusing training on hard negatives.

\subsection{Cross-Attention Mechanisms}
\label{sec:bg_cross_attn}

Cross-attention allows one sequence (or modality) to attend to another, enabling
information exchange between different representations. Given queries $Q$ from
one modality and keys/values $K, V$ from another:

\begin{equation}
    \text{CrossAttn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

In multi-modal fusion, bidirectional cross-attention allows each modality to
selectively attend to relevant parts of the other, combining information in a
learned, adaptive manner.

%------------------------------------------------------------------------------
% SECTION 3: RELATED WORK
%------------------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

This section reviews prior work on test case prioritization, focusing on approaches
most relevant to our contribution.

\subsection{Traditional and Coverage-based TCP}

Early TCP research focused on code coverage-based techniques. Rothermel et al.~\cite{rothermel2001prioritizing}
proposed total and additional coverage prioritization. Elbaum et al.~\cite{elbaum2002test}
conducted extensive empirical studies comparing different strategies. History-based
approaches leverage past test execution results---Kim and Porter~\cite{kim2002history}
showed that tests that failed recently are more likely to fail again.

\subsection{Machine Learning for TCP}

Machine learning has been increasingly applied to TCP. Spieker et al.~\cite{spieker2017reinforcement}
introduced RETECS, using reinforcement learning for TCP in CI environments.
Deep learning approaches have shown promising results: Chen et al.~\cite{chen2023deeporder}
proposed DeepOrder using deep neural networks, and TCP-Net~\cite{abdelkarim2022tcp}
introduced an end-to-end approach that learns directly from test execution data.

\subsection{Graph Neural Networks for Testing}

Li et al.~\cite{li2024noderank} proposed NodeRank, a test input prioritization
approach for GNNs using mutation-based analysis. NodeRank leverages ensemble
learning-based ranking models with three types of mutations (graph structure,
node features, and model mutations). While NodeRank targets GNN model testing
rather than software TCP, it demonstrates the potential of graph-based approaches
for test prioritization.

\subsection{Comparison with Our Approach}

Table~\ref{tab:comparison} presents empirical comparison on our industrial dataset:

\begin{table}[h]
\centering
\caption{Comparison with State-of-the-Art TCP Approaches}
\label{tab:comparison}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Architecture} & \textbf{APFD} & \textbf{Std} & \textbf{Effect} \\
\midrule
\textbf{\filopriori{}} & \textbf{GAT + SBERT} & \textbf{0.7595} & \textbf{0.189} & -- \\
DeepOrder~\cite{chen2023deeporder} & Deep NN & 0.6919 & 0.267 & +9.8\% \\
NodeRank~\cite{li2024noderank} & Ensemble + Mutation & 0.6623 & 0.270 & +14.7\% \\
Random & -- & 0.5000 & -- & +51.9\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Results on industrial dataset (277 builds with failures).
\item *** $p < 0.001$ (Wilcoxon signed-rank test vs \filopriori{}); effects are improvements of \filopriori{} over each baseline.
\end{tablenotes}
\end{table}

Key differentiators of \filopriori{}:
\begin{itemize}
    \item \textbf{Graph-based modeling}: Explicitly models test relationships
    using Graph Attention Networks, capturing co-failure and semantic patterns.
    \item \textbf{Dual-stream architecture}: Combines semantic (SBERT) and
    structural (GAT) features for complementary information fusion.
    \item \textbf{Superior performance}: Achieves APFD=0.7595, significantly
    outperforming both DeepOrder (+9.8\%) and NodeRank (+14.7\%) with
    $p < 0.001$.
\end{itemize}

%------------------------------------------------------------------------------
% SECTION 4: APPROACH
%------------------------------------------------------------------------------
\section{Approach: \filopriori{}}
\label{sec:approach}

This section describes the \filopriori{} approach for Test Case Prioritization.

\subsection{Overview}

The \filopriori{} architecture is a dual-stream system that
takes as input: (1) test case descriptions and commit messages (semantic),
(2) historical test execution patterns (structural), and (3) test relationship
graph. It outputs a ranking of test cases by predicted failure probability.

The approach consists of three main modules:
\begin{enumerate}
    \item \textbf{Semantic Stream}: A feed-forward network that processes SBERT
    embeddings of test descriptions and commit messages, capturing textual
    similarity between test cases and code changes.
    \item \textbf{Structural Stream}: A Graph Attention Network (GAT) that processes
    historical features (failure rate, recency, flakiness) over the test relationship
    graph, learning to propagate failure signals between related tests.
    \item \textbf{Cross-Attention Fusion}: Bidirectional cross-attention that
    fuses semantic and structural representations, allowing each modality to
    attend to the other for final classification.
\end{enumerate}

This dual-stream design captures both what tests do (semantics) and how they
behave (structure), combining complementary information sources.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_framework_overview_new.pdf}
    \caption{Overview of the \filopriori{} framework. The system processes test case
    descriptions and execution history through dual semantic and structural streams,
    fuses representations via cross-attention, and applies post-processing for orphan
    handling and threshold optimization.}
    \label{fig:framework}
\end{figure}

\subsection{Semantic Feature Extraction}

We use Sentence-BERT (SBERT)~\cite{reimers2019sentence} with the \texttt{all-mpnet-base-v2}
model to encode textual information. For each test case, we concatenate:
\begin{itemize}
    \item Test case summary (TC\_Summary)
    \item Test case steps (TC\_Steps)
\end{itemize}

For each commit, we encode:
\begin{itemize}
    \item Commit message
    \item Code diff (truncated to 2000 characters)
\end{itemize}

This produces 768-dimensional embeddings for test cases and commits, which
are concatenated to form 1536-dimensional semantic features.

\subsection{Structural Feature Extraction}

We extract 19 structural features capturing historical execution patterns, organized
into three categories:

\textbf{Base Features (10 features):}
\begin{itemize}
    \item \textbf{test\_age}: Number of builds since first appearance
    \item \textbf{failure\_rate}: Historical failure percentage
    \item \textbf{recent\_failure\_rate}: Failure rate in last 5 builds
    \item \textbf{flakiness\_rate}: Pass/fail oscillation frequency
    \item \textbf{consecutive\_failures}: Current failure streak
    \item \textbf{max\_consecutive\_failures}: Maximum observed streak
    \item \textbf{failure\_trend}: Recent vs. overall failure rate difference
    \item \textbf{commit\_count}, \textbf{cr\_count}, \textbf{test\_novelty}
\end{itemize}

\textbf{DeepOrder-Inspired Features (9 features):} Following~\cite{chen2023deeporder},
we add temporal execution patterns:
\begin{itemize}
    \item \textbf{execution\_status\_last\_[1,2,3,5,10]}: Failure proportion in last N executions
    \item \textbf{cycles\_since\_last\_fail}: Builds since most recent failure
    \item \textbf{distance}: Temporal distance from last failure
    \item \textbf{status\_changes}: Number of Pass$\leftrightarrow$Fail transitions
    \item \textbf{fail\_rate\_last\_10}: Short-term failure rate
\end{itemize}

These temporal features capture patterns that static metrics miss---a test that
failed in the last execution is more likely to fail again than one that failed
only in distant history. Feature selection reduced the initial 29 features to 19
based on importance analysis and correlation filtering.

\subsection{Multi-Edge Test Relationship Graph}

We construct a dense multi-edge graph where nodes represent test cases and edges
capture five complementary relationship types. Dense graph connectivity is critical
for effective GAT message passing---sparse graphs limit information propagation
between related test cases.

\textbf{Edge Types and Weights}: We define five edge types with empirically-tuned weights:

\begin{table}[h]
\centering
\caption{Multi-Edge Graph Edge Types}
\label{tab:edge_types}
\footnotesize
\begin{tabular}{lcl}
\toprule
\textbf{Edge Type} & \textbf{Weight} & \textbf{Formula} \\
\midrule
Co-Failure & 1.0 & $\min(P(t_j|t_i), P(t_i|t_j))$ \\
Co-Success & 0.5 & $\min(P(t_j|t_i), P(t_i|t_j))$ \\
Component & 0.4 & $|C_i \cap C_j| / |C_i \cup C_j|$ \\
Semantic & 0.3 & $\cos(\mathbf{e}_i, \mathbf{e}_j)$ \\
Temporal & 0.2 & $\text{count}_{adj} / \max(\text{count})$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Co-Failure Edges} (weight 1.0) connect tests that fail together, where the
edge weight is the minimum conditional probability: if tests $t_i$ and $t_j$ co-failed
$n$ times, with $t_i$ failing $f_i$ times total, the weight is $\min(n/f_i, n/f_j)$.
This symmetric formulation ensures robust correlation measurement.

\textbf{Semantic Edges} (weight 0.3) connect tests with embedding similarity above
$\tau = 0.65$, with $k=10$ nearest neighbors per test. This relaxed threshold
(vs. $\tau = 0.75$ in prior work) increases connectivity for previously isolated nodes.

\textbf{Edge Combination}: For each test pair, we compute a combined edge weight
as the weighted sum normalized by total weight:
\begin{equation}
    w_{ij} = \frac{\sum_{e \in E} w_e \cdot \text{type}_e(i,j)}{\sum_{e \in E} w_e}
\end{equation}

\textbf{Impact}: This multi-edge approach increases graph density from 0.02\% (co-failure only)
to 0.5--1.0\% (all types), with \textbf{77.4\%} of test cases connected to the graph
(vs. 50--60\% with sparser configurations).

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_multi_edge_graph.pdf}
    \caption{Multi-edge test relationship graph with five edge types. Edge thickness
    and style indicate relationship type and strength. Failed tests (red) propagate
    failure signals through co-failure edges (weight 1.0), while orphan tests (dashed)
    connect through semantic edges (weight 0.3).}
    \label{fig:multiedge}
\end{figure}

\subsection{Semantic Stream}

The Semantic Stream processes text embeddings using a feed-forward network
with residual connections:

\textbf{Input}: SBERT embeddings of concatenated test case description and
commit message (1536 dimensions = 768 TC + 768 Commit).

\textbf{Architecture}: The stream consists of:
\begin{enumerate}
    \item Input projection: Linear layer projecting 1536-dim to 256-dim
    \item Two residual FFN blocks, each containing:
    \begin{itemize}
        \item Linear (256 $\rightarrow$ 1024)
        \item GELU activation
        \item Dropout (0.3)
        \item Linear (1024 $\rightarrow$ 256)
        \item Dropout (0.3)
        \item LayerNorm
    \end{itemize}
    \item Output LayerNorm
\end{enumerate}

The output is a 256-dimensional semantic representation for each test case.

\subsection{Structural Stream (Graph Attention Network)}

The Structural Stream processes historical features over the test relationship
graph using Graph Attention Networks~\cite{velickovic2018graph}:

\textbf{Input}: 10 structural features per test case (failure\_rate,
recent\_failure\_rate, test\_age, flakiness\_rate, etc.).

\textbf{GAT Architecture}: We use two GAT layers:
\begin{enumerate}
    \item \textbf{Layer 1}: Multi-head attention (4 heads) with concatenation
    \begin{equation}
        h_i^{(1)} = \|_{k=1}^{4} \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k \mathbf{W}^k h_j^{(0)}\right)
    \end{equation}
    where $\alpha_{ij}^k$ are learned attention coefficients for head $k$.
    Output dimension: $256 \times 4 = 1024$.

    \item \textbf{Layer 2}: Single-head attention with averaging, producing
    256-dimensional output per test case.
\end{enumerate}

\textbf{Edge Weights}: The GAT layers incorporate edge weights from the multi-edge
graph, allowing the model to learn that co-failure relationships (weight 1.0)
are more important than semantic similarity (weight 0.3).

\subsection{Cross-Attention Fusion}

The Cross-Attention Fusion module combines semantic and structural representations
through bidirectional attention:

\textbf{Bidirectional Cross-Attention}: Each modality attends to the other,
allowing semantic features to be informed by structural patterns and vice versa:

\begin{enumerate}
    \item \textbf{Semantic $\rightarrow$ Structural}: Semantic features query
    structural features:
    \begin{equation}
        h_{sem}' = h_{sem} + \text{MHA}(h_{sem}, h_{struct}, h_{struct})
    \end{equation}

    \item \textbf{Structural $\rightarrow$ Semantic}: Structural features query
    semantic features:
    \begin{equation}
        h_{struct}' = h_{struct} + \text{MHA}(h_{struct}, h_{sem}, h_{sem})
    \end{equation}
\end{enumerate}

where $\text{MHA}(Q, K, V)$ denotes multi-head attention with query $Q$,
key $K$, and value $V$.

Each attention uses 4 heads with 256-dimensional queries/keys/values, followed
by LayerNorm for stability.

\textbf{Feature Concatenation}: The attended features are concatenated:
\begin{equation}
    h_{fused} = [h_{sem}' \| h_{struct}']
\end{equation}

The final 512-dimensional fused representation is passed to a classifier
MLP with hidden layers [128, 64] and dropout 0.4.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/fig_dual_stream_architecture_new.pdf}
    \caption{Dual-stream architecture. The semantic stream processes SBERT embeddings
    through feed-forward networks, while the structural stream applies Graph Attention
    Networks over historical features. Cross-attention fusion enables bidirectional
    information flow between modalities.}
    \label{fig:architecture}
\end{figure}

\subsection{Training with Single-Mechanism Class Balancing}

Addressing the severe class imbalance (37:1 Pass:Fail ratio) requires careful design.
We found that \emph{multiple compensation mechanisms cause over-correction}---using
class weights, balanced sampling, and focal alpha together led to mode collapse
where the model predicted all samples as the minority class.

\textbf{Key Insight}: A single balancing mechanism is sufficient and more stable.
We use \textbf{balanced sampling only}, with neutral loss parameters:

\begin{equation}
    \mathcal{L} = -\alpha \cdot (1 - p_t)^\gamma \cdot \log(p_t)
\end{equation}

where $\alpha = 0.5$ (neutral, no class preference) and $\gamma = 2.0$ (focus on
hard examples). The balancing is achieved entirely through stratified sampling:

\begin{table}[h]
\centering
\caption{Single-Mechanism Balancing Configuration}
\label{tab:balancing}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Mechanism} & \textbf{Prior Work} & \textbf{Ours} \\
\midrule
Class weights in loss & $19\times$ & \textbf{Disabled} \\
Focal alpha & $0.85$ ($1.7\times$) & $0.5$ (neutral) \\
Balanced sampling & $20\times$ & $15\times$ (only) \\
\midrule
Effective weight & $\approx 646\times$ & $15\times$ \\
\bottomrule
\end{tabular}
\end{table}

The balanced sampler uses minority weight 1.0 and majority weight 0.07 (ratio $\approx$15:1),
ensuring the minority class is adequately represented without over-compensation.

\textbf{Training Configuration}:
\begin{itemize}
    \item Optimizer: AdamW with learning rate $3 \times 10^{-5}$
    \item Weight decay: $1 \times 10^{-4}$
    \item Scheduler: Cosine annealing with $\eta_{min} = 1 \times 10^{-6}$
    \item Early stopping: Patience 15, monitoring validation F1-macro
    \item Gradient clipping: Max norm 1.0
\end{itemize}

\subsection{Threshold Optimization}

With extreme class imbalance, the default classification threshold of 0.5 is suboptimal.
We implement a two-phase threshold search:

\textbf{Phase 1 (Coarse)}: Search range $[0.05, 0.90]$ with step 0.05 (17 points).

\textbf{Phase 2 (Fine)}: Refine around the coarse optimum $\pm 0.05$ with step 0.01.

The optimization metric is $F_\beta$ with $\beta = 0.8$, slightly favoring precision
to avoid excessive false positives while maintaining reasonable recall:
\begin{equation}
    F_\beta = (1 + \beta^2) \cdot \frac{\text{precision} \cdot \text{recall}}{\beta^2 \cdot \text{precision} + \text{recall}}
\end{equation}

This yields an optimal threshold of \textbf{0.28} (vs. default 0.50), significantly
improving minority class recall from 5\% to 30\%.

\subsection{Orphan Test Case Handling}

\textbf{Orphan tests}---those absent from the training graph---receive uniform scores
from the GAT, destroying ranking capability. We address this with a four-stage
KNN-based scoring pipeline:

\textbf{Stage 1: KNN Similarity.} For each orphan, compute Euclidean distance to
all in-graph tests and select $k=20$ nearest neighbors. We use Euclidean (not cosine)
distance because SBERT embeddings have meaningful magnitude information.

\textbf{Stage 2: Structural Blend.} Combine semantic similarity (embeddings) with
structural similarity (historical features) using weight $w_s = 0.35$:
\begin{equation}
    \text{sim}_{combined} = (1 - w_s) \cdot \text{sim}_{semantic} + w_s \cdot \text{sim}_{structural}
\end{equation}

\textbf{Stage 3: Temperature-Scaled Softmax.} Apply softmax with temperature $T=0.7$
to concentrate weights on the most similar neighbors:
\begin{equation}
    w_i = \frac{\exp(\text{sim}_i / T)}{\sum_j \exp(\text{sim}_j / T)}
\end{equation}

\textbf{Stage 4: Alpha Blending.} Compute KNN score as weighted average of neighbor
scores, then blend with base score ($\alpha = 0.55$):
\begin{equation}
    \text{score}_{orphan} = \alpha \cdot \sum_i w_i \cdot \text{score}_i + (1-\alpha) \cdot \text{score}_{base}
\end{equation}

\textbf{Impact}: Before orphan handling, orphan scores had zero variance (all 0.50).
After, scores range from 0.29 to 0.51 with std=0.046, enabling effective ranking
among the 22.6\% of tests that are orphans.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig_orphan_pipeline_new.pdf}
    \caption{Four-stage orphan handling pipeline. Orphan test cases are scored using
    KNN similarity to in-graph tests, with structural blending and temperature-scaled
    softmax to concentrate weights on the most similar neighbors.}
    \label{fig:orphan}
\end{figure}

%------------------------------------------------------------------------------
% SECTION 5: EXPERIMENTAL DESIGN
%------------------------------------------------------------------------------
\section{Experimental Design}
\label{sec:experimental}

\subsection{Research Questions}

\begin{itemize}
    \item \textbf{RQ1 (Effectiveness)}: How effective is \filopriori{} compared to baseline methods?
    \item \textbf{RQ2 (Components)}: What is the contribution of each architectural component?
    \item \textbf{RQ3 (Robustness)}: How robust is \filopriori{} across different time periods?
    \item \textbf{RQ4 (Sensitivity)}: How sensitive is \filopriori{} to hyperparameter choices?
\end{itemize}

\subsection{Datasets}

We evaluate \filopriori{} on three complementary datasets spanning industrial
software testing, GNN model testing, and open-source CI/CD environments.

\textbf{Dataset 1: Industrial QTA Dataset.} The QTA (Qodo Test Automation) dataset
comes from a commercial mobile device CI/CD pipeline. This dataset provides rich
semantic information including detailed test descriptions, test steps, and commit
messages. Table~\ref{tab:dataset_qta} summarizes its statistics.

\begin{table}[h]
\centering
\caption{Industrial QTA Dataset Statistics}
\label{tab:dataset_qta}
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Total test executions & 52,102 \\
Unique builds & 1,339 \\
Builds with failures & 277 (20.7\%) \\
Unique test cases & 2,347 \\
Pass:Fail ratio & 37:1 \\
Semantic info & Rich (descriptions, commits) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset 2: GNN Benchmark Datasets.} To evaluate generalization to GNN
model testing, we use three standard citation network datasets from Li et al.'s
NodeRank~\cite{li2024noderank}: \textbf{Cora}, \textbf{CiteSeer}, and \textbf{PubMed}.
These datasets are widely used in GNN research and provide a direct comparison
against state-of-the-art test input prioritization methods. Table~\ref{tab:dataset_gnn}
summarizes their characteristics.

\begin{table}[h]
\centering
\caption{GNN Benchmark Dataset Statistics}
\label{tab:dataset_gnn}
\begin{tabular}{lrrrl}
\toprule
\textbf{Dataset} & \textbf{Nodes} & \textbf{Edges} & \textbf{Classes} & \textbf{Domain} \\
\midrule
Cora & 2,708 & 10,556 & 7 & Citation network \\
CiteSeer & 3,327 & 9,104 & 6 & Citation network \\
PubMed & 19,717 & 88,648 & 3 & Citation network \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset 3: RTPTorrent Open-Source Dataset.} To evaluate generalization
across diverse open-source projects, we use the RTPTorrent dataset~\cite{mattis2020rtptorrent},
an open-source benchmark from MSR 2020 containing test execution histories from
20 Java projects on GitHub with over 100,000 Travis CI build logs.

\begin{table}[h]
\centering
\caption{RTPTorrent Dataset Statistics}
\label{tab:dataset_rtptorrent}
\begin{tabular}{lr}
\toprule
\textbf{Statistic} & \textbf{Value} \\
\midrule
Projects & 20 Java projects \\
Source & Travis CI build logs \\
Semantic info & Limited (test names) \\
License & CC BY 4.0 \\
\bottomrule
\end{tabular}
\end{table}

The complementary nature of these datasets enables comprehensive evaluation:
the industrial dataset tests semantic feature utilization, the GNN benchmarks
enable direct comparison with NodeRank on established datasets, and RTPTorrent
tests cross-project generalization in open-source environments.

\subsection{Baselines}

We compare against eight baselines:

\textbf{Heuristic Baselines}:
\begin{itemize}
    \item \textbf{Random}: Random ordering (expected APFD $\approx$ 0.5)
    \item \textbf{Recency}: Prioritize recently failed tests
    \item \textbf{RecentFailureRate}: Failure rate in last 5 builds
    \item \textbf{FailureRate}: Historical failure rate
    \item \textbf{GreedyHistorical}: Combined heuristics
\end{itemize}

\textbf{ML Baselines}:
\begin{itemize}
    \item \textbf{Logistic Regression}
    \item \textbf{Random Forest}
    \item \textbf{XGBoost}
\end{itemize}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{APFD}: Primary metric, computed per build
    \item \textbf{Statistical tests}: Wilcoxon signed-rank test ($\alpha = 0.05$)
    \item \textbf{Confidence intervals}: 95\% bootstrap CI (1000 iterations)
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
    \item \textbf{Framework}: PyTorch 2.0, PyTorch Geometric 2.3
    \item \textbf{Hardware}: NVIDIA RTX 3090 (24GB VRAM)
    \item \textbf{Training}: 50 epochs, batch size 32, AdamW optimizer
    \item \textbf{Learning rate}: $3 \times 10^{-5}$ with cosine annealing
    \item \textbf{Early stopping}: Patience 15, monitoring val\_f1\_macro
\end{itemize}

%------------------------------------------------------------------------------
% SECTION 6: RESULTS
%------------------------------------------------------------------------------
\section{Results}
\label{sec:results}

\input{sections/results_ieee}

%------------------------------------------------------------------------------
% SECTION 7: DISCUSSION
%------------------------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}

\input{sections/discussion_ieee}

%------------------------------------------------------------------------------
% SECTION 8: THREATS TO VALIDITY
%------------------------------------------------------------------------------
\section{Threats to Validity}
\label{sec:threats}

\input{sections/threats_ieee}

%------------------------------------------------------------------------------
% SECTION 9: CONCLUSION
%------------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

We presented \filopriori{}, a dual-stream deep learning approach for Test Case
Prioritization that combines Graph Attention Networks with semantic embeddings.
By modeling test relationships through multi-edge graphs, we capture complex
dependencies between test cases that simpler approaches ignore.

Our key contributions include:
\begin{itemize}
    \item A dual-stream GAT architecture combining SBERT semantic embeddings with
    structural feature learning from test execution history.
    \item Multi-edge test relationship graphs capturing co-failure, co-success,
    and semantic similarity relationships.
    \item Empirical validation on industrial data achieving \textbf{APFD=0.7595},
    significantly outperforming NodeRank ($+14.7\%$, $p < 0.001$) and DeepOrder ($+9.8\%$).
    \item State-of-the-art results on GNN benchmarks (\textbf{APFD=0.796}),
    outperforming NodeRank on Cora ($+3.4\%$) and CiteSeer ($+1.3\%$).
    \item Cross-dataset validation on RTPTorrent achieving \textbf{APFD=0.838}
    across 20 Java projects, outperforming 6 of 7 baselines.
    \item Robust temporal generalization (APFD: 0.73--0.78 across time periods).
\end{itemize}

The ablation study reveals that the Dense Multi-Edge Graph construction is the
most critical component ($+10.0\%$), validating our hypothesis that modeling
test relationships through rich graph structures is more effective than treating
tests as independent entities.

\textbf{Future Work.} We plan to: (1) investigate dynamic graph construction,
(2) explore cross-project transfer learning with pre-trained code embeddings,
and (3) develop real-time prioritization for CI/CD pipelines.

\textbf{Data Availability.} Our replication package, including source code,
trained models, configuration files, and anonymized dataset, is available at:
\url{https://github.com/[anonymized]/filo-priori-v9}

%------------------------------------------------------------------------------
% ACKNOWLEDGMENTS
%------------------------------------------------------------------------------
\section*{Acknowledgments}
This work was supported by [funding information]. We thank [acknowledgments].

%------------------------------------------------------------------------------
% REFERENCES
%------------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{references_ieee}

%------------------------------------------------------------------------------
% BIOGRAPHY (Optional for TSE)
%------------------------------------------------------------------------------
\begin{IEEEbiographynophoto}{Acauan Cardoso Ribeiro}
is a researcher at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil. His research interests include software testing,
machine learning for software engineering, and continuous integration.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Eduardo Luzeiro Feitosa}
is a professor at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Andre Luiz da Costa Carvalho}
is a professor at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Eulanda Miranda dos Santos}
is a professor at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Bruno Freitas Gadelha}
is a professor at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Yan Rodrigo Da Silva Soares}
is a researcher at the Institute of Computing (IComp), Federal University of
Amazonas (UFAM), Brazil.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Jose Nascimento}
is at Motorola Mobility LLC, Manaus, AM, Brazil.
\end{IEEEbiographynophoto}

\end{document}
