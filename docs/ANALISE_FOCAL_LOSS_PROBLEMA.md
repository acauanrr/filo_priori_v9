# AnÃ¡lise: Focal Loss Causando Colapso de PrediÃ§Ã£o

**Data**: 2025-11-07
**Status**: ðŸ”´ **PROBLEMA CRÃTICO**

---

## ðŸ”´ SINTOMAS

```python
Classification Report:
              precision    recall  f1-score   support

    Not-Pass       0.03      1.00      0.06       174
        Pass       0.00      0.00      0.00      5888

    accuracy                           0.03      6062
```

**O modelo prevÃª TUDO como classe 0 (Not-Pass/Fail)!**

---

## ðŸ” ANÃLISE MATEMÃTICA

### ConfiguraÃ§Ã£o Atual

```yaml
focal:
  alpha: [0.995, 0.005]  # [classe_0_Fail, classe_1_Pass]
  gamma: 3.5
```

### Como o Focal Loss Funciona

```python
# losses.py linha 93:
alpha_t = self.alpha[targets]  # Indexa pelo target VERDADEIRO
focal_loss = alpha_t * focal_weight * ce_loss
```

**TraduÃ§Ã£o:**
- Samples com label=0 (Fail): loss Ã— 0.995
- Samples com label=1 (Pass): loss Ã— 0.005

### O Ratio Criado

```
Peso Fail / Peso Pass = 0.995 / 0.005 = 199:1
```

**199x MAIS peso para Fail que para Pass!**

### Por Que o Modelo Colapsa?

O modelo "raciocina":

```
"Se eu errar 1 Fail: loss = 0.995 Ã— ...  = GIGANTE
 Se eu errar 199 Pass: loss = 199 Ã— 0.005 Ã— ... = 0.995 Ã— ... = MESMO!

 Logo, melhor prever TUDO como Fail para minimizar o risco!"
```

---

## ðŸ“Š COMPORTAMENTO OBSERVADO

### PrediÃ§Ãµes do Modelo

```python
# Validation set: 6062 samples
predictions = [0, 0, 0, 0, ..., 0]  # TODOS zeros!

# True labels:
# - 174 Fail (classe 0)
# - 5888 Pass (classe 1)
```

### ConfusÃ£o Matrix Resultante

```
                Predicted
               Not-Pass  Pass
Actual Not-Pass   174      0    â† 100% recall
       Pass      5888      0    â† 0% recall
```

### MÃ©tricas Calculadas

```python
# Not-Pass:
Recall    = 174 / (174 + 0) = 1.00   (100%)
Precision = 174 / (174 + 5888) = 0.0287  (2.9%)

# Pass:
Recall    = 0 / (0 + 5888) = 0.00   (0%)
Precision = undefined (nenhuma prediÃ§Ã£o)

# Overall:
Accuracy = (174 + 0) / 6062 = 0.0287  (2.9%)
```

---

## âŒ POR QUE ALPHA [0.995, 0.005] ESTÃ ERRADO?

### 1. Ratio Excessivo (199:1)

Para imbalance de 37:1 (Pass:Fail), um ratio de peso de **199:1** Ã© **EXCESSIVAMENTE AGRESSIVO**!

### 2. ComparaÃ§Ã£o com Class Weights

```python
# Class weights calculados automaticamente:
# class_weights = [19.00785973, 0.51350777]
# Ratio: 19.00 / 0.513 â‰ˆ 37:1

# Focal Loss alpha atual:
# alpha = [0.995, 0.005]
# Ratio: 0.995 / 0.005 = 199:1  â† 5.4x MAIS AGRESSIVO!
```

### 3. InterpretaÃ§Ã£o do Alpha

No Focal Loss, **alpha NÃƒO deve ser interpretado como class weight direto**!

Alpha Ã© um **fator de escala da loss**, nÃ£o um peso de classe. Valores muito extremos causam colapso.

---

## âœ… SOLUÃ‡Ã•ES

### SOLUÃ‡ÃƒO 1: Ajustar Alpha para Valores RazoÃ¡veis

```yaml
# âŒ ATUAL (causa colapso):
focal:
  alpha: [0.995, 0.005]  # Ratio 199:1
  gamma: 3.5

# âœ… OPÃ‡ÃƒO A (conservador):
focal:
  alpha: [0.75, 0.25]  # Ratio 3:1
  gamma: 2.5

# âœ… OPÃ‡ÃƒO B (moderado):
focal:
  alpha: [0.85, 0.15]  # Ratio 5.7:1
  gamma: 3.0

# âœ… OPÃ‡ÃƒO C (agressivo mas razoÃ¡vel):
focal:
  alpha: [0.95, 0.05]  # Ratio 19:1 (igual ao class weight)
  gamma: 3.0
```

**RECOMENDAÃ‡ÃƒO**: ComeÃ§ar com **OPÃ‡ÃƒO C** (alpha=[0.95, 0.05]) que tem ratio igual ao class weight natural.

### SOLUÃ‡ÃƒO 2: Usar Weighted Cross-Entropy (RECOMENDADO!)

```yaml
# âœ… MAIS SIMPLES E INTUITIVO:
loss:
  type: "weighted_ce"
  weighted_ce:
    use_class_weights: true
    # Usa class_weights do DataLoader automaticamente
    # class_weights = [19.0, 0.51] â†’ ratio 37:1
```

**Vantagens:**
- Mais intuitivo que Focal Loss
- Ratio de peso corresponde exatamente ao imbalance
- Menos propenso a colapso
- Amplamente testado e validado

### SOLUÃ‡ÃƒO 3: Focal Loss com Gamma Baixo

```yaml
# âœ… Reduzir gamma tambÃ©m ajuda:
focal:
  alpha: [0.8, 0.2]  # Ratio 4:1 (conservador)
  gamma: 1.5        # Mais suave que 3.5
```

---

## ðŸŽ¯ RECOMENDAÃ‡ÃƒO FINAL

### Usar Weighted Cross-Entropy

```yaml
loss:
  type: "weighted_ce"
  weighted_ce:
    use_class_weights: true
```

**Por quÃª?**
1. âœ… Simples e intuitivo
2. âœ… NÃ£o requer ajuste fino de alpha/gamma
3. âœ… Usa class weights naturais (37:1)
4. âœ… Menos propenso a colapso
5. âœ… ImplementaÃ§Ã£o jÃ¡ existe em losses.py

---

## ðŸ“ CORREÃ‡ÃƒO APLICAR

### 1. Criar nova configuraÃ§Ã£o: `experiment_v8_weighted_ce.yaml`

```yaml
experiment:
  name: "v8_weighted_ce"
  version: "8.0.2"
  description: "V8 with Weighted CE instead of problematic Focal Loss"

# ... (resto igual) ...

# Loss Function - WEIGHTED CE
loss:
  type: "weighted_ce"

  weighted_ce:
    use_class_weights: true
    label_smoothing: 0.0

  # For reference, these were the problematic focal loss values:
  # focal:
  #   alpha: [0.995, 0.005]  # âŒ TOO EXTREME - ratio 199:1
  #   gamma: 3.5
```

### 2. Atualizar main_v8.py

```python
# main_v8.py linha 473-479
# âŒ ANTES:
if config['loss']['type'] == 'focal':
    criterion = FocalLoss(
        alpha=config['loss']['focal']['alpha'],
        gamma=config['loss']['focal']['gamma']
    ).to(device)
else:
    criterion = nn.CrossEntropyLoss().to(device)

# âœ… DEPOIS:
if config['loss']['type'] == 'focal':
    criterion = FocalLoss(
        alpha=config['loss']['focal']['alpha'],
        gamma=config['loss']['focal']['gamma']
    ).to(device)
elif config['loss']['type'] == 'weighted_ce':
    # Use class weights from DataLoader
    class_weights_tensor = torch.FloatTensor(data_dict['class_weights']).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor).to(device)
else:
    criterion = nn.CrossEntropyLoss().to(device)
```

---

## ðŸ“Š RESULTADOS ESPERADOS (Weighted CE)

```
Classification Report (EXPECTED):
              precision    recall  f1-score   support

    Not-Pass       0.35      0.45      0.40       174
        Pass       0.98      0.97      0.98      5888

    accuracy                           0.95      6062
   macro avg       0.66      0.71      0.69      6062
weighted avg       0.96      0.95      0.95      6062
```

**Metas:**
- Recall Not-Pass: â‰¥ 40% (detecta falhas!)
- Recall Pass: â‰¥ 95% (mantÃ©m performance)
- Accuracy: â‰¥ 95% (alta overall)
- F1 Macro: â‰¥ 0.65 (balanceado)

---

## ðŸŽ“ LIÃ‡Ã•ES APRENDIDAS

1. **Focal Loss Alpha â‰  Class Weight**
   - Alpha Ã© fator de escala, nÃ£o peso direto
   - Valores extremos causam colapso

2. **Ratio de 199:1 Ã© Excessivo**
   - Para imbalance 37:1, ratio 199:1 Ã© ~5x agressivo demais
   - Ratio de peso deve ser prÃ³ximo ao ratio natural

3. **Weighted CE Ã© Mais Seguro**
   - Mais intuitivo
   - Menos propenso a colapso
   - Usa weights naturais diretamente

4. **Sempre Validar MÃ©tricas Cedo**
   - Se Recall de uma classe = 100% e outra = 0%, PARE!
   - Isso indica colapso de prediÃ§Ã£o

5. **Focal Loss Precisa Ajuste Fino**
   - Alpha e gamma sÃ£o sensÃ­veis
   - Valores padrÃ£o da literatura (alpha=0.25, gamma=2.0) sÃ£o para cenÃ¡rios diferentes
   - Precisa experimentaÃ§Ã£o cuidadosa

---

## âœ… PRÃ“XIMO PASSO

1. Criar `configs/experiment_v8_weighted_ce.yaml`
2. Atualizar main_v8.py para suportar weighted_ce
3. Executar treino com Weighted CE
4. Validar que ambas as classes sÃ£o preditas (diversity > 0.3)

---

**Status**: ðŸ”´ **PROBLEMA IDENTIFICADO E SOLUÃ‡ÃƒO PROPOSTA**

**AÃ§Ã£o Imediata**: Trocar Focal Loss por Weighted Cross-Entropy
