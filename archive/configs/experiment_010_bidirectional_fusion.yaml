# PHASE II: ARCHITECTURAL REFINEMENT - Experiment 010
# Objective: Improve fusion with Bidirectional Cross-Attention
# Expected: Test F1 Macro 0.70-0.72 → 0.72-0.75 (+2-3 pp cumulative)
#
# CHANGES from Experiment 007:
# - Structural stream: GAT → GATv2 (from Exp 008)
# - Semantic stream: Mean → Attention Pooling (from Exp 009)
# - Fusion: Concatenation → Bidirectional Cross-Attention (NEW in Exp 010)
#
# Baseline: Experiment 009 (Expected F1 Macro ≈ 0.70-0.72)

# Data Configuration (SAME AS EXP 007)
data:
  train_path: "datasets/train.csv"
  test_path: "datasets/test.csv"
  output_dir: "results/experiment_010_bidirectional_fusion"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42
  sample_size: null

  # Binary Classification
  binary_classification: true
  binary_positive_class: "Pass"
  binary_strategy: "pass_vs_all"

  # NO DATA AUGMENTATION
  use_smote: false
  use_resampling: false

# Text Processing (SAME AS EXP 007)
text:
  max_summary_length: 128
  max_steps_length: 256
  max_commit_length: 512
  commit_strategy: "first_n"
  num_commits_to_keep: 3

# Embedding Configuration (SAME AS EXP 007)
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  batch_size: 32
  max_length: 512
  pooling_strategy: "mean"
  normalize_embeddings: true
  query_instruction: null
  cache_embeddings: true
  embedding_dim: 1024

# Phylogenetic Structure (SAME AS EXP 007)
phylogenetic:
  distance_metric: "cosine"
  tree_method: "knn_only"
  linkage_method: "average"
  tree_type: "none"
  use_approx: true
  subsample_for_tree: 50000
  cache_tree: true

# Model Architecture - CHANGES: GATv2 + Attention Pooling + Bidirectional Fusion
model:
  # Semantic Stream - CHANGE: Attention Pooling (from Exp 009)
  semantic_stream:
    hidden_dim: 256
    num_layers: 2
    dropout: 0.4
    activation: "gelu"
    pooling_type: "attention"
    attention_pooling:
      dropout: 0.1
      use_layer_norm: true

  # Structural Stream - CHANGE: GATv2 (from Exp 008)
  structural_stream:
    hidden_dim: 256
    num_gnn_layers: 2
    num_neighbors: 10
    dropout: 0.4
    aggregation: "mean"
    layer_type: "gatv2"
    use_residual: true

  # Cross-Attention Fusion - CHANGE: Bidirectional Cross-Attention
  cross_attention:
    num_heads: 4
    hidden_dim: 256
    dropout: 0.4
    num_layers: 1
    fusion_type: "bidirectional_cross_attention"  # ⭐ NEW: Changed from "concatenation"
    use_feedforward: true                          # ⭐ NEW: Add FFN after cross-attention
    use_gating: true                               # ⭐ NEW: Use gating mechanism for final fusion

  # Classification Head - NO CHANGE
  classifier:
    hidden_dim: 128
    num_classes: 2
    dropout: 0.5
    classifier_type: "simple"

# Training Configuration (SLIGHTLY ADJUSTED for stability)
training:
  num_epochs: 80
  batch_size: 64
  learning_rate: 4.0e-5  # ⚠️ SLIGHTLY REDUCED: More complex fusion may need slower LR
  weight_decay: 1.0e-4
  warmup_steps: 1000
  gradient_clip: 1.0
  use_balanced_sampling: true
  optimize_threshold: true
  threshold_search_range: [0.20, 0.80]

  # Loss Configuration (SAME AS EXP 007)
  loss:
    type: "focal"
    focal_alpha: [0.25, 0.75]
    use_class_weights: false
    focal_gamma: 2.0
    label_smoothing: 0.05

  # Optimization (SAME AS EXP 007)
  optimizer: "adamw"
  scheduler: "cosine"

  # Early Stopping (INCREASED patience for more complex model)
  early_stopping:
    patience: 15  # ⚠️ INCREASED: Complex model may need more time
    min_delta: 5.0e-4
    monitor: "f1_macro"
    mode: "max"

  # Checkpointing
  save_every_n_epochs: 5
  save_best_only: true

# Evaluation Configuration (SAME AS EXP 007)
evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "precision_weighted"
    - "recall_macro"
    - "recall_weighted"
    - "confusion_matrix"
    - "roc_auc"
    - "auprc"

# Hardware Configuration (SAME AS EXP 007)
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Logging
logging:
  use_tensorboard: true
  log_every_n_steps: 50
  log_dir: "results/experiment_010_bidirectional_fusion/logs"
  save_predictions: true

# ADVANCED MONITORING (SAME AS EXP 007)
monitoring:
  enabled: true
  check_every_n_batches: 50
  prediction_diversity_threshold: 0.05
  gradient_norm_max: 100.0
  gradient_norm_min: 1e-6
  loss_spike_multiplier: 5.0
  auto_terminate_on_collapse: true

# SUCCESS CRITERIA (Phase II: Architectural Refinement)
success_criteria:
  phase: "architectural_refinement"
  required_metrics:
    f1_macro: 0.72  # Target: 0.70-0.72 + 0.02-0.03
    accuracy: 0.60
    recall_pass: 0.50
    prediction_diversity: 0.30
  blocking_failures:
    - "prediction_collapse"
    - "performance_regression"  # F1 < 0.69 (below Exp 009 - tolerance)

# Experiment Metadata
experiment:
  phase: "II"
  phase_name: "Architectural Refinement"
  experiment_id: "010"
  description: "Add Bidirectional Cross-Attention fusion (cumulative with GATv2 + Attention Pooling)"
  hypothesis: "Bidirectional cross-attention will allow semantic and structural streams to mutually enhance each other, improving F1 Macro by additional 2-3 pp"
  baseline_experiment: "009"
  baseline_metrics:
    f1_macro: 0.71  # Expected from 009 (midpoint of 0.70-0.72)
    accuracy: 0.88
  expected_improvements:
    - "F1 Macro: 0.71 → 0.72-0.75 (+1-4 pp over Exp 009)"
    - "F1 Macro: 0.6677 → 0.72-0.75 (+5-8 pp over Exp 007)"
    - "Richer interaction between semantic and structural features"
    - "Better fusion of complementary information"
  rationale: |
    Bidirectional cross-attention allows semantic features to attend to structural
    features AND vice versa, creating a richer and more interactive fusion.
    This goes beyond simple concatenation by learning which features from each
    stream are most relevant to the other. The gating mechanism further refines
    the fusion by learning optimal weights for combining the enhanced features.
  training_notes: |
    - Slightly reduced LR (4e-5 vs 5e-5) due to increased model complexity
    - Increased patience (15 vs 12) to allow more time for convergence
    - More parameters due to cross-attention mechanisms
